{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"example docs \u00b6 This is a basic example of documentation.","title":"Home"},{"location":"#example-docs","text":"This is a basic example of documentation.","title":"example docs"},{"location":"ADR-template/","text":"Title \u00b6 Decision Made: yes/no Decision Date: mo year Revisit Decision: yes/no Date mo year Revisit criteria: Decision Made: No, but open to revisiting Decision Date: 12/2018 Revisit Decision: Yes Revisit Date: July 2019 Revisit Criteria: If a developer is interested in Jest and has time or suggestions for fixing the speed issues, we should revisit this. Decision Makers: @githubusername, @githubsername tl;dr \u00b6 Summary of problem and decision History \u00b6 Dates, links to the PRs, notes- anything relevant to understand the decision making process / evolution. Important to list what you tried here so avoid retreading Pros \u00b6 upside Cons \u00b6 downside Decision \u00b6 Detailed reasoning about decision Example","title":"Title"},{"location":"ADR-template/#title","text":"Decision Made: yes/no Decision Date: mo year Revisit Decision: yes/no Date mo year Revisit criteria: Decision Made: No, but open to revisiting Decision Date: 12/2018 Revisit Decision: Yes Revisit Date: July 2019 Revisit Criteria: If a developer is interested in Jest and has time or suggestions for fixing the speed issues, we should revisit this. Decision Makers: @githubusername, @githubsername","title":"Title"},{"location":"ADR-template/#tldr","text":"Summary of problem and decision","title":"tl;dr"},{"location":"ADR-template/#history","text":"Dates, links to the PRs, notes- anything relevant to understand the decision making process / evolution. Important to list what you tried here so avoid retreading","title":"History"},{"location":"ADR-template/#pros","text":"upside","title":"Pros"},{"location":"ADR-template/#cons","text":"downside","title":"Cons"},{"location":"ADR-template/#decision","text":"Detailed reasoning about decision Example","title":"Decision"},{"location":"allstar/","text":"Allstar Configuration Files \u00b6 Allstar is a GitHub App ran by OpenSSF that helps set and enforce security policies for GitHub Repositories. Our repository uses a Repository Level Opt In Strategy . This means our repository contains our own .allstar directory to manage our security policies instead of using an organizational level .allstar directory. Inside the .allstar directory are several configuration files that outline our security policies and what Actions to take in the event of a security violation. Actions \u00b6 log : This is the default action, and actually takes place for all actions. All policy run results and details are logged. Logs are currently only visible to the app operator, plans to expose these are under discussion. issue : This action creates a GitHub issue. Only one issue is created per policy, and the text describes the details of the policy violation. If the issue is already open, it is pinged with a comment every 24 hours (not currently user configurable). Once the violation is addressed, the issue will be automatically closed by Allstar within 5-10 minutes. fix : This action is policy specific. The policy will make the changes to the GitHub settings to correct the policy violation. Not all policies will be able to support this (see below). Configuration Files in .allstar Directory \u00b6 allstar.yaml \u00b6 Purpose \u00b6 Configures whether our repository will opt in or opt out of using Allstar app for reporting security violations. Since our organization does not contain a .allstar directory, the default Allstar strategy for all repositories is assumed requiring each repository to opt in to manage security policies. binary_artifacts.yaml \u00b6 Purpose \u00b6 This policy uses check from scorecard . Remove the binary artifact from the repository to achieve compliance. As the scorecard results can be verbose, you may need to run scorecard itself to see all the detailed information. branch_protection.yaml \u00b6 Purpose \u00b6 This policy checks if our repository's branch protection settings match with the branch protection settings outlined in this file. outside.yaml \u00b6 Purpose \u00b6 By default this policy checks that only organizational members have administrative or push access to the repository. security.yaml \u00b6 Purpose \u00b6 This policy checks that the repository has a security policy file in SECURITY.md and it is not empty.","title":"Allstar"},{"location":"allstar/#allstar-configuration-files","text":"Allstar is a GitHub App ran by OpenSSF that helps set and enforce security policies for GitHub Repositories. Our repository uses a Repository Level Opt In Strategy . This means our repository contains our own .allstar directory to manage our security policies instead of using an organizational level .allstar directory. Inside the .allstar directory are several configuration files that outline our security policies and what Actions to take in the event of a security violation.","title":"Allstar Configuration Files"},{"location":"allstar/#actions","text":"log : This is the default action, and actually takes place for all actions. All policy run results and details are logged. Logs are currently only visible to the app operator, plans to expose these are under discussion. issue : This action creates a GitHub issue. Only one issue is created per policy, and the text describes the details of the policy violation. If the issue is already open, it is pinged with a comment every 24 hours (not currently user configurable). Once the violation is addressed, the issue will be automatically closed by Allstar within 5-10 minutes. fix : This action is policy specific. The policy will make the changes to the GitHub settings to correct the policy violation. Not all policies will be able to support this (see below).","title":"Actions"},{"location":"allstar/#configuration-files-in-allstar-directory","text":"","title":"Configuration Files in .allstar Directory"},{"location":"allstar/#allstaryaml","text":"","title":"allstar.yaml"},{"location":"allstar/#purpose","text":"Configures whether our repository will opt in or opt out of using Allstar app for reporting security violations. Since our organization does not contain a .allstar directory, the default Allstar strategy for all repositories is assumed requiring each repository to opt in to manage security policies.","title":"Purpose"},{"location":"allstar/#binary_artifactsyaml","text":"","title":"binary_artifacts.yaml"},{"location":"allstar/#purpose_1","text":"This policy uses check from scorecard . Remove the binary artifact from the repository to achieve compliance. As the scorecard results can be verbose, you may need to run scorecard itself to see all the detailed information.","title":"Purpose"},{"location":"allstar/#branch_protectionyaml","text":"","title":"branch_protection.yaml"},{"location":"allstar/#purpose_2","text":"This policy checks if our repository's branch protection settings match with the branch protection settings outlined in this file.","title":"Purpose"},{"location":"allstar/#outsideyaml","text":"","title":"outside.yaml"},{"location":"allstar/#purpose_3","text":"By default this policy checks that only organizational members have administrative or push access to the repository.","title":"Purpose"},{"location":"allstar/#securityyaml","text":"","title":"security.yaml"},{"location":"allstar/#purpose_4","text":"This policy checks that the repository has a security policy file in SECURITY.md and it is not empty.","title":"Purpose"},{"location":"backstage-update/","text":"Automated Backstage Update \u00b6 Backstage Update Workflow file Update Backstage Workflow Jobs \u00b6 check-for-existing-update \u00b6 Description: Before upgrade process begins, this job checks if there is currently an open pull request to update backstage. 1 2 3 4 5 6 7 Steps: - Checkout - Uses: [actions/checkout@v2](https://github.com/actions/checkout) - Checkout github repository so workflow can access it - Check for existing auto-update PR - Uses command line to make cURL request to GitHub API for all open pull requests using the \"auto-update-backstage\" branch - Stores url to open pull request as output link-pr \u00b6 Description: If there is an open pull request to update backstage, this job will output a link to the open pull request. 1 2 3 4 Steps: - Failed to Update Backstage - Uses: [actions/github-script@v3](https://github.com/actions/github-script) - Outputs the open pull request url retrieved from GitHub API request update-backstage \u00b6 Description: If there is no open pull request to update backstage, this job will create a branch, perform the upgrade process, and create a pull request with the new changes. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Steps: - Checkout - Uses: [actions/checkout@v2](https://github.com/actions/checkout) - Checkout github repository so workflow can access it - Install Dependencies - Uses yarn install to install dependencies for backstage upgrade process - Update Backstage - Runs npm script \"backstage-update\" which runs \"yarn backstage-cli versions:bump 2>&1 | tee backstage-update-log.txt\" - Compare Create App versions - Uses command line to get current version of \"@backstage/create-app\" from package.json and most recent version from NPM registry - Format PR body - Uses command line to format a summary of the update log to the pull request body - Set output variables - Sets variables for PR title and current date - Create Pull Request - Uses: [peter-evans/create-pull-request@v3](https://github.com/peter-evans/create-pull-request) - Creates pull request containing the changes from running \"backstage-update\" - Pull request body also contains a summary of the update log, outputs from the error log, current and most recent versions of \"@backstage/create-app\" and a link to the create-app changelog - Check outputs - Logs the number and url of the newly generated Pull Request","title":"Backstage Update"},{"location":"backstage-update/#automated-backstage-update","text":"Backstage Update Workflow file","title":"Automated Backstage Update"},{"location":"backstage-update/#update-backstage-workflow-jobs","text":"","title":"Update Backstage Workflow Jobs"},{"location":"backstage-update/#check-for-existing-update","text":"Description: Before upgrade process begins, this job checks if there is currently an open pull request to update backstage. 1 2 3 4 5 6 7 Steps: - Checkout - Uses: [actions/checkout@v2](https://github.com/actions/checkout) - Checkout github repository so workflow can access it - Check for existing auto-update PR - Uses command line to make cURL request to GitHub API for all open pull requests using the \"auto-update-backstage\" branch - Stores url to open pull request as output","title":"check-for-existing-update"},{"location":"backstage-update/#link-pr","text":"Description: If there is an open pull request to update backstage, this job will output a link to the open pull request. 1 2 3 4 Steps: - Failed to Update Backstage - Uses: [actions/github-script@v3](https://github.com/actions/github-script) - Outputs the open pull request url retrieved from GitHub API request","title":"link-pr"},{"location":"backstage-update/#update-backstage","text":"Description: If there is no open pull request to update backstage, this job will create a branch, perform the upgrade process, and create a pull request with the new changes. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Steps: - Checkout - Uses: [actions/checkout@v2](https://github.com/actions/checkout) - Checkout github repository so workflow can access it - Install Dependencies - Uses yarn install to install dependencies for backstage upgrade process - Update Backstage - Runs npm script \"backstage-update\" which runs \"yarn backstage-cli versions:bump 2>&1 | tee backstage-update-log.txt\" - Compare Create App versions - Uses command line to get current version of \"@backstage/create-app\" from package.json and most recent version from NPM registry - Format PR body - Uses command line to format a summary of the update log to the pull request body - Set output variables - Sets variables for PR title and current date - Create Pull Request - Uses: [peter-evans/create-pull-request@v3](https://github.com/peter-evans/create-pull-request) - Creates pull request containing the changes from running \"backstage-update\" - Pull request body also contains a summary of the update log, outputs from the error log, current and most recent versions of \"@backstage/create-app\" and a link to the create-app changelog - Check outputs - Logs the number and url of the newly generated Pull Request","title":"update-backstage"},{"location":"cypress-e2e-tests/","text":"Cypress e2e Tests ADR \u00b6 Decision Made: yes Decision Date: 08/2021 Revisit criteria: Decision Made: Yes Revisit Decision: Not planned, but if another e2e testing frameworks become available. Revisit Criteria: If another e2e testing frameworks becomes available that is faster, or performs better, we should revisit this. Decision Makers: @KaemonIsland tl;dr \u00b6 Backstage should have e2e testing to ensure that various dependencies of an application are working accurately. Along with allowing refactoring, and bug fixing to be much faster and easier. There are various e2e testing frameworks available and we chose Cypress. History \u00b6 Backstage came with Cypress already installed . So it was easy to get started. Testing locally was a bit weird (Ran into an issue with missing dependencies). For Linux users, or those using WSL. You can install all of the Cypress dependencies by following this guide. Tests are also run as part of the CI/CD. Here is the PR adding them. Pros \u00b6 It's open source. Allows End-to-end, Integration, and Unit tests. Time Travel - Takes snapshots of tests. Debuggability - Debugging can be done using the Developer tools. Cross Browser Testing - Run tests on Firefox, chrome, edge, etc. Stubbing responses. Cons \u00b6 Can have difficulty setting up. (I've had trouble on multiple systems getting Cypress to run locally) Uses synthetic events instead of native ones. Stubbing can get out of hand fairly quickly. Bad support for iFrames. Usually have to use custom code. Decision \u00b6 Backstage came with Cypress installed and most of the current developers have had an enjoyable experience working with it. So we've decided to stick with it.","title":"Cypress e2e Tests(ADR)"},{"location":"cypress-e2e-tests/#cypress-e2e-tests-adr","text":"Decision Made: yes Decision Date: 08/2021 Revisit criteria: Decision Made: Yes Revisit Decision: Not planned, but if another e2e testing frameworks become available. Revisit Criteria: If another e2e testing frameworks becomes available that is faster, or performs better, we should revisit this. Decision Makers: @KaemonIsland","title":"Cypress e2e Tests ADR"},{"location":"cypress-e2e-tests/#tldr","text":"Backstage should have e2e testing to ensure that various dependencies of an application are working accurately. Along with allowing refactoring, and bug fixing to be much faster and easier. There are various e2e testing frameworks available and we chose Cypress.","title":"tl;dr"},{"location":"cypress-e2e-tests/#history","text":"Backstage came with Cypress already installed . So it was easy to get started. Testing locally was a bit weird (Ran into an issue with missing dependencies). For Linux users, or those using WSL. You can install all of the Cypress dependencies by following this guide. Tests are also run as part of the CI/CD. Here is the PR adding them.","title":"History"},{"location":"cypress-e2e-tests/#pros","text":"It's open source. Allows End-to-end, Integration, and Unit tests. Time Travel - Takes snapshots of tests. Debuggability - Debugging can be done using the Developer tools. Cross Browser Testing - Run tests on Firefox, chrome, edge, etc. Stubbing responses.","title":"Pros"},{"location":"cypress-e2e-tests/#cons","text":"Can have difficulty setting up. (I've had trouble on multiple systems getting Cypress to run locally) Uses synthetic events instead of native ones. Stubbing can get out of hand fairly quickly. Bad support for iFrames. Usually have to use custom code.","title":"Cons"},{"location":"cypress-e2e-tests/#decision","text":"Backstage came with Cypress installed and most of the current developers have had an enjoyable experience working with it. So we've decided to stick with it.","title":"Decision"},{"location":"deployment/","text":"Automated workflows and infrastructure (WIP) \u00b6 This is a draft and does not represent current state. Application deployment \u00b6 TechDocs publication \u00b6 Jenkins environment variables \u00b6 Name Description TECHDOCS_S3_BUCKET_NAME AWS_ACCESS_KEY_ID AWS IAM user AWS_SECRET_ACCESS_KEY AWS_REGION Minimum IAM user access policy \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [ \"s3:PutObject\", ], \"Resource\": [ \"arn:aws:s3:::TECHDOCS_S3_BUCKET_NAME/*\", \"arn:aws:s3:::TECHDOCS_S3_BUCKET_NAME\" ] } ] } Backstage backend components \u00b6 Backstage backend container environment variables \u00b6 Name Description Privileges, permissions GITHUB_TOKEN GitHub Personal Access Token admin:org:read:org, user:read:user AUTH_GITHUB_CLIENT_ID GitHub OAuth AUTH_GITHUB_CLIENT_SECRET TECHDOCS_S3_BUCKET_NAME AWS_ACCESS_KEY_ID AWS IAM user AWS_SECRET_ACCESS_KEY AWS_REGION POSTGRES_USER PostgreSQL instance user SELECT, INSERT, UPDATE, DELETE, TRUNCATE, CREATE, CONNECT POSTGRES_HOST POSTGRES_PORT POSTGRES_PASSWORD Minimum IAM user policy \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [ \"s3:GetObject\", \"s3:ListBucket\" ], \"Resource\": [ \"arn:aws:s3:::TECHDOCS_S3_BUCKET_NAME/*\", \"arn:aws:s3:::TECHDOCS_S3_BUCKET_NAME\" ] } ] } Backstage frontend components \u00b6","title":"Deployment"},{"location":"deployment/#automated-workflows-and-infrastructure-wip","text":"This is a draft and does not represent current state.","title":"Automated workflows and infrastructure (WIP)"},{"location":"deployment/#application-deployment","text":"","title":"Application deployment"},{"location":"deployment/#techdocs-publication","text":"","title":"TechDocs publication"},{"location":"deployment/#jenkins-environment-variables","text":"Name Description TECHDOCS_S3_BUCKET_NAME AWS_ACCESS_KEY_ID AWS IAM user AWS_SECRET_ACCESS_KEY AWS_REGION","title":"Jenkins environment variables"},{"location":"deployment/#minimum-iam-user-access-policy","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [ \"s3:PutObject\", ], \"Resource\": [ \"arn:aws:s3:::TECHDOCS_S3_BUCKET_NAME/*\", \"arn:aws:s3:::TECHDOCS_S3_BUCKET_NAME\" ] } ] }","title":"Minimum IAM user access policy"},{"location":"deployment/#backstage-backend-components","text":"","title":"Backstage backend components"},{"location":"deployment/#backstage-backend-container-environment-variables","text":"Name Description Privileges, permissions GITHUB_TOKEN GitHub Personal Access Token admin:org:read:org, user:read:user AUTH_GITHUB_CLIENT_ID GitHub OAuth AUTH_GITHUB_CLIENT_SECRET TECHDOCS_S3_BUCKET_NAME AWS_ACCESS_KEY_ID AWS IAM user AWS_SECRET_ACCESS_KEY AWS_REGION POSTGRES_USER PostgreSQL instance user SELECT, INSERT, UPDATE, DELETE, TRUNCATE, CREATE, CONNECT POSTGRES_HOST POSTGRES_PORT POSTGRES_PASSWORD","title":"Backstage backend container environment variables"},{"location":"deployment/#minimum-iam-user-policy","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [ \"s3:GetObject\", \"s3:ListBucket\" ], \"Resource\": [ \"arn:aws:s3:::TECHDOCS_S3_BUCKET_NAME/*\", \"arn:aws:s3:::TECHDOCS_S3_BUCKET_NAME\" ] } ] }","title":"Minimum IAM user policy"},{"location":"deployment/#backstage-frontend-components","text":"","title":"Backstage frontend components"},{"location":"local-k8s/","text":"Deploying Backstage to local Kubernetes Cluster \u00b6 Using minikube and Kubernetes \u00b6 Prerequisites Install Minikube Install Docker Desktop: Mac , Windows Install Kubectl Configure app-config.yaml for postgres service \u00b6 1 2 3 4 5 connection: host: ${POSTGRES_SERVICE_HOST} port: ${POSTGRES_SERVICE_PORT} user: ${POSTGRES_USER} password: ${POSTGRES_PASSWORD} Create images for minikube \u00b6 Note: if you already have images in a container registery it is much quicker to use those directly, but if you're making changes locally you'll need to rebuild the images with your new changes and pass those to minikube. Modify .dockerignore \u00b6 Change .dockerignore to only include these lines: 1 2 node_modules packages/*/node_modules Build image for backend container \u00b6 1 yarn build && yarn build-image Build image for frontend container \u00b6 1 docker build . -f Dockerfile.dockerbuild --tag frontend Start up minikube \u00b6 Note: Need to have Docker running for minikube to be able to use docker driver 1 minikube start --driver=docker Load Images for minikube \u00b6 Note: Only need to do this if you are rebuilding images locally Check the image name or image ID you want to load to minikube with: 1 docker images Load the images with: 1 minikube image load <IMAGE_NAME or IMAGE_ID> Optional: you can verify minikube has access to the local image by checking its docker-env by opening a new terminal and running the commands below. If you don't see the image in the list, then you'll need to load it again or wait for it to finish loading. 1 eval $(minikube docker-env) && docker images When you're done, be sure to close this terminal or keep track of which terminal this is so you don't accidentally run commands using the docker daemon inside of the VM minikube uses. View K8s Dashboard \u00b6 Open a new terminal and run: 1 minikube dashboard Configure Kubernetes Secrets \u00b6 First you must encode the token value with base64: 1 echo -n \"token_string_here\" | base64 Copy the encoded value and edit /k8s/backstage-secrets.yaml to include the encoded token value: 1 GITHUB_TOKEN: <BASE64_ENCODED_GITHUB_TOKEN> Create Kubernetes Deployments/Services \u00b6 To create your Kubernetes deployments and services run: 1 kubectl apply -f k8s Now you can check your deployments, pods, and services using the minikube dashboard. Expose and View the Application with minikube tunnel \u00b6 Open a new terminal and start minikube tunnel 1 $ minikube tunnel In a separate terminal, check the service's external ip 1 2 3 4 $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE backstage-svc LoadBalancer 10.96.231.234 127.0.0.1 3000:30197/TCP,7000:30343/TCP 3s postgres ClusterIP 10.108.231.136 <none> 5432/TCP 2m6s View the application running at http://127.0.0.1:3000 Caveats This deployment creates 2 pods: one pod has a single container for the postgres db, and the second pod has 2 containers: one container for the frontend and another container for the backend. Right now with this configuration, backstage-svc is created as a LoadBalancer service so it is designated its own IP address that can be exposed with minikube tunnel . Running the command minikube tunnel creates a network route from the host to the service CIDR of the cluster using the cluster's IP address as a gateway. This exposes the external IP directly to any process running on the host operating system. Running Pulumi to Deploy FE to Kubernetes \u00b6 Through Codespaces \u00b6 Switch .devcontainer with .pulumidevcontainer Run yarn swap-codespaces from the root directory to swap devcontainers If a .pulumidevcontainer is present, this script will rename .devcontainer to .devcontainer_main and rename .pulumidevcontainer to .devcontainer If there is no .pulumidevcontainer , then the script will rename .devcontainer to .pulumidevcontainer and rename .devcontainer_main to .devcontainer Note: Changing the configuration of the Codespaces container will require the Codespaces container to be rebuilt. Rebuilding your container will delete any current changes you've made to the container Rebuild the Codespaces container Use the Command Palette or the Codespaces extension to rebuild the container with the new .devcontainer Start Up Pulumi Run yarn pulumi from the root directory to use the Pulumi start up script This script will check if minikube is running, check if a frontend image is created and create a frontend image if it doesn't exist. Finally, the script will run the pulumi start up commands to generate & apply k8s manifest files and then forward ports with kubectl After the script finishes, you should be able to see the FE running at localhost:3000 Note: this will require creating a Pulumi account","title":"Deploy Local K8s Cluster"},{"location":"local-k8s/#deploying-backstage-to-local-kubernetes-cluster","text":"","title":"Deploying Backstage to local Kubernetes Cluster"},{"location":"local-k8s/#using-minikube-and-kubernetes","text":"Prerequisites Install Minikube Install Docker Desktop: Mac , Windows Install Kubectl","title":"Using minikube and Kubernetes"},{"location":"local-k8s/#configure-app-configyaml-for-postgres-service","text":"1 2 3 4 5 connection: host: ${POSTGRES_SERVICE_HOST} port: ${POSTGRES_SERVICE_PORT} user: ${POSTGRES_USER} password: ${POSTGRES_PASSWORD}","title":"Configure app-config.yaml for postgres service"},{"location":"local-k8s/#create-images-for-minikube","text":"Note: if you already have images in a container registery it is much quicker to use those directly, but if you're making changes locally you'll need to rebuild the images with your new changes and pass those to minikube.","title":"Create images for minikube"},{"location":"local-k8s/#modify-dockerignore","text":"Change .dockerignore to only include these lines: 1 2 node_modules packages/*/node_modules","title":"Modify .dockerignore"},{"location":"local-k8s/#build-image-for-backend-container","text":"1 yarn build && yarn build-image","title":"Build image for backend container"},{"location":"local-k8s/#build-image-for-frontend-container","text":"1 docker build . -f Dockerfile.dockerbuild --tag frontend","title":"Build image for frontend container"},{"location":"local-k8s/#start-up-minikube","text":"Note: Need to have Docker running for minikube to be able to use docker driver 1 minikube start --driver=docker","title":"Start up minikube"},{"location":"local-k8s/#load-images-for-minikube","text":"Note: Only need to do this if you are rebuilding images locally Check the image name or image ID you want to load to minikube with: 1 docker images Load the images with: 1 minikube image load <IMAGE_NAME or IMAGE_ID> Optional: you can verify minikube has access to the local image by checking its docker-env by opening a new terminal and running the commands below. If you don't see the image in the list, then you'll need to load it again or wait for it to finish loading. 1 eval $(minikube docker-env) && docker images When you're done, be sure to close this terminal or keep track of which terminal this is so you don't accidentally run commands using the docker daemon inside of the VM minikube uses.","title":"Load Images for minikube"},{"location":"local-k8s/#view-k8s-dashboard","text":"Open a new terminal and run: 1 minikube dashboard","title":"View K8s Dashboard"},{"location":"local-k8s/#configure-kubernetes-secrets","text":"First you must encode the token value with base64: 1 echo -n \"token_string_here\" | base64 Copy the encoded value and edit /k8s/backstage-secrets.yaml to include the encoded token value: 1 GITHUB_TOKEN: <BASE64_ENCODED_GITHUB_TOKEN>","title":"Configure Kubernetes Secrets"},{"location":"local-k8s/#create-kubernetes-deploymentsservices","text":"To create your Kubernetes deployments and services run: 1 kubectl apply -f k8s Now you can check your deployments, pods, and services using the minikube dashboard.","title":"Create Kubernetes Deployments/Services"},{"location":"local-k8s/#expose-and-view-the-application-with-minikube-tunnel","text":"Open a new terminal and start minikube tunnel 1 $ minikube tunnel In a separate terminal, check the service's external ip 1 2 3 4 $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE backstage-svc LoadBalancer 10.96.231.234 127.0.0.1 3000:30197/TCP,7000:30343/TCP 3s postgres ClusterIP 10.108.231.136 <none> 5432/TCP 2m6s View the application running at http://127.0.0.1:3000 Caveats This deployment creates 2 pods: one pod has a single container for the postgres db, and the second pod has 2 containers: one container for the frontend and another container for the backend. Right now with this configuration, backstage-svc is created as a LoadBalancer service so it is designated its own IP address that can be exposed with minikube tunnel . Running the command minikube tunnel creates a network route from the host to the service CIDR of the cluster using the cluster's IP address as a gateway. This exposes the external IP directly to any process running on the host operating system.","title":"Expose and View the Application with minikube tunnel"},{"location":"local-k8s/#running-pulumi-to-deploy-fe-to-kubernetes","text":"","title":"Running Pulumi to Deploy FE to Kubernetes"},{"location":"local-k8s/#through-codespaces","text":"Switch .devcontainer with .pulumidevcontainer Run yarn swap-codespaces from the root directory to swap devcontainers If a .pulumidevcontainer is present, this script will rename .devcontainer to .devcontainer_main and rename .pulumidevcontainer to .devcontainer If there is no .pulumidevcontainer , then the script will rename .devcontainer to .pulumidevcontainer and rename .devcontainer_main to .devcontainer Note: Changing the configuration of the Codespaces container will require the Codespaces container to be rebuilt. Rebuilding your container will delete any current changes you've made to the container Rebuild the Codespaces container Use the Command Palette or the Codespaces extension to rebuild the container with the new .devcontainer Start Up Pulumi Run yarn pulumi from the root directory to use the Pulumi start up script This script will check if minikube is running, check if a frontend image is created and create a frontend image if it doesn't exist. Finally, the script will run the pulumi start up commands to generate & apply k8s manifest files and then forward ports with kubectl After the script finishes, you should be able to see the FE running at localhost:3000 Note: this will require creating a Pulumi account","title":"Through Codespaces"},{"location":"rfc-catalog-import-research/","text":"[RFC] Catalog Import Research \u00b6 Summary : Users can add GitHub and GitHub Enterprise repositories to the backstage catalog. We need to also check if it's possible to add repositories from other sources like GitLab. Background \u00b6 The GitHub integration supports loading catalog entities from github.com or GitHub Enterprise. Entities can be added to static catalog configuration , registered with the catalog-import plugin, or discovered from a GitHub organization. Users and Groups can also be loaded from an organization . Goal \u00b6 Our goal is to allow multiple repository hosting services, like GitHub/GitLab, to import repositories using the catalog-plugin. Information \u00b6 Integrations \u00b6 Backstage supports integrations from multiple repository hosting websites. GitHub is supported by default. Integrations are required in order to understand how to retrieve a given URL. Adding an integration can be done by editing the app-config.yml file manually, or adding via the backstage API. For Example if we had this location within our app-config.yml file: 1 2 3 4 catalog: locations: - type: url target: https://github.com/backstage/backstage/blob/master/packages/catalog-model/examples/artist-lookup-component.yaml We would also need to list GitHub under integrations. 1 2 3 4 integrations: github: - host: github.com token: ${GITHUB_TOKEN} Example adding a GitLab integration : 1 2 3 4 integrations: gitlab: - host: gitlab.com token: ${GITLAB_TOKEN} Each integration has options configurations. host - The host fo the instance, e.g. github.com token (optional) - An authentication token as expected by the host. IF this is no supplied, anonymous access will be used apiBaseUrl (optional) - The URL fo the GitHub/GitLab API. For self-hosted installations, it is commonly at https://<host>/api/ . For gitlab.com, this configuration is not needed as it can be inferred. baseUrl (optional) - The base URL for this provider, e.g. https://gitlab.com . If this is not provided, it is assumed to be https://<host> Note: You need to supply either apiBaseUrl or rawBaseUrl or both (except for public GitHub, for which we can infer them). The apiBaseUrl will always be preferred over the other if a token is given, otherwise rawBaseUrl will be preferred. Catalog-Plugin \u00b6 catalog-plugin The Catalog Import Plugin provides a wizard to onboard projects with existing catalog-info.yaml files. It also assists by creating pull requests in repositories where no catalog-info.yaml exists. The plugin can import any catalog-info.yml file that is listed within one of our integrations. GitHub Only: The plugin can also search for all catalog-info.yml files along with analyse, generate a Component Entity, and create pull requests. Recommendation \u00b6 It seems like the best idea would be to add GitHub Enterprise and GitLab to the integrations section of app-config.yml . Then, we could provide a way for users to request additional integrations. Otherwise it doesn't look like we will need to make any updates to the catalog-plugin, or to even create our own. The only reason I could think to add our own is if we want special behavior. Other things like titles, descriptions, and other wording can be changed manually. Questions \u00b6 Q: Do we need to make a catalog import plugin to support multiple instances of GitHub? A: No, a catalog-import plugin is already included within backstage. This plugin provides a wizard to onboard projects with existing catalog-info.yml files, or creates a pull request with an example catalog-info.yml if none are found. Integrations from other repositories can be included within the app-config.yml config. Q: How might we support importing catalog entries that aren't in GitHub? A: Other repository hosting services can be added by adding them to integrations within app-config.yml . For example to add support for GitLab we would add: 1 2 3 4 integrations: gitlab: - host: gitlab.com token: ${GITLAB_TOKEN} Q: How does it determine what credentials to use? A: Credentials are based off of the configuration set in app-config.yml or added using the catalog-import plugin. A token can be applied for authentication. Anonymous access will be used if none is applied. Q: Do we need to modify the starter kit UI? A: I don't believe so. Per, Backstage Integrations Integrations are configured at the root level of app-config.yaml since integrations are used by many Backstage core features and other plugins. Each key under integrations is a separate configuration for a single external provider. Providers each have different configuration; here's an example of configuration to use both GitHub and Bitbucket: 1 2 3 4 5 6 7 8 integrations: github: - host: github.com token: ${GITHUB_TOKEN} bitbucket: - host: bitbucket.org username: ${BITBUCKET_USERNAME} appPassword: ${BITBUCKET_APP_PASSWORD} Q: Would we have naming conflicts? A: Yes, but only for specific situations. Per the Backstage catalog configuration : When multiple catalog-info.yaml files with the same metadata.name property are discovered, one will be processed and all others will be skipped. This action is logged for further investigation.","title":"[RFC] Catalog Import Research"},{"location":"rfc-catalog-import-research/#rfc-catalog-import-research","text":"Summary : Users can add GitHub and GitHub Enterprise repositories to the backstage catalog. We need to also check if it's possible to add repositories from other sources like GitLab.","title":"[RFC] Catalog Import Research"},{"location":"rfc-catalog-import-research/#background","text":"The GitHub integration supports loading catalog entities from github.com or GitHub Enterprise. Entities can be added to static catalog configuration , registered with the catalog-import plugin, or discovered from a GitHub organization. Users and Groups can also be loaded from an organization .","title":"Background"},{"location":"rfc-catalog-import-research/#goal","text":"Our goal is to allow multiple repository hosting services, like GitHub/GitLab, to import repositories using the catalog-plugin.","title":"Goal"},{"location":"rfc-catalog-import-research/#information","text":"","title":"Information"},{"location":"rfc-catalog-import-research/#integrations","text":"Backstage supports integrations from multiple repository hosting websites. GitHub is supported by default. Integrations are required in order to understand how to retrieve a given URL. Adding an integration can be done by editing the app-config.yml file manually, or adding via the backstage API. For Example if we had this location within our app-config.yml file: 1 2 3 4 catalog: locations: - type: url target: https://github.com/backstage/backstage/blob/master/packages/catalog-model/examples/artist-lookup-component.yaml We would also need to list GitHub under integrations. 1 2 3 4 integrations: github: - host: github.com token: ${GITHUB_TOKEN} Example adding a GitLab integration : 1 2 3 4 integrations: gitlab: - host: gitlab.com token: ${GITLAB_TOKEN} Each integration has options configurations. host - The host fo the instance, e.g. github.com token (optional) - An authentication token as expected by the host. IF this is no supplied, anonymous access will be used apiBaseUrl (optional) - The URL fo the GitHub/GitLab API. For self-hosted installations, it is commonly at https://<host>/api/ . For gitlab.com, this configuration is not needed as it can be inferred. baseUrl (optional) - The base URL for this provider, e.g. https://gitlab.com . If this is not provided, it is assumed to be https://<host> Note: You need to supply either apiBaseUrl or rawBaseUrl or both (except for public GitHub, for which we can infer them). The apiBaseUrl will always be preferred over the other if a token is given, otherwise rawBaseUrl will be preferred.","title":"Integrations"},{"location":"rfc-catalog-import-research/#catalog-plugin","text":"catalog-plugin The Catalog Import Plugin provides a wizard to onboard projects with existing catalog-info.yaml files. It also assists by creating pull requests in repositories where no catalog-info.yaml exists. The plugin can import any catalog-info.yml file that is listed within one of our integrations. GitHub Only: The plugin can also search for all catalog-info.yml files along with analyse, generate a Component Entity, and create pull requests.","title":"Catalog-Plugin"},{"location":"rfc-catalog-import-research/#recommendation","text":"It seems like the best idea would be to add GitHub Enterprise and GitLab to the integrations section of app-config.yml . Then, we could provide a way for users to request additional integrations. Otherwise it doesn't look like we will need to make any updates to the catalog-plugin, or to even create our own. The only reason I could think to add our own is if we want special behavior. Other things like titles, descriptions, and other wording can be changed manually.","title":"Recommendation"},{"location":"rfc-catalog-import-research/#questions","text":"Q: Do we need to make a catalog import plugin to support multiple instances of GitHub? A: No, a catalog-import plugin is already included within backstage. This plugin provides a wizard to onboard projects with existing catalog-info.yml files, or creates a pull request with an example catalog-info.yml if none are found. Integrations from other repositories can be included within the app-config.yml config. Q: How might we support importing catalog entries that aren't in GitHub? A: Other repository hosting services can be added by adding them to integrations within app-config.yml . For example to add support for GitLab we would add: 1 2 3 4 integrations: gitlab: - host: gitlab.com token: ${GITLAB_TOKEN} Q: How does it determine what credentials to use? A: Credentials are based off of the configuration set in app-config.yml or added using the catalog-import plugin. A token can be applied for authentication. Anonymous access will be used if none is applied. Q: Do we need to modify the starter kit UI? A: I don't believe so. Per, Backstage Integrations Integrations are configured at the root level of app-config.yaml since integrations are used by many Backstage core features and other plugins. Each key under integrations is a separate configuration for a single external provider. Providers each have different configuration; here's an example of configuration to use both GitHub and Bitbucket: 1 2 3 4 5 6 7 8 integrations: github: - host: github.com token: ${GITHUB_TOKEN} bitbucket: - host: bitbucket.org username: ${BITBUCKET_USERNAME} appPassword: ${BITBUCKET_APP_PASSWORD} Q: Would we have naming conflicts? A: Yes, but only for specific situations. Per the Backstage catalog configuration : When multiple catalog-info.yaml files with the same metadata.name property are discovered, one will be processed and all others will be skipped. This action is logged for further investigation.","title":"Questions"},{"location":"rfc-ci-cd/","text":"[RFC] Modifications to existing CI/CD process \u00b6 Summary : We are currently in the process of building our CI/CD pipeline. We know we need to make modifications to our existing CI/CD process to incorporate changesets for releases and also to use the Lightkeeper CLI for creating Kubernetes clusters. Background \u00b6 The process of integrating and delivering new changes for an application from source code to a production environment requires a multi-staged process to build, test, and deploy each new release. Automating the process of building, testing, and deploying an application is known as a Continuous Integration/Continuous Delivery, or CI/CD, pipeline. Utilizing a CI/CD pipeline is the preferred method for delivering new releases because it accelerates the process while reducing errors and allows the use a variety of tests and tools to generate feedback for the software development lifecycle. There is not a single way to structure a CI/CD Pipeline, but the intent is to use a multistaged process that creates a feedback loop to drive the development cycle forward. Goal \u00b6 Our goal is to build an optimized CI/CD pipeline that produces fast, accurate, reliable, and comprehensive feedback to expedite our development cycle. We plan to achieve this goal by implementing automation tools and workflows that are triggered by merges pushed to the \"main\" branch of our Lighthouse Backstage repository. Note: Is this an accurate description of our goals relating to creating a CI/CD pipeline? Are there other goals I should include? Sequence Diagram of CI/CD Pipeline (WIP) \u00b6 This is a rough sequence diagram of the CI/CD pipeline to illustrate the role of each stage and how it can provide feedback to the developer in the event of an error at some stage in the process. Note: Still a work in progress and needs more steps or more stages to reflect multiple deployment environments Source Code \u00b6 Merging to the \"main\" branch initiates the CI/CD Pipeline so those changes can be deployed to the production environment. The \"Release\" Github action will automatically generate changesets and create a new release. We can extend this workflow to include different types of releases like prereleases and to store images for the frontend and backend containers. Build Stage \u00b6 This stage is responsible for building the images for our application and storing these images in a container registry. It is important that we build only once during the CI/CD process to reduce time/resources because the build process is lengthy. Once an image is built, subsequent steps of the CI/CD process that require an image can utilize the already built artifact instead of spending time/resources on rebuilding the image. Testing Stage \u00b6 This stage is responsible for running a variety of automated tests and checks to catch potential errors early and provide feedback to the developer. Deployment Stage \u00b6 This stage is responsible for deploying our application as a Kubernetes cluster to a cloud server. We plan to use Lightkeeper CLI to generate the assets for our Kubernetes cluster. Right now the Lightkeeper CLI requires a login to use that must be done manually through the browser.","title":"RFC CI/CD Modifications"},{"location":"rfc-ci-cd/#rfc-modifications-to-existing-cicd-process","text":"Summary : We are currently in the process of building our CI/CD pipeline. We know we need to make modifications to our existing CI/CD process to incorporate changesets for releases and also to use the Lightkeeper CLI for creating Kubernetes clusters.","title":"[RFC] Modifications to existing CI/CD process"},{"location":"rfc-ci-cd/#background","text":"The process of integrating and delivering new changes for an application from source code to a production environment requires a multi-staged process to build, test, and deploy each new release. Automating the process of building, testing, and deploying an application is known as a Continuous Integration/Continuous Delivery, or CI/CD, pipeline. Utilizing a CI/CD pipeline is the preferred method for delivering new releases because it accelerates the process while reducing errors and allows the use a variety of tests and tools to generate feedback for the software development lifecycle. There is not a single way to structure a CI/CD Pipeline, but the intent is to use a multistaged process that creates a feedback loop to drive the development cycle forward.","title":"Background"},{"location":"rfc-ci-cd/#goal","text":"Our goal is to build an optimized CI/CD pipeline that produces fast, accurate, reliable, and comprehensive feedback to expedite our development cycle. We plan to achieve this goal by implementing automation tools and workflows that are triggered by merges pushed to the \"main\" branch of our Lighthouse Backstage repository. Note: Is this an accurate description of our goals relating to creating a CI/CD pipeline? Are there other goals I should include?","title":"Goal"},{"location":"rfc-ci-cd/#sequence-diagram-of-cicd-pipeline-wip","text":"This is a rough sequence diagram of the CI/CD pipeline to illustrate the role of each stage and how it can provide feedback to the developer in the event of an error at some stage in the process. Note: Still a work in progress and needs more steps or more stages to reflect multiple deployment environments","title":"Sequence Diagram of CI/CD Pipeline (WIP)"},{"location":"rfc-ci-cd/#source-code","text":"Merging to the \"main\" branch initiates the CI/CD Pipeline so those changes can be deployed to the production environment. The \"Release\" Github action will automatically generate changesets and create a new release. We can extend this workflow to include different types of releases like prereleases and to store images for the frontend and backend containers.","title":"Source Code"},{"location":"rfc-ci-cd/#build-stage","text":"This stage is responsible for building the images for our application and storing these images in a container registry. It is important that we build only once during the CI/CD process to reduce time/resources because the build process is lengthy. Once an image is built, subsequent steps of the CI/CD process that require an image can utilize the already built artifact instead of spending time/resources on rebuilding the image.","title":"Build Stage"},{"location":"rfc-ci-cd/#testing-stage","text":"This stage is responsible for running a variety of automated tests and checks to catch potential errors early and provide feedback to the developer.","title":"Testing Stage"},{"location":"rfc-ci-cd/#deployment-stage","text":"This stage is responsible for deploying our application as a Kubernetes cluster to a cloud server. We plan to use Lightkeeper CLI to generate the assets for our Kubernetes cluster. Right now the Lightkeeper CLI requires a login to use that must be done manually through the browser.","title":"Deployment Stage"},{"location":"rfc-doc-generator/","text":"[RFC] Documentation Generator \u00b6 Summary : Allow documentation to be generated from code comments. Most documentation generators have a fairly similar syntax. So the main purpose of this RFC is to help decide between which syntax we would like more. I was able to find two generators that I think we should choose between, JSDoc and TypeDoc. Both of these generators have eslint-plugins that would allow us to require comments, and other nitpicking. They also allow us to generate documentation within GitHub Actions which is great. Like I said, the main difference is how the syntax will work with TypeScript. Note: Please feel free to let me know if there is another doc generator that you think we should consider. Background \u00b6 Why do we need a documentation generator? I think I speak for everyone when I say that documenting everything can become a huge pain and can take up time a lot of our time. A documentation generator can take our commented code and build a good looking HTML website, so long as we make good explanatory comments. Automating documentation is just another thing that can save us a ton of time and allow us to work on more important/fun features. Goal \u00b6 To decide on which documentation generator, JSDoc or TypeDoc, should be used when adding comments to the codebase. Similar to the summary both generators have similar syntax, eslint support, and GitHub Actions. The only real decision to make is which syntax we'd rather use. Generating documentation \u00b6 Both doc generators are able to run with a single CLI command. They can take inline arguments, or use a config file to determine where to look for files, and where it should output the generated documentation. For example if we want to generate docs from src using a specified config and then putting the generated docs into a docs folder. JSDoc can have plugins or other settings added to a jsdoc.json file as well. JSDoc -> /path/to/jsdoc src -r -c /path/to/my/conf.json -d docs Typedoc can use most of the TypeScript compiler options as well as the tsconfig file. TypeDoc -> typedoc --out docs src JSDoc \u00b6 JSDoc GitHub JSDoc + TypeScript Playground JSDoc has a lot of support and is quite popular. Documenting code can be incredibly easy and provide a lot of information about what's going on in a file. It's been a great way to add typing to JavaScript without using TypeScript. Supported Types: @type @param (or @arg or @argument) @returns (or @return) @typedef @callback @template @class (or @constructor) @this @extends (or @augments) @enum @deprecated In JSDoc types are listed within the comment. 1 2 3 4 5 6 7 8 9 10 11 12 13 /** @type {string} */ let s ; /** * @param {string} p1 - A string param. * @param {string=} p2 - An optional param (Closure syntax) * @param {string} [p3] - Another optional param (JSDoc syntax). * @param {string} [p4=\"test\"] - An optional param with a default value * @return {string} This is the result */ function stringsStringStrings ( p1 , p2 , p3 , p4 ) { // TODO } Unfortunately, JSDoc doesn't support TypeScript out of the box. There are some plugins like better-docs that can parse .ts | .tsx files. However, types aren't displayed correctly within the generated documentation. Meaning that we'd have to list out types within the comments and within the code. There is a way around it (I've done it before) but it requires some extra time and monkey-patching to get it to work correctly. TypeDoc \u00b6 TypeDoc GitHub It appears that TypeDoc can do everything that JSDoc can, but with out of the box support for TypeScript files. It also uses the tsconfig file to figure out where it should look for .ts | .tsx files. All comments are parsed as Markdown which adds additional styling for us! TypeDoc doesn't support all the tags that JSDoc does, but that's because it infers more information from the TypeScript code. So you are more than welcome to type regular TypeScript without needing to declare types within comments. 1 2 3 4 /** * @param text Comment for parameter \u00b4text\u00b4. */ function doSomething ( target : any , text : string ) : number ; Supported Tags: @param <param name> @typeParam <param name> or @template <param name> @return(s) @event @hidden and @ignore @internal @category @module @typedef, @callback @public, @protected, and @private TypeDoc can also support .js | .jsx files if the allowJS option is set to true in the tsconfig . Just note that it can't derive the type from the code so they would need to be explicitly stated within the comment. Recommendation \u00b6 Considering everything that we would want for a documentation generator. Linting Syntax Highlighting Doc Generation with CI/CD JavaScript/TypeScript Support Tags I believe that TypeDoc is the best choice considering it supports TypeScript out of the box. The syntax isn't very different from JSDoc, we just omit the types after the @param and @returns because it's written within the code! Techdocs \u00b6 We can specify another documentation folder so that we don't conflict with TechDocs. The only issue will be that GitHub Pages can only have one site per project. I don't think that there is a way for us to merge Techdocs + another site. We could possibly setup an account with Netlify or Heroku to publish the generated documentation. Or open another repository which we push to using GitHub Actions? eslint enforcement \u00b6 In my experience, I'm used to enforcing comments on All functions, just to describe the purpose, params, and return values. It helps devs to quickly understand how to use the function. It also helps to reveal if the function is doing too much. Anything else like Types, Interfaces, Variables have felt pretty self explanatory. When something does become convoluted, or can be confusing I'll usually add a comment explaining it's purpose. Errors Classes (Which should be avoided) Required for all Functions and include: A short description All parameters (If applicable) Returns (If applicable) Anything else is optional for me. We could include warnings for Types and Interfaces?","title":"RFC Doc Generator"},{"location":"rfc-doc-generator/#rfc-documentation-generator","text":"Summary : Allow documentation to be generated from code comments. Most documentation generators have a fairly similar syntax. So the main purpose of this RFC is to help decide between which syntax we would like more. I was able to find two generators that I think we should choose between, JSDoc and TypeDoc. Both of these generators have eslint-plugins that would allow us to require comments, and other nitpicking. They also allow us to generate documentation within GitHub Actions which is great. Like I said, the main difference is how the syntax will work with TypeScript. Note: Please feel free to let me know if there is another doc generator that you think we should consider.","title":"[RFC] Documentation Generator"},{"location":"rfc-doc-generator/#background","text":"Why do we need a documentation generator? I think I speak for everyone when I say that documenting everything can become a huge pain and can take up time a lot of our time. A documentation generator can take our commented code and build a good looking HTML website, so long as we make good explanatory comments. Automating documentation is just another thing that can save us a ton of time and allow us to work on more important/fun features.","title":"Background"},{"location":"rfc-doc-generator/#goal","text":"To decide on which documentation generator, JSDoc or TypeDoc, should be used when adding comments to the codebase. Similar to the summary both generators have similar syntax, eslint support, and GitHub Actions. The only real decision to make is which syntax we'd rather use.","title":"Goal"},{"location":"rfc-doc-generator/#generating-documentation","text":"Both doc generators are able to run with a single CLI command. They can take inline arguments, or use a config file to determine where to look for files, and where it should output the generated documentation. For example if we want to generate docs from src using a specified config and then putting the generated docs into a docs folder. JSDoc can have plugins or other settings added to a jsdoc.json file as well. JSDoc -> /path/to/jsdoc src -r -c /path/to/my/conf.json -d docs Typedoc can use most of the TypeScript compiler options as well as the tsconfig file. TypeDoc -> typedoc --out docs src","title":"Generating documentation"},{"location":"rfc-doc-generator/#jsdoc","text":"JSDoc GitHub JSDoc + TypeScript Playground JSDoc has a lot of support and is quite popular. Documenting code can be incredibly easy and provide a lot of information about what's going on in a file. It's been a great way to add typing to JavaScript without using TypeScript. Supported Types: @type @param (or @arg or @argument) @returns (or @return) @typedef @callback @template @class (or @constructor) @this @extends (or @augments) @enum @deprecated In JSDoc types are listed within the comment. 1 2 3 4 5 6 7 8 9 10 11 12 13 /** @type {string} */ let s ; /** * @param {string} p1 - A string param. * @param {string=} p2 - An optional param (Closure syntax) * @param {string} [p3] - Another optional param (JSDoc syntax). * @param {string} [p4=\"test\"] - An optional param with a default value * @return {string} This is the result */ function stringsStringStrings ( p1 , p2 , p3 , p4 ) { // TODO } Unfortunately, JSDoc doesn't support TypeScript out of the box. There are some plugins like better-docs that can parse .ts | .tsx files. However, types aren't displayed correctly within the generated documentation. Meaning that we'd have to list out types within the comments and within the code. There is a way around it (I've done it before) but it requires some extra time and monkey-patching to get it to work correctly.","title":"JSDoc"},{"location":"rfc-doc-generator/#typedoc","text":"TypeDoc GitHub It appears that TypeDoc can do everything that JSDoc can, but with out of the box support for TypeScript files. It also uses the tsconfig file to figure out where it should look for .ts | .tsx files. All comments are parsed as Markdown which adds additional styling for us! TypeDoc doesn't support all the tags that JSDoc does, but that's because it infers more information from the TypeScript code. So you are more than welcome to type regular TypeScript without needing to declare types within comments. 1 2 3 4 /** * @param text Comment for parameter \u00b4text\u00b4. */ function doSomething ( target : any , text : string ) : number ; Supported Tags: @param <param name> @typeParam <param name> or @template <param name> @return(s) @event @hidden and @ignore @internal @category @module @typedef, @callback @public, @protected, and @private TypeDoc can also support .js | .jsx files if the allowJS option is set to true in the tsconfig . Just note that it can't derive the type from the code so they would need to be explicitly stated within the comment.","title":"TypeDoc"},{"location":"rfc-doc-generator/#recommendation","text":"Considering everything that we would want for a documentation generator. Linting Syntax Highlighting Doc Generation with CI/CD JavaScript/TypeScript Support Tags I believe that TypeDoc is the best choice considering it supports TypeScript out of the box. The syntax isn't very different from JSDoc, we just omit the types after the @param and @returns because it's written within the code!","title":"Recommendation"},{"location":"rfc-doc-generator/#techdocs","text":"We can specify another documentation folder so that we don't conflict with TechDocs. The only issue will be that GitHub Pages can only have one site per project. I don't think that there is a way for us to merge Techdocs + another site. We could possibly setup an account with Netlify or Heroku to publish the generated documentation. Or open another repository which we push to using GitHub Actions?","title":"Techdocs"},{"location":"rfc-doc-generator/#eslint-enforcement","text":"In my experience, I'm used to enforcing comments on All functions, just to describe the purpose, params, and return values. It helps devs to quickly understand how to use the function. It also helps to reveal if the function is doing too much. Anything else like Types, Interfaces, Variables have felt pretty self explanatory. When something does become convoluted, or can be confusing I'll usually add a comment explaining it's purpose. Errors Classes (Which should be avoided) Required for all Functions and include: A short description All parameters (If applicable) Returns (If applicable) Anything else is optional for me. We could include warnings for Types and Interfaces?","title":"eslint enforcement"},{"location":"rfc-node-app-logging/","text":"[RFC] Node App Logging \u00b6 Summary : Logging provides numerous benefits to applications. Mainly to help provide visibility into how our applications are running on each of the various infrastructure components. There are three major concerns for choosing a suitable logging library: recording, formatting, and storing messages. We need to make sure that our library of choice addresses all three concerns in a satisfactory manner. Background \u00b6 Backstage comes with Winston already included which is nice as Winston is a popular logger for node applications. The logger is being used within various points of the src/backend . Routes seem to be the only thing being logged at the moment. Backstage initiate's the logger within the makeCreateEnv() function. Whether we decide to use Winston or another logging service ( Bunyan ) we should decouple the various import statements for winston and only use it within a single file. Any file wanting to use the logger should then import from our controlled logger making it easy to manage and/or change in the future. Goal \u00b6 Our goal is to utilize a logger in order to provide a viewpoint into the backstage backend application. This will help to locate errors, catch bugs, and gather general information on how our application is used. We should also decide on how or what should be logged within the application, and what logging levels should be used. Either way logs will need to be output to stdout for easy consumption for services like Datadog . Recommendation \u00b6 Winston is designed to be a simple and universal logging library with support for multiple storage devices. Each logger can have multiple transports configured at different levels. Winston also allows for flexibility and configuration for logging levels, formats, and interpolation of error messages. It's also one of the more popular logging libraries which means that others give it a lot of trust as well. All current instances of importing the logger from backstage should be replaced with an import from a single file that we can name logger.js . This will decouple the logger from our code and make iterations or changing services much easier. For logging levels, and usage I think it'd be best to be as vanilla as possible. Logging levels should conform to the severity ordering specified by RFC5424 : severity of all levels is assumed to be numerically ascending from most important to least important. Logging Levels \u00b6 1 2 3 4 5 6 7 8 9 10 // Default logging levels const levels = { error : 0 , warn : 1 , info : 2 , http : 3 , verbose : 4 , debug : 5 , silly : 6 , }; Logging Level Descriptions ERROR - Represents an error condition in the system that happens to halt a specific operation, but not the overall system. WARN - Indicates runtime conditions that are undesirable or unusual, but not necessarily errors. An example could be using a backup data source when the primary source is unavailable. INFO - Info messages are purely informative. Events that are user-driven or application-specific may be logged at this level. A common use of this level is to log interesting runtime events, such ast he startup or shutdown of a service. DEBUG - Used to represent diagnostic information that may be needed for troubleshooting. Example usage. logger.info(\"Informative Message\") // {\"message\":\"Informative Message\", \"level\":\"info\"} Be Descriptive \u00b6 Log entries should adequately describe the events that they represent. Each message should be unique to the situation and should clearly explain the event that occurred at that point. Bad Example \"Request Failed, will retry.\" Good Example \"POST\" request to \" https://example.com/api \" failed. Response code: \"429\", response message: \"too many requests\". Retrying after \"60\" seconds.","title":"RFC Node App Logging"},{"location":"rfc-node-app-logging/#rfc-node-app-logging","text":"Summary : Logging provides numerous benefits to applications. Mainly to help provide visibility into how our applications are running on each of the various infrastructure components. There are three major concerns for choosing a suitable logging library: recording, formatting, and storing messages. We need to make sure that our library of choice addresses all three concerns in a satisfactory manner.","title":"[RFC] Node App Logging"},{"location":"rfc-node-app-logging/#background","text":"Backstage comes with Winston already included which is nice as Winston is a popular logger for node applications. The logger is being used within various points of the src/backend . Routes seem to be the only thing being logged at the moment. Backstage initiate's the logger within the makeCreateEnv() function. Whether we decide to use Winston or another logging service ( Bunyan ) we should decouple the various import statements for winston and only use it within a single file. Any file wanting to use the logger should then import from our controlled logger making it easy to manage and/or change in the future.","title":"Background"},{"location":"rfc-node-app-logging/#goal","text":"Our goal is to utilize a logger in order to provide a viewpoint into the backstage backend application. This will help to locate errors, catch bugs, and gather general information on how our application is used. We should also decide on how or what should be logged within the application, and what logging levels should be used. Either way logs will need to be output to stdout for easy consumption for services like Datadog .","title":"Goal"},{"location":"rfc-node-app-logging/#recommendation","text":"Winston is designed to be a simple and universal logging library with support for multiple storage devices. Each logger can have multiple transports configured at different levels. Winston also allows for flexibility and configuration for logging levels, formats, and interpolation of error messages. It's also one of the more popular logging libraries which means that others give it a lot of trust as well. All current instances of importing the logger from backstage should be replaced with an import from a single file that we can name logger.js . This will decouple the logger from our code and make iterations or changing services much easier. For logging levels, and usage I think it'd be best to be as vanilla as possible. Logging levels should conform to the severity ordering specified by RFC5424 : severity of all levels is assumed to be numerically ascending from most important to least important.","title":"Recommendation"},{"location":"rfc-node-app-logging/#logging-levels","text":"1 2 3 4 5 6 7 8 9 10 // Default logging levels const levels = { error : 0 , warn : 1 , info : 2 , http : 3 , verbose : 4 , debug : 5 , silly : 6 , }; Logging Level Descriptions ERROR - Represents an error condition in the system that happens to halt a specific operation, but not the overall system. WARN - Indicates runtime conditions that are undesirable or unusual, but not necessarily errors. An example could be using a backup data source when the primary source is unavailable. INFO - Info messages are purely informative. Events that are user-driven or application-specific may be logged at this level. A common use of this level is to log interesting runtime events, such ast he startup or shutdown of a service. DEBUG - Used to represent diagnostic information that may be needed for troubleshooting. Example usage. logger.info(\"Informative Message\") // {\"message\":\"Informative Message\", \"level\":\"info\"}","title":"Logging Levels"},{"location":"rfc-node-app-logging/#be-descriptive","text":"Log entries should adequately describe the events that they represent. Each message should be unique to the situation and should clearly explain the event that occurred at that point. Bad Example \"Request Failed, will retry.\" Good Example \"POST\" request to \" https://example.com/api \" failed. Response code: \"429\", response message: \"too many requests\". Retrying after \"60\" seconds.","title":"Be Descriptive"},{"location":"running-locally/","text":"Open a Codespace (preferred- work in progress) \u00b6 This repo is configured to run a production-like environment in a GitHub Codespace . Open a Codespace Run application: 1 yarn dev Install and run locally with Docker (work in progress) \u00b6 Prerequisites Install git Install Docker Desktop: Mac , Windows Run 1 sh local.sh start After the application runs for the first time, copy node_modules . While the application is running, run this is a separate terminal: 1 sh local.sh copy Caveats What does this do : This will install the application and its dependencies and then run the backend and frontend in separate containers. To ensure fast hot-reloading, node_modules and postgreSQL db are stored in a docker volume and your local source files are mounted into the container. Why do you need to copy after the first run : The application uses node_modules from Docker volume not your local files. Copy these locally so that dependencies resolve correctly in your editor. Environment Variable Injection The local dev environment is setup to use chamber for environment variable injection. A few changes need to be made before this will work. First you'll need to add a .env file within the .localdevcontainer folder. An example.env is available to copy from. Then, you need to uncomment the ENTRYPOINT located at the bottom of the local-Dockerfile, and comment ENTRYPOINT [ \"/entrypoint.sh\" ] . Running sh local.sh.start should now inject any environment variables stored within the SSM Parameter Store based on the values within .env . It'll grab any variables that start with lighthouse-backstage . Install and run locally (TBD) \u00b6 Use nvm to install node You will need to update the Backstage configuration for running locally. Update these instructions if you try this out. app-config.yaml is used for Codespaces and it is merged with app-config.production.yaml in production environments. Supporting Codespaces is the priorty so consider that when changing the way configurations are organized.","title":"Running Locally"},{"location":"running-locally/#open-a-codespace-preferred-work-in-progress","text":"This repo is configured to run a production-like environment in a GitHub Codespace . Open a Codespace Run application: 1 yarn dev","title":"Open a Codespace (preferred- work in progress)"},{"location":"running-locally/#install-and-run-locally-with-docker-work-in-progress","text":"Prerequisites Install git Install Docker Desktop: Mac , Windows Run 1 sh local.sh start After the application runs for the first time, copy node_modules . While the application is running, run this is a separate terminal: 1 sh local.sh copy Caveats What does this do : This will install the application and its dependencies and then run the backend and frontend in separate containers. To ensure fast hot-reloading, node_modules and postgreSQL db are stored in a docker volume and your local source files are mounted into the container. Why do you need to copy after the first run : The application uses node_modules from Docker volume not your local files. Copy these locally so that dependencies resolve correctly in your editor. Environment Variable Injection The local dev environment is setup to use chamber for environment variable injection. A few changes need to be made before this will work. First you'll need to add a .env file within the .localdevcontainer folder. An example.env is available to copy from. Then, you need to uncomment the ENTRYPOINT located at the bottom of the local-Dockerfile, and comment ENTRYPOINT [ \"/entrypoint.sh\" ] . Running sh local.sh.start should now inject any environment variables stored within the SSM Parameter Store based on the values within .env . It'll grab any variables that start with lighthouse-backstage .","title":"Install and run locally with Docker (work in progress)"},{"location":"running-locally/#install-and-run-locally-tbd","text":"Use nvm to install node You will need to update the Backstage configuration for running locally. Update these instructions if you try this out. app-config.yaml is used for Codespaces and it is merged with app-config.production.yaml in production environments. Supporting Codespaces is the priorty so consider that when changing the way configurations are organized.","title":"Install and run locally (TBD)"},{"location":"split-fe-be-adr/","text":"Split Frontend and Backend ADR \u00b6 Decision Made: yes Decision Date: 08/2021 Revisit criteria: Decision Made: Yes Revisit Decision: No Revisit Criteria: None Decision Makers: @KaemonIsland, @rianfowler tl;dr \u00b6 The backstage app will have separate frontend and backend containers in order to scale both individually. i.e. We won't have to create as many frontend instances as backend instances if there is more traffic going to the backend. History \u00b6 Originally, the lighthouse backstage docker images were setup to use a Multi-stage Build. Both the frontend and backend instances were built together, which worked fine however we wouldn't be taking advantage of scaling our application. It doesn't make sense to create more instances of the frontend if the backend is receiving a lot of traffic. Backstage also has a guide to setup a separate frontend that uses NGINX. Pros \u00b6 Frontend and Backend containers can be scaled individually. Easier to work on one without altering the other. Faster deployment Cons \u00b6 The build time for docker compose now takes longer. Decision \u00b6 The Frontend and Backend will run in separate containers in order to allow for individual scaling. PR","title":"Split FE/BE(ADR)"},{"location":"split-fe-be-adr/#split-frontend-and-backend-adr","text":"Decision Made: yes Decision Date: 08/2021 Revisit criteria: Decision Made: Yes Revisit Decision: No Revisit Criteria: None Decision Makers: @KaemonIsland, @rianfowler","title":"Split Frontend and Backend ADR"},{"location":"split-fe-be-adr/#tldr","text":"The backstage app will have separate frontend and backend containers in order to scale both individually. i.e. We won't have to create as many frontend instances as backend instances if there is more traffic going to the backend.","title":"tl;dr"},{"location":"split-fe-be-adr/#history","text":"Originally, the lighthouse backstage docker images were setup to use a Multi-stage Build. Both the frontend and backend instances were built together, which worked fine however we wouldn't be taking advantage of scaling our application. It doesn't make sense to create more instances of the frontend if the backend is receiving a lot of traffic. Backstage also has a guide to setup a separate frontend that uses NGINX.","title":"History"},{"location":"split-fe-be-adr/#pros","text":"Frontend and Backend containers can be scaled individually. Easier to work on one without altering the other. Faster deployment","title":"Pros"},{"location":"split-fe-be-adr/#cons","text":"The build time for docker compose now takes longer.","title":"Cons"},{"location":"split-fe-be-adr/#decision","text":"The Frontend and Backend will run in separate containers in order to allow for individual scaling. PR","title":"Decision"},{"location":"zero-install-adr/","text":"Yarn Zero Install ADR \u00b6 Decision Made: no Decision Date: 00/0000 Revisit Decision: yes Date 09/2021 Revisit criteria: Decision Made: No, but open to revisiting Decision Date: 09/2021 Revisit Decision: Yes, Revisit Date: September 2021 Revisit Criteria: When @backstage/cli replaces their @yarnpkg/lockfile parser usage with an updated package parser, @yarnpkg/parsers . Specifically, this file. Decision Makers: @kaemonisland, @rianfowler tl;dr \u00b6 CI/CD can take up to 10+ minutes to complete, specifically the build-containers action. Being able to use Yarn Zero-Install would allow us to cache dependencies thus lowering the run-time of CI/CD by a significant amount. History \u00b6 Yarn recently released a feature called Zero Install which allows packages to be cached. Yarn states the problem pretty clearly... 1 While Yarn does its best to guarantee that what works now will keep working, there's always the off chance that a future Yarn release will introduce a bug that will prevent you from installing your project. Or maybe your production environments will change and yarn install won't be able to write in the temporary directories anymore. Or maybe the network will fail and your packages won't be available anymore. Or maybe your credentials will rotate and you will start getting authentication issues. Or ... so many things can go wrong, and not all of them are things we can control. Pros \u00b6 Fast Caching packages will allow packages to be installed much faster than the current yarn install . ALso, when updating packages, Yarn 2 will change exactly one file for each updated package. Plug N Play Removes the need for node modules completely. If we were able to get Zero Install + Plug n Play our install time would be almost instantaneous. https://yarnpkg.com/features/pnp#fixing-node_modules Small File Size To give you an idea, a node_modules folder of 135k uncompressed files (for a total of 1.2GB) gives a Yarn cache of 2k binary archives (for a total of 139MB). Git simply cannot support the former, while the latter is perfectly fine. Secure Projects accepting PRs from external users will have to be careful that the PRs affecting the package archives are legit (since it would otherwise be possible to a malicious user to send a PR for a new dependency after having altered its archive content). yarn install --check-cache This way Yarn will re-download the package files from whatever their remote location would be and will report any mismatching checksum. Cons \u00b6 Configuration Zero install can get confusing when configuring zero install for Lerna + TypeScript. Additional configuration settings will need to be applied in order for the linter to recognize the package locations. Plugins Decision \u00b6 Adding Yarn Zero Installs will allow CI/CD and local development to get running faster. Currently, it takes about 2+ minutes to install all the necessary packages by running yarn install . Zero Install should allow us to cut that time down to less than one minute by caching packages. If we decide to also setup Plug N Play, we could cut that time down to almost instantaneous speeds. The only thing that is holding us back is @backstage/cli which uses an incompatible package, @yarnpkg/lockfile , to parse the yarn.lock file.","title":"Yarn Zero Install(ADR)"},{"location":"zero-install-adr/#yarn-zero-install-adr","text":"Decision Made: no Decision Date: 00/0000 Revisit Decision: yes Date 09/2021 Revisit criteria: Decision Made: No, but open to revisiting Decision Date: 09/2021 Revisit Decision: Yes, Revisit Date: September 2021 Revisit Criteria: When @backstage/cli replaces their @yarnpkg/lockfile parser usage with an updated package parser, @yarnpkg/parsers . Specifically, this file. Decision Makers: @kaemonisland, @rianfowler","title":"Yarn Zero Install ADR"},{"location":"zero-install-adr/#tldr","text":"CI/CD can take up to 10+ minutes to complete, specifically the build-containers action. Being able to use Yarn Zero-Install would allow us to cache dependencies thus lowering the run-time of CI/CD by a significant amount.","title":"tl;dr"},{"location":"zero-install-adr/#history","text":"Yarn recently released a feature called Zero Install which allows packages to be cached. Yarn states the problem pretty clearly... 1 While Yarn does its best to guarantee that what works now will keep working, there's always the off chance that a future Yarn release will introduce a bug that will prevent you from installing your project. Or maybe your production environments will change and yarn install won't be able to write in the temporary directories anymore. Or maybe the network will fail and your packages won't be available anymore. Or maybe your credentials will rotate and you will start getting authentication issues. Or ... so many things can go wrong, and not all of them are things we can control.","title":"History"},{"location":"zero-install-adr/#pros","text":"Fast Caching packages will allow packages to be installed much faster than the current yarn install . ALso, when updating packages, Yarn 2 will change exactly one file for each updated package. Plug N Play Removes the need for node modules completely. If we were able to get Zero Install + Plug n Play our install time would be almost instantaneous. https://yarnpkg.com/features/pnp#fixing-node_modules Small File Size To give you an idea, a node_modules folder of 135k uncompressed files (for a total of 1.2GB) gives a Yarn cache of 2k binary archives (for a total of 139MB). Git simply cannot support the former, while the latter is perfectly fine. Secure Projects accepting PRs from external users will have to be careful that the PRs affecting the package archives are legit (since it would otherwise be possible to a malicious user to send a PR for a new dependency after having altered its archive content). yarn install --check-cache This way Yarn will re-download the package files from whatever their remote location would be and will report any mismatching checksum.","title":"Pros"},{"location":"zero-install-adr/#cons","text":"Configuration Zero install can get confusing when configuring zero install for Lerna + TypeScript. Additional configuration settings will need to be applied in order for the linter to recognize the package locations. Plugins","title":"Cons"},{"location":"zero-install-adr/#decision","text":"Adding Yarn Zero Installs will allow CI/CD and local development to get running faster. Currently, it takes about 2+ minutes to install all the necessary packages by running yarn install . Zero Install should allow us to cut that time down to less than one minute by caching packages. If we decide to also setup Plug N Play, we could cut that time down to almost instantaneous speeds. The only thing that is holding us back is @backstage/cli which uses an incompatible package, @yarnpkg/lockfile , to parse the yarn.lock file.","title":"Decision"}]}