{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"example docs \u00b6 This is a basic example of documentation.","title":"Home"},{"location":"#example-docs","text":"This is a basic example of documentation.","title":"example docs"},{"location":"allstar/","text":"Allstar Configuration Files \u00b6 Allstar is a GitHub App ran by OpenSSF that helps set and enforce security policies for GitHub Repositories. Our repository uses a Repository Level Opt In Strategy . This means our repository contains our own .allstar directory to manage our security policies instead of using an organizational level .allstar directory. Inside the .allstar directory are several configuration files that outline our security policies and what Actions to take in the event of a security violation. Actions \u00b6 log : This is the default action, and actually takes place for all actions. All policy run results and details are logged. Logs are currently only visible to the app operator, plans to expose these are under discussion. issue : This action creates a GitHub issue. Only one issue is created per policy, and the text describes the details of the policy violation. If the issue is already open, it is pinged with a comment every 24 hours (not currently user configurable). Once the violation is addressed, the issue will be automatically closed by Allstar within 5-10 minutes. fix : This action is policy specific. The policy will make the changes to the GitHub settings to correct the policy violation. Not all policies will be able to support this (see below). Configuration Files in .allstar Directory \u00b6 allstar.yaml \u00b6 Purpose \u00b6 Configures whether our repository will opt in or opt out of using Allstar app for reporting security violations. Since our organization does not contain a .allstar directory, the default Allstar strategy for all repositories is assumed requiring each repository to opt in to manage security policies. binary_artifacts.yaml \u00b6 Purpose \u00b6 This policy uses check from scorecard . Remove the binary artifact from the repository to achieve compliance. As the scorecard results can be verbose, you may need to run scorecard itself to see all the detailed information. branch_protection.yaml \u00b6 Purpose \u00b6 This policy checks if our repository's branch protection settings match with the branch protection settings outlined in this file. outside.yaml \u00b6 Purpose \u00b6 By default this policy checks that only organizational members have administrative or push access to the repository. security.yaml \u00b6 Purpose \u00b6 This policy checks that the repository has a security policy file in SECURITY.md and it is not empty.","title":"Allstar"},{"location":"allstar/#allstar-configuration-files","text":"Allstar is a GitHub App ran by OpenSSF that helps set and enforce security policies for GitHub Repositories. Our repository uses a Repository Level Opt In Strategy . This means our repository contains our own .allstar directory to manage our security policies instead of using an organizational level .allstar directory. Inside the .allstar directory are several configuration files that outline our security policies and what Actions to take in the event of a security violation.","title":"Allstar Configuration Files"},{"location":"allstar/#actions","text":"log : This is the default action, and actually takes place for all actions. All policy run results and details are logged. Logs are currently only visible to the app operator, plans to expose these are under discussion. issue : This action creates a GitHub issue. Only one issue is created per policy, and the text describes the details of the policy violation. If the issue is already open, it is pinged with a comment every 24 hours (not currently user configurable). Once the violation is addressed, the issue will be automatically closed by Allstar within 5-10 minutes. fix : This action is policy specific. The policy will make the changes to the GitHub settings to correct the policy violation. Not all policies will be able to support this (see below).","title":"Actions"},{"location":"allstar/#configuration-files-in-allstar-directory","text":"","title":"Configuration Files in .allstar Directory"},{"location":"allstar/#allstaryaml","text":"","title":"allstar.yaml"},{"location":"allstar/#purpose","text":"Configures whether our repository will opt in or opt out of using Allstar app for reporting security violations. Since our organization does not contain a .allstar directory, the default Allstar strategy for all repositories is assumed requiring each repository to opt in to manage security policies.","title":"Purpose"},{"location":"allstar/#binary_artifactsyaml","text":"","title":"binary_artifacts.yaml"},{"location":"allstar/#purpose_1","text":"This policy uses check from scorecard . Remove the binary artifact from the repository to achieve compliance. As the scorecard results can be verbose, you may need to run scorecard itself to see all the detailed information.","title":"Purpose"},{"location":"allstar/#branch_protectionyaml","text":"","title":"branch_protection.yaml"},{"location":"allstar/#purpose_2","text":"This policy checks if our repository's branch protection settings match with the branch protection settings outlined in this file.","title":"Purpose"},{"location":"allstar/#outsideyaml","text":"","title":"outside.yaml"},{"location":"allstar/#purpose_3","text":"By default this policy checks that only organizational members have administrative or push access to the repository.","title":"Purpose"},{"location":"allstar/#securityyaml","text":"","title":"security.yaml"},{"location":"allstar/#purpose_4","text":"This policy checks that the repository has a security policy file in SECURITY.md and it is not empty.","title":"Purpose"},{"location":"backstage-update/","text":"Automated Backstage Update \u00b6 Backstage Update Workflow file Update Backstage Workflow Jobs \u00b6 check-for-existing-update \u00b6 Description: Before upgrade process begins, this job checks if there is currently an open pull request to update backstage. 1 2 3 4 5 6 7 Steps: - Checkout - Uses: [actions/checkout@v2](https://github.com/actions/checkout) - Checkout github repository so workflow can access it - Check for existing auto-update PR - Uses command line to make cURL request to GitHub API for all open pull requests using the \"auto-update-backstage\" branch - Stores url to open pull request as output link-pr \u00b6 Description: If there is an open pull request to update backstage, this job will output a link to the open pull request. 1 2 3 4 Steps: - Failed to Update Backstage - Uses: [actions/github-script@v3](https://github.com/actions/github-script) - Outputs the open pull request url retrieved from GitHub API request update-backstage \u00b6 Description: If there is no open pull request to update backstage, this job will create a branch, perform the upgrade process, and create a pull request with the new changes. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Steps: - Checkout - Uses: [actions/checkout@v2](https://github.com/actions/checkout) - Checkout github repository so workflow can access it - Install Dependencies - Uses yarn install to install dependencies for backstage upgrade process - Update Backstage - Runs npm script \"backstage-update\" which runs \"yarn backstage-cli versions:bump 2>&1 | tee backstage-update-log.txt\" - Compare Create App versions - Uses command line to get current version of \"@backstage/create-app\" from package.json and most recent version from NPM registry - Format PR body - Uses command line to format a summary of the update log to the pull request body - Set output variables - Sets variables for PR title and current date - Create Pull Request - Uses: [peter-evans/create-pull-request@v3](https://github.com/peter-evans/create-pull-request) - Creates pull request containing the changes from running \"backstage-update\" - Pull request body also contains a summary of the update log, outputs from the error log, current and most recent versions of \"@backstage/create-app\" and a link to the create-app changelog - Check outputs - Logs the number and url of the newly generated Pull Request","title":"Backstage Update"},{"location":"backstage-update/#automated-backstage-update","text":"Backstage Update Workflow file","title":"Automated Backstage Update"},{"location":"backstage-update/#update-backstage-workflow-jobs","text":"","title":"Update Backstage Workflow Jobs"},{"location":"backstage-update/#check-for-existing-update","text":"Description: Before upgrade process begins, this job checks if there is currently an open pull request to update backstage. 1 2 3 4 5 6 7 Steps: - Checkout - Uses: [actions/checkout@v2](https://github.com/actions/checkout) - Checkout github repository so workflow can access it - Check for existing auto-update PR - Uses command line to make cURL request to GitHub API for all open pull requests using the \"auto-update-backstage\" branch - Stores url to open pull request as output","title":"check-for-existing-update"},{"location":"backstage-update/#link-pr","text":"Description: If there is an open pull request to update backstage, this job will output a link to the open pull request. 1 2 3 4 Steps: - Failed to Update Backstage - Uses: [actions/github-script@v3](https://github.com/actions/github-script) - Outputs the open pull request url retrieved from GitHub API request","title":"link-pr"},{"location":"backstage-update/#update-backstage","text":"Description: If there is no open pull request to update backstage, this job will create a branch, perform the upgrade process, and create a pull request with the new changes. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Steps: - Checkout - Uses: [actions/checkout@v2](https://github.com/actions/checkout) - Checkout github repository so workflow can access it - Install Dependencies - Uses yarn install to install dependencies for backstage upgrade process - Update Backstage - Runs npm script \"backstage-update\" which runs \"yarn backstage-cli versions:bump 2>&1 | tee backstage-update-log.txt\" - Compare Create App versions - Uses command line to get current version of \"@backstage/create-app\" from package.json and most recent version from NPM registry - Format PR body - Uses command line to format a summary of the update log to the pull request body - Set output variables - Sets variables for PR title and current date - Create Pull Request - Uses: [peter-evans/create-pull-request@v3](https://github.com/peter-evans/create-pull-request) - Creates pull request containing the changes from running \"backstage-update\" - Pull request body also contains a summary of the update log, outputs from the error log, current and most recent versions of \"@backstage/create-app\" and a link to the create-app changelog - Check outputs - Logs the number and url of the newly generated Pull Request","title":"update-backstage"},{"location":"catalog-entry/","text":"Catalog Entry Best Practices \u00b6 This is a best-practices guide to setting up catalog entries for lighthouse-backstage. It covers the most frequently used or suggested setup for the catalog yaml file. For more in-depth coverage, you can check the Backstage YAML File Format doc. Usually named catalog-info.yaml , the yaml file used for catalog configurations can have any name. The following is an example of a descriptor file for a Component entity: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion : backstage.io/v1alpha1 kind : Component metadata : name : artist-web description : The place to be, for great artists labels : example.com/custom : custom_label_value annotations : example.com/service-discovery : artistweb circleci.com/project-slug : github/example-org/artist-website tags : - java links : - url : https://admin.example-org.com title : Admin Dashboard icon : dashboard spec : type : website lifecycle : production owner : artist-relations-team system : public-websites The root fields apiVersion , kind , metadata , and spec are part of the envelope , defining the overall structure of all kinds of entities. Some metadata fields like name , labels , and annotations are of particular significance and have reserved purposes and distinct shapes. kind [required] \u00b6 The high-level entity type. Also known as System Models , additional information can be found on the Backstage docs here. Software within the Backstage catalog can fit into three core entities. Component - Individual pieces of software API - Boundaries between different components Resource - Physical or virtual infrastructure needed to operate a component An extensive catalog of components, APIs, and resources can be hard to understand as a whole. There are additional categories that can help make sense of an ecosystem. System - Collection of entities that cooperate to perform some function Domain - Relates entities and systems to part of the business. apiVersion [required] \u00b6 The version of specification format for that particular entity that the specification is made against. The tuple of apiVersion and kind should be enough for a parser to know how to interpret the contents of the rest of the data. Early catalog versions will be using alpha/beta versions, e.g., backstage.io/v1alpha1 , to signal that the format may still change. After that, we will be using backstage.io/v1 and up. metadata [required] \u00b6 A structure containing metadata about the entity: these aren't directly part of the entity specification itself. name [required] \u00b6 The name of the entity. They are used to recognize the entity along with being used in components to reference the entity. Names must be unique per kind within a given namespace (if specified) at any point in time. The uniqueness constraint is case-sensitive. Names have two requirements: Strings of length at least 1, and at most 63 Must consist of sequences of [a-z0-9A-Z] possibly separated by one of [-_.] Namespaces can also be used, see here title [optional] \u00b6 A title can also be used to represent the entity when available. The title can be helpful when the name is cumbersome or perceived as overly technical. There are no requirements on it, but please keep it short. description [optional] \u00b6 A human-readable description of the entity. It should be kept short and informative. labels [optional] \u00b6 Labels are optional key/value pairs attached to the entity, and their use is identical to Kubernetes object labels . Their primary purpose is for references to other entities and for information that is in one way or another classifying for the current entity. They are often used as values in queries or filters. Values are strings that follow the same restrictions as name . annotations [optional] \u00b6 An object with arbitrary non-identifying metadata attached to the entity, identical in use to Kubernetes object annotations . Their purpose is mainly, but not limited to, to reference external systems. You can also view this section of well-known annotations . tags [optional] \u00b6 A list of single-valued strings, for example, to classify catalog entities in various ways. This is different from the labels in metadata, as labels are key-value pairs. links [optional] \u00b6 A list of external hyperlinks related to the entity. Links can provide additional reference information to external content and resources. Fields of a link are: url : [required] - A url in a standard uri format. ( https://example.com ) title : [optional] - A user friendly display name for the link icon : [optional] - A key representing a visual icon to be displayed in the UI. Note: The icon field value is meant to be a semantic key that will map to a specific icon that an icon library may provide (e.g., material-ui icons). spec [varies] \u00b6 Data that describes the entity. The structure of spec is dependent on the apiVersion and kind combination. Some might not have a spec at all. Kinds \u00b6 The following will talk about fields specific to a kind . There are usually apiVersion and kind requirements for each of these. This article will only cover the most common kind templates. That being component , template , and API . Additional kind options can be found here . relations [all] \u00b6 The relations root field is a read-only list of references between the current entity and other entities. More information can be found here The fields of a relation are: target : A [compound reference] to the other end of the relation. type : The type of relation FROM a source entity TO the target entity. status [all] \u00b6 The status root object is a read-only set of statuses about the current state of health of the entity, described in the well-known statuses section . The only defined field is the items array. Each item describes some aspect of the entity's state, as seen from the point of view of some specific system. The current primary use case for this field is for the ingestion processes for the catalog itself to convey information about errors and warnings back to the user. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 { // ... \"status\" : { \"items\" : [ { \"type\" : \"backstage.io/catalog-processing\" , \"level\" : \"error\" , \"message\" : \"NotFoundError: File not found\" , \"error\" : { \"name\" : \"NotFoundError\" , \"message\" : \"File not found\" , \"stack\" : \"...\" } } ] }, \"spec\" : { // ... } } The fields of a status item are: type - The type of status as a unique key per source. Each type may appear more than once in the array. level - The level/severity of the status item: 'info', 'warning', or 'error' message - A brief message describing the status intended for human consumption. error - An optional serialized error object related to the status. Kind: Component \u00b6 A Component describes a software component. It is typically intimately linked to the source code that constitutes the component and should be what a developer may regard a \"unit of software,\" usually with a distinct deployable or linkable artifact. This section covers the required fields. Additional fields can be found here . spec.type [required] \u00b6 The software catalog accepts any value. Still, an organization should take great care to establish a proper taxonomy for these. Tools, including Backstage itself, may read this field and behave differently depending on its value. The current set of well-known and standard values for this field is: service - A backend service, typically exposing an API website - A website. library - A software library, such as an npm module or a java library. spec.lifecycle [required] \u00b6 The lifecycle state of the component. The current set of well-known and common values for this field is: experimental - An experiment or early, non-production component, signaling that users may not prefer to consume it over other more established components or that there are low or no reliability guarantees. production - An established, owned, maintained component. deprecated - A component that is at the end of its lifecycle and may disappear at a later point in time. spec.owner [required] \u00b6 An entity reference to the owner of the component. In Backstage, a component owner is a singular entity (commonly a team) that bears ultimate responsibility for the component and has the authority and capability to develop and maintain it. Kind: Template \u00b6 A complete list of kind: Template specific fields can be found here . spec.type [required] \u00b6 The type of component created by the template. See spec.type above. The available options are website , service and library . spec.parameters [required] \u00b6 These are template variables that can be modified in the frontend as a sequence. It can either be one Step if you want one extensive list of different fields in the frontend, or it can be broken up into multiple extra steps, rendered as additional steps in the scaffolder plugin frontend. You can find more here . Kind: API \u00b6 A complete list of kind: API specific fields can be found here . spec.type [required] \u00b6 The type of the API definition as a string. The software catalog accepts any kind of value, but an organization should take great care to establish a proper taxonomy for these. The current set of well-known and common values for this field is: openapi - An API definition in YAML or JSON format based on the OpenAPI v2 or v3 spec. asyncapi - An API definition baesd on the AsyncAPI spec. graphql - An API definition based on GraphQL schemas for consuming GraphQL based APIs. grpc An API definition based on Protocol Bufferes to use with gRPC . spec.lifecycle [required] \u00b6 The lifecycle state of the API. It can be experimental , production , or deprecated. See the lifecycle definition within kind: Component` for further information. spec.owner [required] \u00b6 An entity reference to the owner of the component, e.g. artist-relations-team. This field is required. spec.definition [required] \u00b6 The definition of the API, based on the format defined by spec.type .","title":"Catalog Entry"},{"location":"catalog-entry/#catalog-entry-best-practices","text":"This is a best-practices guide to setting up catalog entries for lighthouse-backstage. It covers the most frequently used or suggested setup for the catalog yaml file. For more in-depth coverage, you can check the Backstage YAML File Format doc. Usually named catalog-info.yaml , the yaml file used for catalog configurations can have any name. The following is an example of a descriptor file for a Component entity: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion : backstage.io/v1alpha1 kind : Component metadata : name : artist-web description : The place to be, for great artists labels : example.com/custom : custom_label_value annotations : example.com/service-discovery : artistweb circleci.com/project-slug : github/example-org/artist-website tags : - java links : - url : https://admin.example-org.com title : Admin Dashboard icon : dashboard spec : type : website lifecycle : production owner : artist-relations-team system : public-websites The root fields apiVersion , kind , metadata , and spec are part of the envelope , defining the overall structure of all kinds of entities. Some metadata fields like name , labels , and annotations are of particular significance and have reserved purposes and distinct shapes.","title":"Catalog Entry Best Practices"},{"location":"catalog-entry/#kind-required","text":"The high-level entity type. Also known as System Models , additional information can be found on the Backstage docs here. Software within the Backstage catalog can fit into three core entities. Component - Individual pieces of software API - Boundaries between different components Resource - Physical or virtual infrastructure needed to operate a component An extensive catalog of components, APIs, and resources can be hard to understand as a whole. There are additional categories that can help make sense of an ecosystem. System - Collection of entities that cooperate to perform some function Domain - Relates entities and systems to part of the business.","title":"kind [required]"},{"location":"catalog-entry/#apiversion-required","text":"The version of specification format for that particular entity that the specification is made against. The tuple of apiVersion and kind should be enough for a parser to know how to interpret the contents of the rest of the data. Early catalog versions will be using alpha/beta versions, e.g., backstage.io/v1alpha1 , to signal that the format may still change. After that, we will be using backstage.io/v1 and up.","title":"apiVersion [required]"},{"location":"catalog-entry/#metadata-required","text":"A structure containing metadata about the entity: these aren't directly part of the entity specification itself.","title":"metadata [required]"},{"location":"catalog-entry/#name-required","text":"The name of the entity. They are used to recognize the entity along with being used in components to reference the entity. Names must be unique per kind within a given namespace (if specified) at any point in time. The uniqueness constraint is case-sensitive. Names have two requirements: Strings of length at least 1, and at most 63 Must consist of sequences of [a-z0-9A-Z] possibly separated by one of [-_.] Namespaces can also be used, see here","title":"name [required]"},{"location":"catalog-entry/#title-optional","text":"A title can also be used to represent the entity when available. The title can be helpful when the name is cumbersome or perceived as overly technical. There are no requirements on it, but please keep it short.","title":"title [optional]"},{"location":"catalog-entry/#description-optional","text":"A human-readable description of the entity. It should be kept short and informative.","title":"description [optional]"},{"location":"catalog-entry/#labels-optional","text":"Labels are optional key/value pairs attached to the entity, and their use is identical to Kubernetes object labels . Their primary purpose is for references to other entities and for information that is in one way or another classifying for the current entity. They are often used as values in queries or filters. Values are strings that follow the same restrictions as name .","title":"labels [optional]"},{"location":"catalog-entry/#annotations-optional","text":"An object with arbitrary non-identifying metadata attached to the entity, identical in use to Kubernetes object annotations . Their purpose is mainly, but not limited to, to reference external systems. You can also view this section of well-known annotations .","title":"annotations [optional]"},{"location":"catalog-entry/#tags-optional","text":"A list of single-valued strings, for example, to classify catalog entities in various ways. This is different from the labels in metadata, as labels are key-value pairs.","title":"tags [optional]"},{"location":"catalog-entry/#links-optional","text":"A list of external hyperlinks related to the entity. Links can provide additional reference information to external content and resources. Fields of a link are: url : [required] - A url in a standard uri format. ( https://example.com ) title : [optional] - A user friendly display name for the link icon : [optional] - A key representing a visual icon to be displayed in the UI. Note: The icon field value is meant to be a semantic key that will map to a specific icon that an icon library may provide (e.g., material-ui icons).","title":"links [optional]"},{"location":"catalog-entry/#spec-varies","text":"Data that describes the entity. The structure of spec is dependent on the apiVersion and kind combination. Some might not have a spec at all.","title":"spec [varies]"},{"location":"catalog-entry/#kinds","text":"The following will talk about fields specific to a kind . There are usually apiVersion and kind requirements for each of these. This article will only cover the most common kind templates. That being component , template , and API . Additional kind options can be found here .","title":"Kinds"},{"location":"catalog-entry/#relations-all","text":"The relations root field is a read-only list of references between the current entity and other entities. More information can be found here The fields of a relation are: target : A [compound reference] to the other end of the relation. type : The type of relation FROM a source entity TO the target entity.","title":"relations [all]"},{"location":"catalog-entry/#status-all","text":"The status root object is a read-only set of statuses about the current state of health of the entity, described in the well-known statuses section . The only defined field is the items array. Each item describes some aspect of the entity's state, as seen from the point of view of some specific system. The current primary use case for this field is for the ingestion processes for the catalog itself to convey information about errors and warnings back to the user. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 { // ... \"status\" : { \"items\" : [ { \"type\" : \"backstage.io/catalog-processing\" , \"level\" : \"error\" , \"message\" : \"NotFoundError: File not found\" , \"error\" : { \"name\" : \"NotFoundError\" , \"message\" : \"File not found\" , \"stack\" : \"...\" } } ] }, \"spec\" : { // ... } } The fields of a status item are: type - The type of status as a unique key per source. Each type may appear more than once in the array. level - The level/severity of the status item: 'info', 'warning', or 'error' message - A brief message describing the status intended for human consumption. error - An optional serialized error object related to the status.","title":"status [all]"},{"location":"catalog-entry/#kind-component","text":"A Component describes a software component. It is typically intimately linked to the source code that constitutes the component and should be what a developer may regard a \"unit of software,\" usually with a distinct deployable or linkable artifact. This section covers the required fields. Additional fields can be found here .","title":"Kind: Component"},{"location":"catalog-entry/#spectype-required","text":"The software catalog accepts any value. Still, an organization should take great care to establish a proper taxonomy for these. Tools, including Backstage itself, may read this field and behave differently depending on its value. The current set of well-known and standard values for this field is: service - A backend service, typically exposing an API website - A website. library - A software library, such as an npm module or a java library.","title":"spec.type [required]"},{"location":"catalog-entry/#speclifecycle-required","text":"The lifecycle state of the component. The current set of well-known and common values for this field is: experimental - An experiment or early, non-production component, signaling that users may not prefer to consume it over other more established components or that there are low or no reliability guarantees. production - An established, owned, maintained component. deprecated - A component that is at the end of its lifecycle and may disappear at a later point in time.","title":"spec.lifecycle [required]"},{"location":"catalog-entry/#specowner-required","text":"An entity reference to the owner of the component. In Backstage, a component owner is a singular entity (commonly a team) that bears ultimate responsibility for the component and has the authority and capability to develop and maintain it.","title":"spec.owner [required]"},{"location":"catalog-entry/#kind-template","text":"A complete list of kind: Template specific fields can be found here .","title":"Kind: Template"},{"location":"catalog-entry/#spectype-required_1","text":"The type of component created by the template. See spec.type above. The available options are website , service and library .","title":"spec.type [required]"},{"location":"catalog-entry/#specparameters-required","text":"These are template variables that can be modified in the frontend as a sequence. It can either be one Step if you want one extensive list of different fields in the frontend, or it can be broken up into multiple extra steps, rendered as additional steps in the scaffolder plugin frontend. You can find more here .","title":"spec.parameters [required]"},{"location":"catalog-entry/#kind-api","text":"A complete list of kind: API specific fields can be found here .","title":"Kind: API"},{"location":"catalog-entry/#spectype-required_2","text":"The type of the API definition as a string. The software catalog accepts any kind of value, but an organization should take great care to establish a proper taxonomy for these. The current set of well-known and common values for this field is: openapi - An API definition in YAML or JSON format based on the OpenAPI v2 or v3 spec. asyncapi - An API definition baesd on the AsyncAPI spec. graphql - An API definition based on GraphQL schemas for consuming GraphQL based APIs. grpc An API definition based on Protocol Bufferes to use with gRPC .","title":"spec.type [required]"},{"location":"catalog-entry/#speclifecycle-required_1","text":"The lifecycle state of the API. It can be experimental , production , or deprecated. See the lifecycle definition within kind: Component` for further information.","title":"spec.lifecycle [required]"},{"location":"catalog-entry/#specowner-required_1","text":"An entity reference to the owner of the component, e.g. artist-relations-team. This field is required.","title":"spec.owner [required]"},{"location":"catalog-entry/#specdefinition-required","text":"The definition of the API, based on the format defined by spec.type .","title":"spec.definition [required]"},{"location":"codespace/","text":"Codespace Configuration \u00b6 Background \u00b6 The default image Codespace uses, mcr.microsoft.com/vscode/devcontainers/universal:1-linux , is ~10gb uncompressed. This makes rebuilding the devcontainer while in Codespaces take a long time. A majority of this wait time is unnecessary because we are waiting for files and directories to be rebuilt for tools we don't use. Creating a Custom Image \u00b6 To reduce our rebuilding wait times, I created a smaller image , around ~2gb. The new devcontainer image is uploaded to ghcr.io. To be able to access GitHub Packages uploaded to ghcr.io, we need to have some kind of authorization. For Codespaces this is done by creating secrets with a specific naming convention. Once Codespaces has the secrets, you will have authorization to access a private registry when pulling Docker images for your Codespace's devcontainer. Codespaces looks for any secrets with the following naming convention: 1 2 3 <*>_CONTAINER_REGISTRY_SERVER <*>_CONTAINER_REGISTRY_USER <*>_CONTAINER_REGISTRY_PASSWORD You can generate a personal access token with the read:packages permission to create these secrets for Codespaces: 1 2 3 DEV_CONTAINER_REGISTRY_SERVER = ghcr.io DEV_CONTAINER_REGISTRY_USER = <username> DEV_CONTAINER_REGISTRY_PASSWORD = <PAT> Modifying the Image \u00b6 If we need to make changes in the future, we can modify the base.Dockerfile located in .devcontainer . It is better to do this process locally rather than from inside a Codespace because you will not be able to cache some or all layers, making the rebuilding process take a much longer time than necessary. Rebuilding process \u00b6 Navigate to .devcontainer locally 1 $ cd /path/to/workspace/.devcontainer Make some changes to base.Dockerfile to install some new tools Rebuild the image 1 $ docker build -f base.Dockerfile . -t ghcr.io/department-of-veterans-affairs/lighthouse-backstage/devcontainer:latest Log into container registry using PAT 1 2 3 $ echo CR_PAT=<YOUR_TOKEN> $ echo $CR_PAT | docker login ghcr.io -u <USERNAME> --password-stdin > Login Succeeded Push the image 1 $ docker push ghcr.io/department-of-veterans-affairs/lighthouse-backstage/devcontainer:latest","title":"Codespace Configuration"},{"location":"codespace/#codespace-configuration","text":"","title":"Codespace Configuration"},{"location":"codespace/#background","text":"The default image Codespace uses, mcr.microsoft.com/vscode/devcontainers/universal:1-linux , is ~10gb uncompressed. This makes rebuilding the devcontainer while in Codespaces take a long time. A majority of this wait time is unnecessary because we are waiting for files and directories to be rebuilt for tools we don't use.","title":"Background"},{"location":"codespace/#creating-a-custom-image","text":"To reduce our rebuilding wait times, I created a smaller image , around ~2gb. The new devcontainer image is uploaded to ghcr.io. To be able to access GitHub Packages uploaded to ghcr.io, we need to have some kind of authorization. For Codespaces this is done by creating secrets with a specific naming convention. Once Codespaces has the secrets, you will have authorization to access a private registry when pulling Docker images for your Codespace's devcontainer. Codespaces looks for any secrets with the following naming convention: 1 2 3 <*>_CONTAINER_REGISTRY_SERVER <*>_CONTAINER_REGISTRY_USER <*>_CONTAINER_REGISTRY_PASSWORD You can generate a personal access token with the read:packages permission to create these secrets for Codespaces: 1 2 3 DEV_CONTAINER_REGISTRY_SERVER = ghcr.io DEV_CONTAINER_REGISTRY_USER = <username> DEV_CONTAINER_REGISTRY_PASSWORD = <PAT>","title":"Creating a Custom Image"},{"location":"codespace/#modifying-the-image","text":"If we need to make changes in the future, we can modify the base.Dockerfile located in .devcontainer . It is better to do this process locally rather than from inside a Codespace because you will not be able to cache some or all layers, making the rebuilding process take a much longer time than necessary.","title":"Modifying the Image"},{"location":"codespace/#rebuilding-process","text":"Navigate to .devcontainer locally 1 $ cd /path/to/workspace/.devcontainer Make some changes to base.Dockerfile to install some new tools Rebuild the image 1 $ docker build -f base.Dockerfile . -t ghcr.io/department-of-veterans-affairs/lighthouse-backstage/devcontainer:latest Log into container registry using PAT 1 2 3 $ echo CR_PAT=<YOUR_TOKEN> $ echo $CR_PAT | docker login ghcr.io -u <USERNAME> --password-stdin > Login Succeeded Push the image 1 $ docker push ghcr.io/department-of-veterans-affairs/lighthouse-backstage/devcontainer:latest","title":"Rebuilding process"},{"location":"cypress-troubleshooting/","text":"Cypress Troubleshooting \u00b6 Most issues can be resolved by reading the error listed within the test logs. Most errors are pretty straightforward and often provide tips on how to fix them. You can also lookup an error here . Otherwise, continue below. Common Errors \u00b6 There are two common types of errors that you'll come across. Cypress failed to run the test. The App failed the test criteria. (assertion errors) Cypress Failed \u00b6 Cypress failing can happen whenever a test isn't written correctly or if there's an issue with the syntax. Most of these problems can be resolved by reviewing the test, referencing the API, or checking the best practices page. API Best Practices Assertion Errors \u00b6 This failure occurred when the app didn't meet the test criteria. For example, if the test is attempting to target text that cannot be located, or click a button that isn't there. 1 AssertionError: Timed out retrying: Expected to find element: `span[title=\"Disable\"]`, but never found it. Cypress recommends that we use data-* tags to easily select elements for testing. Selectors should always target attributes that are resilient to changes. Some best practices to keep in mind. Don't target elements based on CSS attributes such as: id, class, tag Don't target elements that may change their textContent Add data-* attributes to make it easier to target elements Timeout Failures \u00b6 Another common issue that can occur is if the app fails to load within a set time. Cypress will wait 1 minute before failing a test suite due to the app not loading. Usually, rerunning the test can fix it. Otherwise, there might be an issue with the test itself. 1 2 3 4 5 6 7 8 9 10 11 CypressError: Timed out after waiting `60000ms` for your remote page to load. Your page did not fire its `load` event within `60000ms`. You can try increasing the `pageLoadTimeout` value in `cypress.json` to wait longer. Browsers will not fire the `load` event until all stylesheets and scripts are done downloading. When this `load` event occurs, Cypress will continue running commands. Because this error occurred during a `before each` hook we are skipping the remaining tests in the current suite: `Feature Flags` CI / CD \u00b6 Checking the Logs \u00b6 Viewing the logs can be the easiest way to understanding what went wrong during a test. You can locate them by clicking into any failed browser test and viewing the logs under the Run cypress-io/GitHub-action@v2 step. It should auto-scroll to the end of the record where the error is listed. These boxes display pass/fail metrics along with in-depth checks on failed tests. This will also be where the error will be listed. Once you have the error message, you can look it up on Cypress here. Errors will also come with a few tips on how the error might be resolved. These messages are generally constructive, and following them will usually solve the problem. Viewing Artifacts \u00b6 In addition to logs, Cypress will also take a screenshot of its test runner displaying the error, along with the webpage running next to it. Artifacts can be viewed/downloaded from the workflow summary page by clicking the number below Artifacts or by clicking the file listed at the end of the page. Example screenshot of the cypress test runner. Cypress Test Runner Docs Locally \u00b6 Cypress can also be run locally to speed up debugging/testing. You can run the Cypress CLI by starting the frontend app. cd into packages/app Run yarn start Open a second terminal window and run yarn cy:run This will run through all the cypress tests. You can also specify specific files if you don't want to run through the entire suite. Install Cypress Cypress has a test runner that makes testing 1000% easier as you can view your tests running in real-time. Not to mention the ability to time-travel and interact with your tests. Using the test runner is beyond the scope of this guide, so I'll leave a link to it below. The Test Runner References \u00b6 Install Cypress Cypress Documentation Best Practices Troubleshooting Guide Error message Reference Cypress Test Runner","title":"Cypress Troubleshooting"},{"location":"cypress-troubleshooting/#cypress-troubleshooting","text":"Most issues can be resolved by reading the error listed within the test logs. Most errors are pretty straightforward and often provide tips on how to fix them. You can also lookup an error here . Otherwise, continue below.","title":"Cypress Troubleshooting"},{"location":"cypress-troubleshooting/#common-errors","text":"There are two common types of errors that you'll come across. Cypress failed to run the test. The App failed the test criteria. (assertion errors)","title":"Common Errors"},{"location":"cypress-troubleshooting/#cypress-failed","text":"Cypress failing can happen whenever a test isn't written correctly or if there's an issue with the syntax. Most of these problems can be resolved by reviewing the test, referencing the API, or checking the best practices page. API Best Practices","title":"Cypress Failed"},{"location":"cypress-troubleshooting/#assertion-errors","text":"This failure occurred when the app didn't meet the test criteria. For example, if the test is attempting to target text that cannot be located, or click a button that isn't there. 1 AssertionError: Timed out retrying: Expected to find element: `span[title=\"Disable\"]`, but never found it. Cypress recommends that we use data-* tags to easily select elements for testing. Selectors should always target attributes that are resilient to changes. Some best practices to keep in mind. Don't target elements based on CSS attributes such as: id, class, tag Don't target elements that may change their textContent Add data-* attributes to make it easier to target elements","title":"Assertion Errors"},{"location":"cypress-troubleshooting/#timeout-failures","text":"Another common issue that can occur is if the app fails to load within a set time. Cypress will wait 1 minute before failing a test suite due to the app not loading. Usually, rerunning the test can fix it. Otherwise, there might be an issue with the test itself. 1 2 3 4 5 6 7 8 9 10 11 CypressError: Timed out after waiting `60000ms` for your remote page to load. Your page did not fire its `load` event within `60000ms`. You can try increasing the `pageLoadTimeout` value in `cypress.json` to wait longer. Browsers will not fire the `load` event until all stylesheets and scripts are done downloading. When this `load` event occurs, Cypress will continue running commands. Because this error occurred during a `before each` hook we are skipping the remaining tests in the current suite: `Feature Flags`","title":"Timeout Failures"},{"location":"cypress-troubleshooting/#ci-cd","text":"","title":"CI / CD"},{"location":"cypress-troubleshooting/#checking-the-logs","text":"Viewing the logs can be the easiest way to understanding what went wrong during a test. You can locate them by clicking into any failed browser test and viewing the logs under the Run cypress-io/GitHub-action@v2 step. It should auto-scroll to the end of the record where the error is listed. These boxes display pass/fail metrics along with in-depth checks on failed tests. This will also be where the error will be listed. Once you have the error message, you can look it up on Cypress here. Errors will also come with a few tips on how the error might be resolved. These messages are generally constructive, and following them will usually solve the problem.","title":"Checking the Logs"},{"location":"cypress-troubleshooting/#viewing-artifacts","text":"In addition to logs, Cypress will also take a screenshot of its test runner displaying the error, along with the webpage running next to it. Artifacts can be viewed/downloaded from the workflow summary page by clicking the number below Artifacts or by clicking the file listed at the end of the page. Example screenshot of the cypress test runner. Cypress Test Runner Docs","title":"Viewing Artifacts"},{"location":"cypress-troubleshooting/#locally","text":"Cypress can also be run locally to speed up debugging/testing. You can run the Cypress CLI by starting the frontend app. cd into packages/app Run yarn start Open a second terminal window and run yarn cy:run This will run through all the cypress tests. You can also specify specific files if you don't want to run through the entire suite. Install Cypress Cypress has a test runner that makes testing 1000% easier as you can view your tests running in real-time. Not to mention the ability to time-travel and interact with your tests. Using the test runner is beyond the scope of this guide, so I'll leave a link to it below. The Test Runner","title":"Locally"},{"location":"cypress-troubleshooting/#references","text":"Install Cypress Cypress Documentation Best Practices Troubleshooting Guide Error message Reference Cypress Test Runner","title":"References"},{"location":"deployment/","text":"Deployment (WIP) \u00b6 Deployment process overview \u00b6 - There are two automated processes for deployment: - The Embark deployment handles the creation and tagging of docker images, the creation of GitHub deployments, and the management of deployment definitions. These processes push changes. - The Argo sync handles the synchronization of deployment definitions between the deployment repo and Kubernetes. These processes pull when changes are detected. Embark deployment detail \u00b6 The deployment repo contains a values file for each environment e.g. /dev.yaml. The goal of this process is to update the values file for the target environment with the correct application version . - ArgoCD (not diagrammed) is configured to use a specific values file for each environment - The values file contains the dynamic elements of the deployment definition such as the application version . The Docker image has a version tag that Kubernetes uses to select the correct Docker image. This version tag is defined in the templates/deployment.yaml as spec.template.spec.containers.image Embark deployment automation \u00b6 release docker images : - The Create release action adds a version tag to Docker images using package.json version it was built from. create environment deployment : - The Create release action creates a deployment in GitHub for the target environment and the version in the version tag deploy webhook : - GitHub POSTs to a deploy webhook which triggers the Deployment action - The webhook contains the target environment and version tag commit app version update : - The Deployment action uses the target environment to determine which values file to update - The Deployment action commits an update to the values file with the version tag create deploy success : - The Deployment action creates a deployment success status Deployment to dev environment \u00b6 See overview and detail in previous sections for more info The developer goes through the PR process. - The CI performs automated validations on the feature branch. - A peer reviews and approves the change. The developer merges the feature branch. - The CI performs automated validations on the latest commit of the main branch. - This is done synchronously to avoid race conditions. The Create release action add a version tag to the image using the git commit SHA. The Create release action creates a deployment using that version tag and dev as the target environment ArcoCD syncs the update and the new version is deployed to dev. Deployment to staging and production \u00b6 See overview and detail in previous sections for more info The Create release action is started with a cron job The Changeset bot is run and a new release branch and PR are created The application version is incremented in package.json The Embark deployment process is triggered. - The Docker images are released with the branch commit SHA as the version tag - This image is deployed to staging. The developer merges the release PR The Create release action creates the release notes. The Embark deployment process is triggered. - The Docker images that were deployed to staging are tagged with the new application version as the version tag . - Important : The image on production must match the image that was deployed and verified on staging . - This image is deployed to production.","title":"Deployment"},{"location":"deployment/#deployment-wip","text":"","title":"Deployment (WIP)"},{"location":"deployment/#deployment-process-overview","text":"- There are two automated processes for deployment: - The Embark deployment handles the creation and tagging of docker images, the creation of GitHub deployments, and the management of deployment definitions. These processes push changes. - The Argo sync handles the synchronization of deployment definitions between the deployment repo and Kubernetes. These processes pull when changes are detected.","title":"Deployment process overview"},{"location":"deployment/#embark-deployment-detail","text":"The deployment repo contains a values file for each environment e.g. /dev.yaml. The goal of this process is to update the values file for the target environment with the correct application version . - ArgoCD (not diagrammed) is configured to use a specific values file for each environment - The values file contains the dynamic elements of the deployment definition such as the application version . The Docker image has a version tag that Kubernetes uses to select the correct Docker image. This version tag is defined in the templates/deployment.yaml as spec.template.spec.containers.image","title":"Embark deployment detail"},{"location":"deployment/#embark-deployment-automation","text":"release docker images : - The Create release action adds a version tag to Docker images using package.json version it was built from. create environment deployment : - The Create release action creates a deployment in GitHub for the target environment and the version in the version tag deploy webhook : - GitHub POSTs to a deploy webhook which triggers the Deployment action - The webhook contains the target environment and version tag commit app version update : - The Deployment action uses the target environment to determine which values file to update - The Deployment action commits an update to the values file with the version tag create deploy success : - The Deployment action creates a deployment success status","title":"Embark deployment automation"},{"location":"deployment/#deployment-to-dev-environment","text":"See overview and detail in previous sections for more info The developer goes through the PR process. - The CI performs automated validations on the feature branch. - A peer reviews and approves the change. The developer merges the feature branch. - The CI performs automated validations on the latest commit of the main branch. - This is done synchronously to avoid race conditions. The Create release action add a version tag to the image using the git commit SHA. The Create release action creates a deployment using that version tag and dev as the target environment ArcoCD syncs the update and the new version is deployed to dev.","title":"Deployment to dev environment"},{"location":"deployment/#deployment-to-staging-and-production","text":"See overview and detail in previous sections for more info The Create release action is started with a cron job The Changeset bot is run and a new release branch and PR are created The application version is incremented in package.json The Embark deployment process is triggered. - The Docker images are released with the branch commit SHA as the version tag - This image is deployed to staging. The developer merges the release PR The Create release action creates the release notes. The Embark deployment process is triggered. - The Docker images that were deployed to staging are tagged with the new application version as the version tag . - Important : The image on production must match the image that was deployed and verified on staging . - This image is deployed to production.","title":"Deployment to staging and production"},{"location":"dev-deploy/","text":"Deploying to Development Environment \u00b6 Setup \u00b6 Kubectl Install \u00b6 If you don't have kubectl you'll need to install it: - Install on Linux - Install on Mac - Install on Windows Helm Install \u00b6 If you don't have helm you'll need to install it: - Find the latest release to download and install helm Building Images \u00b6 Navigate to Root Directory 1 $ cd /workspaces/lighthouse-backstage Install Dependencies & Run Typescript Compiler 1 $ yarn install --frozen-lockfile && yarn tsc Configure app-config.yaml Depending on what environment you're deploying to will determine what you need to set the values for: app.baseUrl backend.baseUrl backend.cors.origin backend.database.connection host, port, user, password 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # app-config.yaml app: title: DVP Developer Portal baseUrl: ${HOST} ... backend: baseUrl: ${HOST} listen: port: 7000 csp: connect-src: [\"'self'\", 'http:', 'https:'] cors: origin: ${HOST} ... database: client: pg connection: host: ${POSTGRES_SERVICE_HOST} port: ${POSTGRES_SERVICE_PORT} user: ${POSTGRES_USER} password: ${POSTGRES_PASSWORD} ... auth: environment: development providers: github: development: clientId: ${GH_CLIENT_ID} clientSecret: ${GH_CLIENT_SECRET} Build Static Assets 1 $ yarn build Determine version number you want to tag images with Replace <version-number> below with the version number https://github.com/orgs/department-of-veterans-affairs/packages?repo_name=lighthouse-backstage Create Image for Backend Container 1 $ docker build --tag ghcr.io/ghcr.io/department-of-veterans-affairs/lighthouse-backstage/backend:<version-number> --tag ghcr.io/department-of-veterans-affairs/lighthouse-backstage/backend:latest -f Dockerfile.backend . Create Image for Frontend Container 1 $ docker build --tag ghcr.io/ghcr.io/department-of-veterans-affairs/lighthouse-backstage/frontend:<version-number> --tag ghcr.io/department-of-veterans-affairs/lighthouse-backstage/frontend:latest -f Dockerfile.frontend . Push Images to GitHub Packages \u00b6 Login to ghcr.io with PAT Create environment variable with your PAT 1 $ export CR_PAT=<YOUR_TOKEN> Login to ghcr.io with your GitHub username 1 2 $ echo $CR_PAT | docker login ghcr.io -u <USERNAME> --password-stdin > Login Succeeded Push the Images to the Container Registry 1 $ docker push --all-tags ghcr.io/department-of-veterans-affairs/lighthouse-backstage/backend:latest 1 $ docker push --all-tags ghcr.io/department-of-veterans-affairs/lighthouse-backstage/frontend:latest Verify the images were pushed by checking when the frontend/backend images were last published on the Lighthouse-Backstage repository Deploy to Dev Environment \u00b6 Login to Lightkeeper \u00b6 Login to Lightkeeper 1 2 3 $ lightkeeper login Go here to complete your login: https://... Login Successful! Set up kube config 1 $ lightkeeper create clusterconfig nonprod > ~/.kube/config Note: if using Codespaces then you cannot install Lightkeeper. To work around this, you can set your kube config locally using the steps above and then copy the config to your Codespaces. In Codespaces, you can make a new .kube directory in ~/ and then make a new file inside ~/.kube/ called config . Inside ~/.kube/config you can copy the contents of your local ~/.kube/config file. Install Helm Chart \u00b6 Modify values.yaml to configure deployment to use nonprod cluster Note: To pull the image, Kubernetes will need authorization for GitHub. One way to verify you have authorization is to create an encoded dockerconfigjson secret using a Personal Access Token. The encoded dockerconfigjson string can be passed as an environment variable to Helm. More information about creating and encoding the dockerconfigjson string Creating a DOCKERCONFIGJSON string First make a PAT with the permission to read access for repositories 1 $ echo -n \"<github_username>:<github_pat>\" | base64 The above command will create an , with the encoded auth string run: 1 $ echo -n '{\"auths\":{\"ghcr.io\":{\"auth\":\"<encoded_auth_string>\"}}}' | base64 This will output the which you can then add to the .env file: 1 DOCKERCONFIGJSON=<encoded_dockerjsonconfig_string> Create Environment Variables for Secrets Create .env file with your environment variables; you will need to set all of these variables in order for the deployment to work. 1 2 3 4 5 6 7 8 9 DOCKERCONFIGJSON=<base64 encoded json string> GH_TOKEN=<github_token> HOST=<host url> GH_CLIENT_ID=<GH OAuth Client ID> GH_CLIENT_SECRET=<GH OAuth Client Secret> NONPROD=true BASE_URL=https://dev.devportal.name GATEWAY=istio-system/dev-devportal-name-gateway ... Export file contents 1 set -o allexport; source .env; set +o allexport Install the Helm chart and set secrets using --set 1 $ helm upgrade backstage-dev helm/lighthouse-backstage/ --debug --values helm/lighthouse-backstage/values.yaml --namespace lighthouse-bandicoot-dev --set DOCKERCONFIGJSON=$DOCKERCONFIGJSON --set GH_TOKEN=$GH_TOKEN --set HOST=$HOST --set GH_CLIENT_ID=$GH_CLIENT_ID --set GH_CLIENT_SECRET=$GH_CLIENT_SECRET --set nonprod=$NONPROD --set BASE_URL=$BASE_URL --set GATEWAY=$GATEWAY --set BACKEND_PORT=5432 --install --atomic --cleanup-on-fail --history-max 5 Verify Deployment \u00b6 Through Terminal 1 2 3 $ helm list -n lighthouse-bandicoot-dev NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION backstage-dev lighthouse-bandicoot-dev 1 2021-10-07 07:37:22.4745171 -0700 PDT deployed lighthouse-backstage-0.1.0 1.16.0 Visit https://dev.devportal.name to view the running application","title":"Deploy to Dev Environment (K8s/Helm)"},{"location":"dev-deploy/#deploying-to-development-environment","text":"","title":"Deploying to Development Environment"},{"location":"dev-deploy/#setup","text":"","title":"Setup"},{"location":"dev-deploy/#kubectl-install","text":"If you don't have kubectl you'll need to install it: - Install on Linux - Install on Mac - Install on Windows","title":"Kubectl Install"},{"location":"dev-deploy/#helm-install","text":"If you don't have helm you'll need to install it: - Find the latest release to download and install helm","title":"Helm Install"},{"location":"dev-deploy/#building-images","text":"Navigate to Root Directory 1 $ cd /workspaces/lighthouse-backstage Install Dependencies & Run Typescript Compiler 1 $ yarn install --frozen-lockfile && yarn tsc Configure app-config.yaml Depending on what environment you're deploying to will determine what you need to set the values for: app.baseUrl backend.baseUrl backend.cors.origin backend.database.connection host, port, user, password 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # app-config.yaml app: title: DVP Developer Portal baseUrl: ${HOST} ... backend: baseUrl: ${HOST} listen: port: 7000 csp: connect-src: [\"'self'\", 'http:', 'https:'] cors: origin: ${HOST} ... database: client: pg connection: host: ${POSTGRES_SERVICE_HOST} port: ${POSTGRES_SERVICE_PORT} user: ${POSTGRES_USER} password: ${POSTGRES_PASSWORD} ... auth: environment: development providers: github: development: clientId: ${GH_CLIENT_ID} clientSecret: ${GH_CLIENT_SECRET} Build Static Assets 1 $ yarn build Determine version number you want to tag images with Replace <version-number> below with the version number https://github.com/orgs/department-of-veterans-affairs/packages?repo_name=lighthouse-backstage Create Image for Backend Container 1 $ docker build --tag ghcr.io/ghcr.io/department-of-veterans-affairs/lighthouse-backstage/backend:<version-number> --tag ghcr.io/department-of-veterans-affairs/lighthouse-backstage/backend:latest -f Dockerfile.backend . Create Image for Frontend Container 1 $ docker build --tag ghcr.io/ghcr.io/department-of-veterans-affairs/lighthouse-backstage/frontend:<version-number> --tag ghcr.io/department-of-veterans-affairs/lighthouse-backstage/frontend:latest -f Dockerfile.frontend .","title":"Building Images"},{"location":"dev-deploy/#push-images-to-github-packages","text":"Login to ghcr.io with PAT Create environment variable with your PAT 1 $ export CR_PAT=<YOUR_TOKEN> Login to ghcr.io with your GitHub username 1 2 $ echo $CR_PAT | docker login ghcr.io -u <USERNAME> --password-stdin > Login Succeeded Push the Images to the Container Registry 1 $ docker push --all-tags ghcr.io/department-of-veterans-affairs/lighthouse-backstage/backend:latest 1 $ docker push --all-tags ghcr.io/department-of-veterans-affairs/lighthouse-backstage/frontend:latest Verify the images were pushed by checking when the frontend/backend images were last published on the Lighthouse-Backstage repository","title":"Push Images to GitHub Packages"},{"location":"dev-deploy/#deploy-to-dev-environment","text":"","title":"Deploy to Dev Environment"},{"location":"dev-deploy/#login-to-lightkeeper","text":"Login to Lightkeeper 1 2 3 $ lightkeeper login Go here to complete your login: https://... Login Successful! Set up kube config 1 $ lightkeeper create clusterconfig nonprod > ~/.kube/config Note: if using Codespaces then you cannot install Lightkeeper. To work around this, you can set your kube config locally using the steps above and then copy the config to your Codespaces. In Codespaces, you can make a new .kube directory in ~/ and then make a new file inside ~/.kube/ called config . Inside ~/.kube/config you can copy the contents of your local ~/.kube/config file.","title":"Login to Lightkeeper"},{"location":"dev-deploy/#install-helm-chart","text":"Modify values.yaml to configure deployment to use nonprod cluster Note: To pull the image, Kubernetes will need authorization for GitHub. One way to verify you have authorization is to create an encoded dockerconfigjson secret using a Personal Access Token. The encoded dockerconfigjson string can be passed as an environment variable to Helm. More information about creating and encoding the dockerconfigjson string Creating a DOCKERCONFIGJSON string First make a PAT with the permission to read access for repositories 1 $ echo -n \"<github_username>:<github_pat>\" | base64 The above command will create an , with the encoded auth string run: 1 $ echo -n '{\"auths\":{\"ghcr.io\":{\"auth\":\"<encoded_auth_string>\"}}}' | base64 This will output the which you can then add to the .env file: 1 DOCKERCONFIGJSON=<encoded_dockerjsonconfig_string> Create Environment Variables for Secrets Create .env file with your environment variables; you will need to set all of these variables in order for the deployment to work. 1 2 3 4 5 6 7 8 9 DOCKERCONFIGJSON=<base64 encoded json string> GH_TOKEN=<github_token> HOST=<host url> GH_CLIENT_ID=<GH OAuth Client ID> GH_CLIENT_SECRET=<GH OAuth Client Secret> NONPROD=true BASE_URL=https://dev.devportal.name GATEWAY=istio-system/dev-devportal-name-gateway ... Export file contents 1 set -o allexport; source .env; set +o allexport Install the Helm chart and set secrets using --set 1 $ helm upgrade backstage-dev helm/lighthouse-backstage/ --debug --values helm/lighthouse-backstage/values.yaml --namespace lighthouse-bandicoot-dev --set DOCKERCONFIGJSON=$DOCKERCONFIGJSON --set GH_TOKEN=$GH_TOKEN --set HOST=$HOST --set GH_CLIENT_ID=$GH_CLIENT_ID --set GH_CLIENT_SECRET=$GH_CLIENT_SECRET --set nonprod=$NONPROD --set BASE_URL=$BASE_URL --set GATEWAY=$GATEWAY --set BACKEND_PORT=5432 --install --atomic --cleanup-on-fail --history-max 5","title":"Install Helm Chart"},{"location":"dev-deploy/#verify-deployment","text":"Through Terminal 1 2 3 $ helm list -n lighthouse-bandicoot-dev NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION backstage-dev lighthouse-bandicoot-dev 1 2021-10-07 07:37:22.4745171 -0700 PDT deployed lighthouse-backstage-0.1.0 1.16.0 Visit https://dev.devportal.name to view the running application","title":"Verify Deployment"},{"location":"gha-dependencies/","text":"GitHub Action Dependencies \u00b6 Actions \u00b6 Checkout uses: actions/checkout@v2 This action checks-out your repository under $GITHUB_WORKSPACE , so your workflow can access it. Upload a Build Artifact uses: actions/upload-artifact@v2 This uploads artifacts from your workflow allowing you to share data between jobs and store data once a workflow is complete. Download a Build Artifact uses: actions/download-artifact@v2 This downloads artifacts from your build GitHub Script uses: actions/github-script@v3 This action makes it easy to quickly write a script in your workflow that uses the GitHub API and the workflow run context. Setup Node.js Environment uses: actions/setup-node@v1 This action provides the following functionality for GitHub Action users: Optionally downloading and caching distribution of the requested Node.js version, and adding it to the PATH. Optionally caching npm/yarn/pnpm dependencies. Registering problem matchers for error output. Configuring authentication for GPR or NPM. Setup Java JDK uses: actions/setup-java@v2 This action provides the following functionality for GitHub Actions runners: Downloading and setting up a requested version of Java. See Usage for a list of supported distributions Extracting and caching custom version of Java from a local file Configuring runner for publishing using Apache Maven Configuring runner for publishing using Gradle Configuring runner for using GPG private key Registering problem matchers for error output Caching dependencies managed by Apache Maven Caching dependencies managed by Gradle Cache uses: actions/cache@v2 This action allows caching dependencies and build outputs to improve workflow execution time. GitHub \u00b6 CodeQL Action uses: github/codeql-action/upload-sarif@v1 This actio runs GitHub's industry-leading semantic code analysis engine, CodeQL, against a repository's source code to find security vulnerabilities. It then automatically uploads the results to GitHub so they can be displayed in the repository's security tab. CodeQL runs an extensible set of queries, which have been developed by the community and the GitHub Security Lab to find common vulnerabilities in your code. Cypress-io \u00b6 cypress-io/github-action uses: cypress-io/github-action@v2 GitHub Action for running Cypress end-to-end tests. Includes NPM installation, custom caching and lots of configuration options. DataDog \u00b6 Datadog Action uses: masci/datadog@v1 This action lets you send events and metrics to Datadog from a GitHub workflow. Docker \u00b6 Docker Setup QEMU uses: docker/setup-qemu-action@v1 GitHub Action to install QEMU static binaries. Docker Setup Buildx uses: docker/setup-buildx-action@v1 GitHub Action to set up Docker Buildx . This action will create and boot a builder that can be used in the following steps of your workflow if you're using buildx. By default, the docker-container builder driver will e used to be able to build multi-platform images and export cache thanks to the BuildKit container. Docker Login uses: docker/login-action@v1 GitHub Action to login against a Docker registry. Build and Push Docker Images uses: docker/build-push-action@v2 GitHub Action to build and push Docker images with Buildx with full support of the features provided by Moby BuildKit builder toolkit. This includes multi-platform build, secrets, remote cache, etc. and different builder deployment/namespacing options. Docker Metadata action uses: docker/metadata-action@98669ae865ea3cffbcbaa878cf57c20bbf1c6c38 GitHub Action to extract metadata from Git reference and GitHub events. This action is particularly useful if used with Docker Build Push action to tag and label Docker images. Azure \u00b6 Kubernetes Set Context uses: azure/k8s-set-context@v1 This action can be used to set cluster context before other actions like azure/k8s-deploy , azure/k8s-create-secret or any kubectl commands (in script) can be run subsequently in the workflow. Changesets \u00b6 Changesets Release Action uses: changesets/action@c3d88279fd9abe11410de073005e34863f735b1c This action for Changesets creates a pull request with all of the package versions updated and changelogs updated and when there are new changesets on master, the PR will be updated. When you're ready, you can merge the pull request and you can either publish the packages to npm manually or setup the action to do it for you. Other \u00b6 Create Pull Request uses: peter-evans/create-pull-request@ebc5e0258578dc3e4b0da1f649c75ec39fb6a1f4 A GitHub action to create a pull request for changes to your repository in the actions workspace. Very Good Coverage uses: VeryGoodOpenSource/very_good_coverage@cfe8b79401ea7689953705f0d161cb51113f346f A Github Action which helps enforce a minimum code coverage threshold. GitHub Pages Deploy Action uses: JamesIves/github-pages-deploy-action@4.1.5 This GitHub Action will automatically deploy your project to GitHub Pages . It can be configured to push your production-ready code into any branch you'd like, including gh-pages and docs. It can also handle cross repository deployments and works with GitHub Enterprise too.","title":"GHA Dependencies"},{"location":"gha-dependencies/#github-action-dependencies","text":"","title":"GitHub Action Dependencies"},{"location":"gha-dependencies/#actions","text":"Checkout uses: actions/checkout@v2 This action checks-out your repository under $GITHUB_WORKSPACE , so your workflow can access it. Upload a Build Artifact uses: actions/upload-artifact@v2 This uploads artifacts from your workflow allowing you to share data between jobs and store data once a workflow is complete. Download a Build Artifact uses: actions/download-artifact@v2 This downloads artifacts from your build GitHub Script uses: actions/github-script@v3 This action makes it easy to quickly write a script in your workflow that uses the GitHub API and the workflow run context. Setup Node.js Environment uses: actions/setup-node@v1 This action provides the following functionality for GitHub Action users: Optionally downloading and caching distribution of the requested Node.js version, and adding it to the PATH. Optionally caching npm/yarn/pnpm dependencies. Registering problem matchers for error output. Configuring authentication for GPR or NPM. Setup Java JDK uses: actions/setup-java@v2 This action provides the following functionality for GitHub Actions runners: Downloading and setting up a requested version of Java. See Usage for a list of supported distributions Extracting and caching custom version of Java from a local file Configuring runner for publishing using Apache Maven Configuring runner for publishing using Gradle Configuring runner for using GPG private key Registering problem matchers for error output Caching dependencies managed by Apache Maven Caching dependencies managed by Gradle Cache uses: actions/cache@v2 This action allows caching dependencies and build outputs to improve workflow execution time.","title":"Actions"},{"location":"gha-dependencies/#github","text":"CodeQL Action uses: github/codeql-action/upload-sarif@v1 This actio runs GitHub's industry-leading semantic code analysis engine, CodeQL, against a repository's source code to find security vulnerabilities. It then automatically uploads the results to GitHub so they can be displayed in the repository's security tab. CodeQL runs an extensible set of queries, which have been developed by the community and the GitHub Security Lab to find common vulnerabilities in your code.","title":"GitHub"},{"location":"gha-dependencies/#cypress-io","text":"cypress-io/github-action uses: cypress-io/github-action@v2 GitHub Action for running Cypress end-to-end tests. Includes NPM installation, custom caching and lots of configuration options.","title":"Cypress-io"},{"location":"gha-dependencies/#datadog","text":"Datadog Action uses: masci/datadog@v1 This action lets you send events and metrics to Datadog from a GitHub workflow.","title":"DataDog"},{"location":"gha-dependencies/#docker","text":"Docker Setup QEMU uses: docker/setup-qemu-action@v1 GitHub Action to install QEMU static binaries. Docker Setup Buildx uses: docker/setup-buildx-action@v1 GitHub Action to set up Docker Buildx . This action will create and boot a builder that can be used in the following steps of your workflow if you're using buildx. By default, the docker-container builder driver will e used to be able to build multi-platform images and export cache thanks to the BuildKit container. Docker Login uses: docker/login-action@v1 GitHub Action to login against a Docker registry. Build and Push Docker Images uses: docker/build-push-action@v2 GitHub Action to build and push Docker images with Buildx with full support of the features provided by Moby BuildKit builder toolkit. This includes multi-platform build, secrets, remote cache, etc. and different builder deployment/namespacing options. Docker Metadata action uses: docker/metadata-action@98669ae865ea3cffbcbaa878cf57c20bbf1c6c38 GitHub Action to extract metadata from Git reference and GitHub events. This action is particularly useful if used with Docker Build Push action to tag and label Docker images.","title":"Docker"},{"location":"gha-dependencies/#azure","text":"Kubernetes Set Context uses: azure/k8s-set-context@v1 This action can be used to set cluster context before other actions like azure/k8s-deploy , azure/k8s-create-secret or any kubectl commands (in script) can be run subsequently in the workflow.","title":"Azure"},{"location":"gha-dependencies/#changesets","text":"Changesets Release Action uses: changesets/action@c3d88279fd9abe11410de073005e34863f735b1c This action for Changesets creates a pull request with all of the package versions updated and changelogs updated and when there are new changesets on master, the PR will be updated. When you're ready, you can merge the pull request and you can either publish the packages to npm manually or setup the action to do it for you.","title":"Changesets"},{"location":"gha-dependencies/#other","text":"Create Pull Request uses: peter-evans/create-pull-request@ebc5e0258578dc3e4b0da1f649c75ec39fb6a1f4 A GitHub action to create a pull request for changes to your repository in the actions workspace. Very Good Coverage uses: VeryGoodOpenSource/very_good_coverage@cfe8b79401ea7689953705f0d161cb51113f346f A Github Action which helps enforce a minimum code coverage threshold. GitHub Pages Deploy Action uses: JamesIves/github-pages-deploy-action@4.1.5 This GitHub Action will automatically deploy your project to GitHub Pages . It can be configured to push your production-ready code into any branch you'd like, including gh-pages and docs. It can also handle cross repository deployments and works with GitHub Enterprise too.","title":"Other"},{"location":"github-packages/","text":"lighthouse-backstage: GitHub Packages \u00b6 Getting Started \u00b6 First, you must create a Personal Access Token in order to use GitHub Packages. You can do that by following this link . The Personal Access Token(PAT) should have the following permissions: repo repo:status repo_deployment public_repo repo:invite security_events write:packages read:packages Once created you will need to use the token. There are two ways to do this. Either create a .npmrc file or login using the CLI. .npmrc Note: This file is ignored by git and will not be pushed to the repo Create a .npmrc file and add the following text. 1 2 // .npmrc //npm.pkg.github.com/:_authToken=<TOKEN> CLI You can use the npm login command. 1 2 3 4 5 npm login --scope = @OWNER --registry = https:/npm.pkg.github.com > Username: USERNAME > Password: TOKEN > Email: PUBLIC_EMAIL_ADDRESS You can verify that your information is correct by running the following command: npm whoami --registry https://npm.pkg.github.com This should return your GitHub username. Pulling Dependencies \u00b6 Pulling dependencies from GitHub Packages should work as long as you've authenticated your Personal Access Token (above) and that your account has access to the organization that your trying to pull from. GitHub Actions must use a Personal Access Token to install private GitHub Packages. Publishing Packages \u00b6 Publishing packages to GitHub Packages requires some settings within a packages package.json file. Note: Packages that belong to a namespace must include the namespace within the package name. 1 2 3 4 5 6 7 { \"name\" : \"@example/package-name\" , \"publishConfig\" : { \"registry\" : \"https://npm.pkg.github.com\" // Tells NPM we are publishi n g t o t he Gi t Hub Regis tr y }, \"repository\" : \"git://github.com/example/package-name.git\" // Mus t ma t ch t he URL o f t he reposi t ory } Now we can publish the package by running npm publish . Assuming everything was configured correctly you should be able to check the packages listed in the repo or within the account owner. GitHub Actions can publish private GitHub Packages by using a GITHUB_TOKEN . Reference \u00b6 Personal Access Token GitHub Packages Department of Veteran Affairs Packages Article: How to Use GitHub Packages for a Private npm Registry","title":"GitHub Packages"},{"location":"github-packages/#lighthouse-backstage-github-packages","text":"","title":"lighthouse-backstage: GitHub Packages"},{"location":"github-packages/#getting-started","text":"First, you must create a Personal Access Token in order to use GitHub Packages. You can do that by following this link . The Personal Access Token(PAT) should have the following permissions: repo repo:status repo_deployment public_repo repo:invite security_events write:packages read:packages Once created you will need to use the token. There are two ways to do this. Either create a .npmrc file or login using the CLI. .npmrc Note: This file is ignored by git and will not be pushed to the repo Create a .npmrc file and add the following text. 1 2 // .npmrc //npm.pkg.github.com/:_authToken=<TOKEN> CLI You can use the npm login command. 1 2 3 4 5 npm login --scope = @OWNER --registry = https:/npm.pkg.github.com > Username: USERNAME > Password: TOKEN > Email: PUBLIC_EMAIL_ADDRESS You can verify that your information is correct by running the following command: npm whoami --registry https://npm.pkg.github.com This should return your GitHub username.","title":"Getting Started"},{"location":"github-packages/#pulling-dependencies","text":"Pulling dependencies from GitHub Packages should work as long as you've authenticated your Personal Access Token (above) and that your account has access to the organization that your trying to pull from. GitHub Actions must use a Personal Access Token to install private GitHub Packages.","title":"Pulling Dependencies"},{"location":"github-packages/#publishing-packages","text":"Publishing packages to GitHub Packages requires some settings within a packages package.json file. Note: Packages that belong to a namespace must include the namespace within the package name. 1 2 3 4 5 6 7 { \"name\" : \"@example/package-name\" , \"publishConfig\" : { \"registry\" : \"https://npm.pkg.github.com\" // Tells NPM we are publishi n g t o t he Gi t Hub Regis tr y }, \"repository\" : \"git://github.com/example/package-name.git\" // Mus t ma t ch t he URL o f t he reposi t ory } Now we can publish the package by running npm publish . Assuming everything was configured correctly you should be able to check the packages listed in the repo or within the account owner. GitHub Actions can publish private GitHub Packages by using a GITHUB_TOKEN .","title":"Publishing Packages"},{"location":"github-packages/#reference","text":"Personal Access Token GitHub Packages Department of Veteran Affairs Packages Article: How to Use GitHub Packages for a Private npm Registry","title":"Reference"},{"location":"helm/","text":"Helm \u00b6 What does Helm do for us? \u00b6 Helm is referred to as a \"Kubernetes Package Manager\", it is a command line tool that creates and installs Helm Charts. Helm Charts provide us with templated versions of the configuration files that describe our kubernetes deployment. In addition to templates, the Chart contains a set of files that define values that will be combined with the templates to generate our Kubernetes files. Using Helm will allow us to deploy to multiple environments without the need to maintain multiple sets of Kubernetes configuration files. Helm Docs File structure of helm chart \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 . \u2514\u2500\u2500 mychart \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 values.yaml \u251c\u2500\u2500 charts/ \u2514\u2500\u2500 templates/ \u251c\u2500\u2500 _helpers.tpl \u251c\u2500\u2500 deployment.yaml \u251c\u2500\u2500 service.yaml \u251c\u2500\u2500 ingress.yaml \u251c\u2500\u2500 configmap.yaml \u2514\u2500\u2500 otherK8smanifestfiles.yaml The mychart/templates/ directory contains templated versions all the yaml files that will be used for our Kubernetes deployment as well as a _helpers.tpl file. Using the helm install CLI will use the templates inconjunction with the Chart.yaml and values.yaml to render the Kubernetes files for deployment. values.yaml file \u00b6 This file contains the default values for a chart. These values may be overridden by users during helm install or helm upgrade. Chart.yaml \u00b6 The Chart.yaml file contains a description of the chart. You can access it from within a template. The charts/ directory may contain other charts called subcharts. Subcharts \u00b6 More information about subcharts 1 2 3 4 - A subchart is considered \"stand-alone\", which means a subchart can never explicitly depend on its parent chart. - For that reason, a subchart cannot access the values of its parent. - A parent chart can override values for subcharts. - Helm has a concept of global values that can be accessed by all charts. What is _helpers.tpl? \u00b6 Files that being with _ inside the templates/ directory are not rendered to Kubernetes object definitions, but these files are available everywhere within other chart templates for use. This file is default location for template partials . Create a partial template containing a block of labels 1 2 3 4 5 6 # templates/_helpers.tpl {{- define \"mychart.labels\" }} labels: generator: helm date: {{ now | htmlDate }} {{- end }} Adding the partial template to the template file 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # templates/configmap.yaml {{- define \"mychart.labels\" }} labels: generator: helm date: {{ now | htmlDate }} {{- end }} apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-configmap {{- template \"mychart.labels\" }} data: myvalue: \"Hello World\" {{- range $key, $val := .Values.favorite }} {{ $key }}: {{ $val | quote }} {{- end }} The rendered output: 1 2 3 4 5 6 7 8 9 10 11 12 # Source: templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: running-panda-configmap labels: generator: helm date: 2016-11-02 data: myvalue: \"Hello World\" drink: \"coffee\" food: \"pizza\" Using Secrets with Helm \u00b6 Passing secrets as command line arguments \u00b6 Currently I am passing secrets as command line arguments when installing helm charts How this works: - First I assigned variable names as values for the following key : value pairs in the values.yaml file: # values.yaml secrets: DOCKERCONFIGJSON : ${DOCKERCONFIGJSON} GITHUB_TOKEN : ${GITHUB_TOKEN} Then these keys are referenced in the deployment.yaml : kind: Secret type: kubernetes.io/dockerconfigjson apiVersion: v1 metadata: name: {{ include \"lighthouse-backstage.fullname\" . }}-dockerconfigjson-ghpkgs labels: {{- include \"lighthouse-backstage.selectorLabels\" . | nindent 8 }} data: .dockerconfigjson: {{ .Values. DOCKERCONFIGJSON }} apiVersion: v1 kind: Secret metadata: name: {{ include \"lighthouse-backstage.fullname\" . }}-secrets labels: {{- include \"lighthouse-backstage.selectorLabels\" . | nindent 8 }} type: Opaque data: GITHUB_TOKEN: {{ .Values. GITHUB_TOKEN }} Note: Kubernetes Secrets have different types. The first secret type is kubernetes.io/dockerconfigjson which is used for a docker config.json file represented as an encoded string. This is used to provide authentication credentials for accessing a container repository to be able to pull images. More information about creating and encoding the json string: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/ . The second secret is a base64 encoded personal access token used by Backstage . Export local variables \u00b6 Now that the environment variables are mapped to keys, and those keys are referenced in templates/deployment.yaml , we need to export the values for these variables - Create a .env file containing your secrets 1 2 3 4 5 6 7 8 DOCKERCONFIGJSON=<base64 encoded json string> GITHUB_TOKEN=<base64 encoded github_token> POSTGRES_USER=<base64 encoded postgres username> POSTGRES_PASSWORD=<base64 encoded postgres password> POSTGRES_DB=<base64 encoded postgres database name> HOST=<host url> GH_CLIENT_ID=<GH OAuth Client ID> GH_CLIENT_SECRET=<GH OAuth Client Secret> - Export the contents of the file 1 set -o allexport; source .env; set +o allexport - Set the variables when installing the helm chart 1 $ helm install backstage-dev helm/lighthouse-backstage/ --debug --values helm/lighthouse-backstage/values.yaml --namespace lighthouse-bandicoot-dev --set DOCKERCONFIGJSON=$DOCKERCONFIGJSON --set GITHUB_TOKEN=$GITHUB_TOKEN --set POSTGRES_USER=$POSTGRES_USER --set POSTGRES_PASSWORD=$POSTGRES_PASSWORD --set POSTGRES_DB=$POSTGRES_DB --set HOST=$HOST --set GH_CLIENT_ID=$GH_CLIENT_ID --set GH_CLIENT_SECRET=$GH_CLIENT_SECRET - View deployment 1 2 3 $ helm list -n lighthouse-bandicoot-dev NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION backstage-dev lighthouse-bandicoot-dev 1 2021-10-07 07:37:22.4745171 -0700 PDT deployed lighthouse-backstage-0.1.0 1.16.0 Other ways to use secrets with Helm \u00b6 helm-secrets plugin This approach uses SOPS to encrypt a secrets.yaml file and seems to be one of the more popular options I found, it is listed on Helms website as well: https://helm.sh/docs/community/related/#helm-plugins Hashicorp's Vault This is a more \"heavy handed\" solution. It uses it own set of helm charts to deploy a vault pod and a vault-agent-injector pod to store and inject secrets into our application pods.","title":"Helm Charts"},{"location":"helm/#helm","text":"","title":"Helm"},{"location":"helm/#what-does-helm-do-for-us","text":"Helm is referred to as a \"Kubernetes Package Manager\", it is a command line tool that creates and installs Helm Charts. Helm Charts provide us with templated versions of the configuration files that describe our kubernetes deployment. In addition to templates, the Chart contains a set of files that define values that will be combined with the templates to generate our Kubernetes files. Using Helm will allow us to deploy to multiple environments without the need to maintain multiple sets of Kubernetes configuration files. Helm Docs","title":"What does Helm do for us?"},{"location":"helm/#file-structure-of-helm-chart","text":"1 2 3 4 5 6 7 8 9 10 11 12 . \u2514\u2500\u2500 mychart \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 values.yaml \u251c\u2500\u2500 charts/ \u2514\u2500\u2500 templates/ \u251c\u2500\u2500 _helpers.tpl \u251c\u2500\u2500 deployment.yaml \u251c\u2500\u2500 service.yaml \u251c\u2500\u2500 ingress.yaml \u251c\u2500\u2500 configmap.yaml \u2514\u2500\u2500 otherK8smanifestfiles.yaml The mychart/templates/ directory contains templated versions all the yaml files that will be used for our Kubernetes deployment as well as a _helpers.tpl file. Using the helm install CLI will use the templates inconjunction with the Chart.yaml and values.yaml to render the Kubernetes files for deployment.","title":"File structure of helm chart"},{"location":"helm/#valuesyaml-file","text":"This file contains the default values for a chart. These values may be overridden by users during helm install or helm upgrade.","title":"values.yaml file"},{"location":"helm/#chartyaml","text":"The Chart.yaml file contains a description of the chart. You can access it from within a template. The charts/ directory may contain other charts called subcharts.","title":"Chart.yaml"},{"location":"helm/#subcharts","text":"More information about subcharts 1 2 3 4 - A subchart is considered \"stand-alone\", which means a subchart can never explicitly depend on its parent chart. - For that reason, a subchart cannot access the values of its parent. - A parent chart can override values for subcharts. - Helm has a concept of global values that can be accessed by all charts.","title":"Subcharts"},{"location":"helm/#what-is-_helperstpl","text":"Files that being with _ inside the templates/ directory are not rendered to Kubernetes object definitions, but these files are available everywhere within other chart templates for use. This file is default location for template partials . Create a partial template containing a block of labels 1 2 3 4 5 6 # templates/_helpers.tpl {{- define \"mychart.labels\" }} labels: generator: helm date: {{ now | htmlDate }} {{- end }} Adding the partial template to the template file 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # templates/configmap.yaml {{- define \"mychart.labels\" }} labels: generator: helm date: {{ now | htmlDate }} {{- end }} apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-configmap {{- template \"mychart.labels\" }} data: myvalue: \"Hello World\" {{- range $key, $val := .Values.favorite }} {{ $key }}: {{ $val | quote }} {{- end }} The rendered output: 1 2 3 4 5 6 7 8 9 10 11 12 # Source: templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: running-panda-configmap labels: generator: helm date: 2016-11-02 data: myvalue: \"Hello World\" drink: \"coffee\" food: \"pizza\"","title":"What is _helpers.tpl?"},{"location":"helm/#using-secrets-with-helm","text":"","title":"Using Secrets with Helm"},{"location":"helm/#passing-secrets-as-command-line-arguments","text":"Currently I am passing secrets as command line arguments when installing helm charts How this works: - First I assigned variable names as values for the following key : value pairs in the values.yaml file: # values.yaml secrets: DOCKERCONFIGJSON : ${DOCKERCONFIGJSON} GITHUB_TOKEN : ${GITHUB_TOKEN} Then these keys are referenced in the deployment.yaml : kind: Secret type: kubernetes.io/dockerconfigjson apiVersion: v1 metadata: name: {{ include \"lighthouse-backstage.fullname\" . }}-dockerconfigjson-ghpkgs labels: {{- include \"lighthouse-backstage.selectorLabels\" . | nindent 8 }} data: .dockerconfigjson: {{ .Values. DOCKERCONFIGJSON }} apiVersion: v1 kind: Secret metadata: name: {{ include \"lighthouse-backstage.fullname\" . }}-secrets labels: {{- include \"lighthouse-backstage.selectorLabels\" . | nindent 8 }} type: Opaque data: GITHUB_TOKEN: {{ .Values. GITHUB_TOKEN }} Note: Kubernetes Secrets have different types. The first secret type is kubernetes.io/dockerconfigjson which is used for a docker config.json file represented as an encoded string. This is used to provide authentication credentials for accessing a container repository to be able to pull images. More information about creating and encoding the json string: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/ . The second secret is a base64 encoded personal access token used by Backstage .","title":"Passing secrets as command line arguments"},{"location":"helm/#export-local-variables","text":"Now that the environment variables are mapped to keys, and those keys are referenced in templates/deployment.yaml , we need to export the values for these variables - Create a .env file containing your secrets 1 2 3 4 5 6 7 8 DOCKERCONFIGJSON=<base64 encoded json string> GITHUB_TOKEN=<base64 encoded github_token> POSTGRES_USER=<base64 encoded postgres username> POSTGRES_PASSWORD=<base64 encoded postgres password> POSTGRES_DB=<base64 encoded postgres database name> HOST=<host url> GH_CLIENT_ID=<GH OAuth Client ID> GH_CLIENT_SECRET=<GH OAuth Client Secret> - Export the contents of the file 1 set -o allexport; source .env; set +o allexport - Set the variables when installing the helm chart 1 $ helm install backstage-dev helm/lighthouse-backstage/ --debug --values helm/lighthouse-backstage/values.yaml --namespace lighthouse-bandicoot-dev --set DOCKERCONFIGJSON=$DOCKERCONFIGJSON --set GITHUB_TOKEN=$GITHUB_TOKEN --set POSTGRES_USER=$POSTGRES_USER --set POSTGRES_PASSWORD=$POSTGRES_PASSWORD --set POSTGRES_DB=$POSTGRES_DB --set HOST=$HOST --set GH_CLIENT_ID=$GH_CLIENT_ID --set GH_CLIENT_SECRET=$GH_CLIENT_SECRET - View deployment 1 2 3 $ helm list -n lighthouse-bandicoot-dev NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION backstage-dev lighthouse-bandicoot-dev 1 2021-10-07 07:37:22.4745171 -0700 PDT deployed lighthouse-backstage-0.1.0 1.16.0","title":"Export local variables"},{"location":"helm/#other-ways-to-use-secrets-with-helm","text":"helm-secrets plugin This approach uses SOPS to encrypt a secrets.yaml file and seems to be one of the more popular options I found, it is listed on Helms website as well: https://helm.sh/docs/community/related/#helm-plugins Hashicorp's Vault This is a more \"heavy handed\" solution. It uses it own set of helm charts to deploy a vault pod and a vault-agent-injector pod to store and inject secrets into our application pods.","title":"Other ways to use secrets with Helm"},{"location":"local-k8s/","text":"Deploying Backstage to local Kubernetes Cluster \u00b6 Using minikube and Kubernetes \u00b6 Prerequisites Install Minikube Install Docker Desktop: Mac , Windows Install Kubectl Configure app-config.yaml for postgres service \u00b6 1 2 3 4 5 connection: host: ${POSTGRES_SERVICE_HOST} port: ${POSTGRES_SERVICE_PORT} user: ${POSTGRES_USER} password: ${POSTGRES_PASSWORD} Create images for minikube \u00b6 Note: if you already have images in a container registery it is much quicker to use those directly, but if you're making changes locally you'll need to rebuild the images with your new changes and pass those to minikube. Modify .dockerignore \u00b6 Change .dockerignore to only include these lines: 1 2 node_modules packages/*/node_modules Build image for backend container \u00b6 1 yarn build && yarn build-image Build image for frontend container \u00b6 1 docker build . -f Dockerfile.dockerbuild --tag frontend Start up minikube \u00b6 Note: Need to have Docker running for minikube to be able to use docker driver 1 minikube start --driver=docker Load Images for minikube \u00b6 Note: Only need to do this if you are rebuilding images locally Check the image name or image ID you want to load to minikube with: 1 docker images Load the images with: 1 minikube image load <IMAGE_NAME or IMAGE_ID> Optional: you can verify minikube has access to the local image by checking its docker-env by opening a new terminal and running the commands below. If you don't see the image in the list, then you'll need to load it again or wait for it to finish loading. 1 eval $(minikube docker-env) && docker images When you're done, be sure to close this terminal or keep track of which terminal this is so you don't accidentally run commands using the docker daemon inside of the VM minikube uses. View K8s Dashboard \u00b6 Open a new terminal and run: 1 minikube dashboard Configure Kubernetes Secrets \u00b6 First you must encode the token value with base64: 1 echo -n \"token_string_here\" | base64 Copy the encoded value and edit /k8s/backstage-secrets.yaml to include the encoded token value: 1 GITHUB_TOKEN: <BASE64_ENCODED_GITHUB_TOKEN> Create Kubernetes Deployments/Services \u00b6 To create your Kubernetes deployments and services run: 1 kubectl apply -f k8s Now you can check your deployments, pods, and services using the minikube dashboard. Expose and View the Application with minikube tunnel \u00b6 Open a new terminal and start minikube tunnel 1 $ minikube tunnel In a separate terminal, check the service's external ip 1 2 3 4 $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE backstage-svc LoadBalancer 10.96.231.234 127.0.0.1 3000:30197/TCP,7000:30343/TCP 3s postgres ClusterIP 10.108.231.136 <none> 5432/TCP 2m6s View the application running at http://127.0.0.1:3000 Caveats This deployment creates 2 pods: one pod has a single container for the postgres db, and the second pod has 2 containers: one container for the frontend and another container for the backend. Right now with this configuration, backstage-svc is created as a LoadBalancer service so it is designated its own IP address that can be exposed with minikube tunnel . Running the command minikube tunnel creates a network route from the host to the service CIDR of the cluster using the cluster's IP address as a gateway. This exposes the external IP directly to any process running on the host operating system. Running Pulumi to Deploy FE to Kubernetes \u00b6 Through Codespaces \u00b6 Switch .devcontainer with .pulumidevcontainer - Run yarn swap-codespaces from the root directory to swap devcontainers - If a .pulumidevcontainer is present, this script will rename .devcontainer to .devcontainer_main and rename .pulumidevcontainer to .devcontainer - If there is no .pulumidevcontainer , then the script will rename .devcontainer to .pulumidevcontainer and rename .devcontainer_main to .devcontainer - Note: Changing the configuration of the Codespaces container will require the Codespaces container to be rebuilt. Rebuilding your container will delete any current changes you've made to the container Rebuild the Codespaces container - Use the Command Palette or the Codespaces extension to rebuild the container with the new .devcontainer Start Up Pulumi - Run yarn pulumi from the root directory to use the Pulumi start up script - This script will check if minikube is running, check if a frontend image is created and create a frontend image if it doesn't exist. Finally, the script will run the pulumi start up commands to generate & apply k8s manifest files and then forward ports with kubectl - After the script finishes, you should be able to see the FE running at localhost:3000 - Note: this will require creating a Pulumi account","title":"Deploy to Local Cluster (K8s/Minikube)"},{"location":"local-k8s/#deploying-backstage-to-local-kubernetes-cluster","text":"","title":"Deploying Backstage to local Kubernetes Cluster"},{"location":"local-k8s/#using-minikube-and-kubernetes","text":"Prerequisites Install Minikube Install Docker Desktop: Mac , Windows Install Kubectl","title":"Using minikube and Kubernetes"},{"location":"local-k8s/#configure-app-configyaml-for-postgres-service","text":"1 2 3 4 5 connection: host: ${POSTGRES_SERVICE_HOST} port: ${POSTGRES_SERVICE_PORT} user: ${POSTGRES_USER} password: ${POSTGRES_PASSWORD}","title":"Configure app-config.yaml for postgres service"},{"location":"local-k8s/#create-images-for-minikube","text":"Note: if you already have images in a container registery it is much quicker to use those directly, but if you're making changes locally you'll need to rebuild the images with your new changes and pass those to minikube.","title":"Create images for minikube"},{"location":"local-k8s/#modify-dockerignore","text":"Change .dockerignore to only include these lines: 1 2 node_modules packages/*/node_modules","title":"Modify .dockerignore"},{"location":"local-k8s/#build-image-for-backend-container","text":"1 yarn build && yarn build-image","title":"Build image for backend container"},{"location":"local-k8s/#build-image-for-frontend-container","text":"1 docker build . -f Dockerfile.dockerbuild --tag frontend","title":"Build image for frontend container"},{"location":"local-k8s/#start-up-minikube","text":"Note: Need to have Docker running for minikube to be able to use docker driver 1 minikube start --driver=docker","title":"Start up minikube"},{"location":"local-k8s/#load-images-for-minikube","text":"Note: Only need to do this if you are rebuilding images locally Check the image name or image ID you want to load to minikube with: 1 docker images Load the images with: 1 minikube image load <IMAGE_NAME or IMAGE_ID> Optional: you can verify minikube has access to the local image by checking its docker-env by opening a new terminal and running the commands below. If you don't see the image in the list, then you'll need to load it again or wait for it to finish loading. 1 eval $(minikube docker-env) && docker images When you're done, be sure to close this terminal or keep track of which terminal this is so you don't accidentally run commands using the docker daemon inside of the VM minikube uses.","title":"Load Images for minikube"},{"location":"local-k8s/#view-k8s-dashboard","text":"Open a new terminal and run: 1 minikube dashboard","title":"View K8s Dashboard"},{"location":"local-k8s/#configure-kubernetes-secrets","text":"First you must encode the token value with base64: 1 echo -n \"token_string_here\" | base64 Copy the encoded value and edit /k8s/backstage-secrets.yaml to include the encoded token value: 1 GITHUB_TOKEN: <BASE64_ENCODED_GITHUB_TOKEN>","title":"Configure Kubernetes Secrets"},{"location":"local-k8s/#create-kubernetes-deploymentsservices","text":"To create your Kubernetes deployments and services run: 1 kubectl apply -f k8s Now you can check your deployments, pods, and services using the minikube dashboard.","title":"Create Kubernetes Deployments/Services"},{"location":"local-k8s/#expose-and-view-the-application-with-minikube-tunnel","text":"Open a new terminal and start minikube tunnel 1 $ minikube tunnel In a separate terminal, check the service's external ip 1 2 3 4 $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE backstage-svc LoadBalancer 10.96.231.234 127.0.0.1 3000:30197/TCP,7000:30343/TCP 3s postgres ClusterIP 10.108.231.136 <none> 5432/TCP 2m6s View the application running at http://127.0.0.1:3000 Caveats This deployment creates 2 pods: one pod has a single container for the postgres db, and the second pod has 2 containers: one container for the frontend and another container for the backend. Right now with this configuration, backstage-svc is created as a LoadBalancer service so it is designated its own IP address that can be exposed with minikube tunnel . Running the command minikube tunnel creates a network route from the host to the service CIDR of the cluster using the cluster's IP address as a gateway. This exposes the external IP directly to any process running on the host operating system.","title":"Expose and View the Application with minikube tunnel"},{"location":"local-k8s/#running-pulumi-to-deploy-fe-to-kubernetes","text":"","title":"Running Pulumi to Deploy FE to Kubernetes"},{"location":"local-k8s/#through-codespaces","text":"Switch .devcontainer with .pulumidevcontainer - Run yarn swap-codespaces from the root directory to swap devcontainers - If a .pulumidevcontainer is present, this script will rename .devcontainer to .devcontainer_main and rename .pulumidevcontainer to .devcontainer - If there is no .pulumidevcontainer , then the script will rename .devcontainer to .pulumidevcontainer and rename .devcontainer_main to .devcontainer - Note: Changing the configuration of the Codespaces container will require the Codespaces container to be rebuilt. Rebuilding your container will delete any current changes you've made to the container Rebuild the Codespaces container - Use the Command Palette or the Codespaces extension to rebuild the container with the new .devcontainer Start Up Pulumi - Run yarn pulumi from the root directory to use the Pulumi start up script - This script will check if minikube is running, check if a frontend image is created and create a frontend image if it doesn't exist. Finally, the script will run the pulumi start up commands to generate & apply k8s manifest files and then forward ports with kubectl - After the script finishes, you should be able to see the FE running at localhost:3000 - Note: this will require creating a Pulumi account","title":"Through Codespaces"},{"location":"pre-build-validation/","text":"Prebuild Validation \u00b6 Pre-build Validation Workflow \u00b6 The pre-build validation workflow is defined in pre-build-validation.yml Name of workflow \u00b6 This is the name of the workflow as it appears in GitHub's actions tab of the GitHub Repository 1 name: Pre-build Validation Events \u00b6 This workflow can be triggered by two types of events: on pull requests with the main branch, and by other workflows. 1 2 3 4 on: pull_request: branches: [main] workflow_call: Jobs \u00b6 The jobs section contains 3 jobs: - unit-tests - This job references the workflow for our unit tests workflow . The commit SHA is used to identify which version of the workflow we want to use. If unit-tests.yml is updated, then the commit SHA will need to be updated in order for the pre-build validation workflow to use the correct version of unit-tests.yml . - validate-unit-tests - This job interprets the results of the unit test job. If the unit tests were successful, it logs a message and the link to the unit-test run. If the unit tests failed, it posts a message to the team-bandicoot Slack channel to alert the team engineers that a changes containing failing unit tests have been merged to the main branch. This job will only run when the pre-build validation workflow is triggered by the workflow_call event which only occurs on merges with the main branch. - validate linting - This job calls a composite action which invokes the Typescript compiler with yarn tsc and linting using yarn and lerna to call backstage-cli lint for each plugin and package. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 jobs: unit-tests: uses: department-of-veterans-affairs/lighthouse-backstage/.github/workflows/unit-tests.yml@b965a7d3fca3d4d4794cd3792ff72c08a7ba0364 validate-unit-tests: runs-on: ubuntu-latest needs: [unit-tests] if: ${{ github.event_name == 'workflow_call' }} steps: - uses: actions/checkout@v2 - uses: ./.github/actions/validate-unit-tests with: SLACK_UID_001: ${{ secrets.SLACK_UID_ABDUSAMAD }} SLACK_UID_002: ${{ secrets.SLACK_UID_FOWLER }} SLACK_UID_003: ${{ secrets.SLACK_UID_LOVENDAHL }} SLACK_UID_004: ${{ secrets.SLACK_UID_LUCKEY }} SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }} validate-linting: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - name: Install dependencies uses: ./.github/actions/install-dependencies - name: Run linting uses: ./.github/actions/validate-linting","title":"Pre-build Validation"},{"location":"pre-build-validation/#prebuild-validation","text":"","title":"Prebuild Validation"},{"location":"pre-build-validation/#pre-build-validation-workflow","text":"The pre-build validation workflow is defined in pre-build-validation.yml","title":"Pre-build Validation Workflow"},{"location":"pre-build-validation/#name-of-workflow","text":"This is the name of the workflow as it appears in GitHub's actions tab of the GitHub Repository 1 name: Pre-build Validation","title":"Name of workflow"},{"location":"pre-build-validation/#events","text":"This workflow can be triggered by two types of events: on pull requests with the main branch, and by other workflows. 1 2 3 4 on: pull_request: branches: [main] workflow_call:","title":"Events"},{"location":"pre-build-validation/#jobs","text":"The jobs section contains 3 jobs: - unit-tests - This job references the workflow for our unit tests workflow . The commit SHA is used to identify which version of the workflow we want to use. If unit-tests.yml is updated, then the commit SHA will need to be updated in order for the pre-build validation workflow to use the correct version of unit-tests.yml . - validate-unit-tests - This job interprets the results of the unit test job. If the unit tests were successful, it logs a message and the link to the unit-test run. If the unit tests failed, it posts a message to the team-bandicoot Slack channel to alert the team engineers that a changes containing failing unit tests have been merged to the main branch. This job will only run when the pre-build validation workflow is triggered by the workflow_call event which only occurs on merges with the main branch. - validate linting - This job calls a composite action which invokes the Typescript compiler with yarn tsc and linting using yarn and lerna to call backstage-cli lint for each plugin and package. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 jobs: unit-tests: uses: department-of-veterans-affairs/lighthouse-backstage/.github/workflows/unit-tests.yml@b965a7d3fca3d4d4794cd3792ff72c08a7ba0364 validate-unit-tests: runs-on: ubuntu-latest needs: [unit-tests] if: ${{ github.event_name == 'workflow_call' }} steps: - uses: actions/checkout@v2 - uses: ./.github/actions/validate-unit-tests with: SLACK_UID_001: ${{ secrets.SLACK_UID_ABDUSAMAD }} SLACK_UID_002: ${{ secrets.SLACK_UID_FOWLER }} SLACK_UID_003: ${{ secrets.SLACK_UID_LOVENDAHL }} SLACK_UID_004: ${{ secrets.SLACK_UID_LUCKEY }} SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }} validate-linting: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - name: Install dependencies uses: ./.github/actions/install-dependencies - name: Run linting uses: ./.github/actions/validate-linting","title":"Jobs"},{"location":"repo-setup/","text":"Standard Repo Setup \u00b6 Go to https://github.com/department-of-veterans-affairs Click the green new button and give repo a name in kebab case starting with the word lighthouse e.g. (lighthouse-repo-name). Leave radio button set to Internal. Select checkboxes for add README and .gitignore Click create repository Once the repo is created go into Settings -> Manage access and add the lighthouse-bandicoot team as admins. Then remove your username from admins. We want to only rely on team admin access. While still in Settings , go to Branches -> Branch protection rules -> Edit . Set options according to the below image: Click the green Save changes button at the bottom. Add a CODEOWNERS file to the root of the repo. See below for contents of file: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Lines starting with '#' are comments. # Each line is a file pattern followed by one or more owners. # More details are here: https://help.github.com/articles/about-codeowners/ # The '*' pattern is global owners. # Order is important. The last matching pattern has the most precedence. # The folders are ordered as follows: # In each subsection folders are ordered first by depth, then alphabetically. # This should make it easy to add new rules without breaking existing ones. # Global rule: * @department-of-veterans-affairs/lighthouse-bandicoot Check here for more details on CODEOWNERS files Add allstar to the repo. You cant do this directly. Go to https://github.com/department-of-veterans-affairs/github-user-requests/issues -> New issue -> General Help Request and click the green Get started button. a. Set title to Install Allstar GitHub App for <repo-name> b. In the Request Information field add the following: 1 Our team wanted to use the Allstar GitHub App to create and enforce custom security policies for our repo <repo-name> but it requires an organization owner to install it. Is it possible to have this app installed for our repo? Thanks 11. Click Submit new issue","title":"Standard Repo Setup"},{"location":"repo-setup/#standard-repo-setup","text":"Go to https://github.com/department-of-veterans-affairs Click the green new button and give repo a name in kebab case starting with the word lighthouse e.g. (lighthouse-repo-name). Leave radio button set to Internal. Select checkboxes for add README and .gitignore Click create repository Once the repo is created go into Settings -> Manage access and add the lighthouse-bandicoot team as admins. Then remove your username from admins. We want to only rely on team admin access. While still in Settings , go to Branches -> Branch protection rules -> Edit . Set options according to the below image: Click the green Save changes button at the bottom. Add a CODEOWNERS file to the root of the repo. See below for contents of file: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Lines starting with '#' are comments. # Each line is a file pattern followed by one or more owners. # More details are here: https://help.github.com/articles/about-codeowners/ # The '*' pattern is global owners. # Order is important. The last matching pattern has the most precedence. # The folders are ordered as follows: # In each subsection folders are ordered first by depth, then alphabetically. # This should make it easy to add new rules without breaking existing ones. # Global rule: * @department-of-veterans-affairs/lighthouse-bandicoot Check here for more details on CODEOWNERS files Add allstar to the repo. You cant do this directly. Go to https://github.com/department-of-veterans-affairs/github-user-requests/issues -> New issue -> General Help Request and click the green Get started button. a. Set title to Install Allstar GitHub App for <repo-name> b. In the Request Information field add the following: 1 Our team wanted to use the Allstar GitHub App to create and enforce custom security policies for our repo <repo-name> but it requires an organization owner to install it. Is it possible to have this app installed for our repo? Thanks 11. Click Submit new issue","title":"Standard Repo Setup"},{"location":"running-locally/","text":"Use Codespaces (preferred- work in progress) \u00b6 This repo is configured to run a production-like environment in a GitHub Codespace . Generate a personal access token with the read:packages permission Add the following to your Codespace secrets : For each secret, select only the lighthouse-backstage repository under Repository Access . 1 2 3 DEV_CONTAINER_REGISTRY_SERVER = ghcr.io DEV_CONTAINER_REGISTRY_USER = <GitHub username> DEV_CONTAINER_REGISTRY_PASSWORD = <personal access token> Create a Codespace for the lighthouse-backstage repo. This option is available by clicking the green Code button. Run application: 1 yarn dev Create a Codespace \u00b6 You can create a new codespace by visiting the lighthouse-backstage repo and by following the listed steps. Prerequisites : - Install Visual Studio Code - Install the Codespaces extension into Visual Studio Code Note: The in-browser VS Code doesn't work properly with this project. Click the <> green button near the search and \"+\" buttons. A dropdown should open up with two tabs, one for \"Local\" and one for \"Codespaces\", click \"Codespaces\". Click \"New codespace\" and then click \"Create codespace\". You'll be taken to a new window that sets up the codespace. Click the \"Open this codespace in VS Code Desktop\" button. Otherwise, click the 3 horizontal lines icon in the upper left sidebar and click \"Open in VS Code\". You can refresh the page if you don't see this icon. Install and run locally with Docker (work in progress) \u00b6 Prerequisites Install git Install Docker Desktop: Mac , Windows Run 1 sh local.sh start After the application runs for the first time, copy node_modules . While the application is running, run this is a separate terminal: 1 sh local.sh copy Caveats What does this do : This will install the application and its dependencies and then run the backend and frontend in separate containers. To ensure fast hot-reloading, node_modules and postgreSQL db are stored in a docker volume and your local source files are mounted into the container. Why do you need to copy after the first run : The application uses node_modules from Docker volume not your local files. Copy these locally so that dependencies resolve correctly in your editor. Environment Variable Injection The local dev environment is setup to use chamber for environment variable injection. A few changes need to be made before this will work. First you'll need to add a .env file within the .localdevcontainer folder. An example.env is available to copy from. Then, you need to uncomment the ENTRYPOINT located at the bottom of the local-Dockerfile, and comment ENTRYPOINT [ \"/entrypoint.sh\" ] . Running sh local.sh.start should now inject any environment variables stored within the SSM Parameter Store based on the values within .env . It'll grab any variables that start with lighthouse-backstage . Install and run locally (TBD) \u00b6 Use nvm to install node You will need to update the Backstage configuration for running locally. Update these instructions if you try this out. app-config.yaml is used for Codespaces and it is merged with app-config.production.yaml in production environments. Supporting Codespaces is the priorty so consider that when changing the way configurations are organized.","title":"Running Locally"},{"location":"running-locally/#use-codespaces-preferred-work-in-progress","text":"This repo is configured to run a production-like environment in a GitHub Codespace . Generate a personal access token with the read:packages permission Add the following to your Codespace secrets : For each secret, select only the lighthouse-backstage repository under Repository Access . 1 2 3 DEV_CONTAINER_REGISTRY_SERVER = ghcr.io DEV_CONTAINER_REGISTRY_USER = <GitHub username> DEV_CONTAINER_REGISTRY_PASSWORD = <personal access token> Create a Codespace for the lighthouse-backstage repo. This option is available by clicking the green Code button. Run application: 1 yarn dev","title":"Use Codespaces (preferred- work in progress)"},{"location":"running-locally/#create-a-codespace","text":"You can create a new codespace by visiting the lighthouse-backstage repo and by following the listed steps. Prerequisites : - Install Visual Studio Code - Install the Codespaces extension into Visual Studio Code Note: The in-browser VS Code doesn't work properly with this project. Click the <> green button near the search and \"+\" buttons. A dropdown should open up with two tabs, one for \"Local\" and one for \"Codespaces\", click \"Codespaces\". Click \"New codespace\" and then click \"Create codespace\". You'll be taken to a new window that sets up the codespace. Click the \"Open this codespace in VS Code Desktop\" button. Otherwise, click the 3 horizontal lines icon in the upper left sidebar and click \"Open in VS Code\". You can refresh the page if you don't see this icon.","title":"Create a Codespace"},{"location":"running-locally/#install-and-run-locally-with-docker-work-in-progress","text":"Prerequisites Install git Install Docker Desktop: Mac , Windows Run 1 sh local.sh start After the application runs for the first time, copy node_modules . While the application is running, run this is a separate terminal: 1 sh local.sh copy Caveats What does this do : This will install the application and its dependencies and then run the backend and frontend in separate containers. To ensure fast hot-reloading, node_modules and postgreSQL db are stored in a docker volume and your local source files are mounted into the container. Why do you need to copy after the first run : The application uses node_modules from Docker volume not your local files. Copy these locally so that dependencies resolve correctly in your editor. Environment Variable Injection The local dev environment is setup to use chamber for environment variable injection. A few changes need to be made before this will work. First you'll need to add a .env file within the .localdevcontainer folder. An example.env is available to copy from. Then, you need to uncomment the ENTRYPOINT located at the bottom of the local-Dockerfile, and comment ENTRYPOINT [ \"/entrypoint.sh\" ] . Running sh local.sh.start should now inject any environment variables stored within the SSM Parameter Store based on the values within .env . It'll grab any variables that start with lighthouse-backstage .","title":"Install and run locally with Docker (work in progress)"},{"location":"running-locally/#install-and-run-locally-tbd","text":"Use nvm to install node You will need to update the Backstage configuration for running locally. Update these instructions if you try this out. app-config.yaml is used for Codespaces and it is merged with app-config.production.yaml in production environments. Supporting Codespaces is the priorty so consider that when changing the way configurations are organized.","title":"Install and run locally (TBD)"},{"location":"starter-guide/","text":"Overview \u00b6 The Backstage Software Catalog is a centralized system that keeps track of ownership and metadata for all the software in your ecosystem (services, websites, libraries, data pipelines, etc). The catalog is built around the concept of metadata YAML files stored together with the code, which are then harvested and visualized in Backstage. More Information about Backstage's Software Catalog Adding a Catalog Entity \u00b6 Backstage identifies catalog entities by scanning every repository in an organization and looking for a catalog-info.yaml file in the root of the repository. The catalog-info.yaml file is a Catalog Entity Descriptor file is not only used to identify which repositories contain Catalog Entities, but it is also used to provide helpful information for other Backstage users who may wish to use your application. Creating an Entity Descriptor File \u00b6 In the root directory of your application, create a catalog-info.yaml file: 1 2 3 4 5 6 7 8 9 10 11 12 13 # Example catalog-info.yaml apiVersion: backstage.io/v1alpha1 kind: Component metadata: name: lighthouse-backstage description: An example of a Backstage application. annotations: backstage.io/techdocs-ref: url:https://github.com/department-of-veterans-affairs/lighthouse-backstage github.com/project-slug: department-of-veterans-affairs/lighthouse-backstage spec: type: website owner: lifecycle: experimental Navigate to Catalog on Developer Portal \u00b6 TODO: Instructions on how to access developer portal Search Catalog \u00b6 Search the Catalog to verify your application has been added to the Catalog. View Catalog Entity \u00b6 Once you find the new entry to the Catalog, you can select it to view more detailed information about the application. Additional Configuration Information \u00b6 Visit Backstage's documentation for more information about how to format catalog entity descriptor files.","title":"Starter Guide"},{"location":"starter-guide/#overview","text":"The Backstage Software Catalog is a centralized system that keeps track of ownership and metadata for all the software in your ecosystem (services, websites, libraries, data pipelines, etc). The catalog is built around the concept of metadata YAML files stored together with the code, which are then harvested and visualized in Backstage. More Information about Backstage's Software Catalog","title":"Overview"},{"location":"starter-guide/#adding-a-catalog-entity","text":"Backstage identifies catalog entities by scanning every repository in an organization and looking for a catalog-info.yaml file in the root of the repository. The catalog-info.yaml file is a Catalog Entity Descriptor file is not only used to identify which repositories contain Catalog Entities, but it is also used to provide helpful information for other Backstage users who may wish to use your application.","title":"Adding a Catalog Entity"},{"location":"starter-guide/#creating-an-entity-descriptor-file","text":"In the root directory of your application, create a catalog-info.yaml file: 1 2 3 4 5 6 7 8 9 10 11 12 13 # Example catalog-info.yaml apiVersion: backstage.io/v1alpha1 kind: Component metadata: name: lighthouse-backstage description: An example of a Backstage application. annotations: backstage.io/techdocs-ref: url:https://github.com/department-of-veterans-affairs/lighthouse-backstage github.com/project-slug: department-of-veterans-affairs/lighthouse-backstage spec: type: website owner: lifecycle: experimental","title":"Creating an Entity Descriptor File"},{"location":"starter-guide/#navigate-to-catalog-on-developer-portal","text":"TODO: Instructions on how to access developer portal","title":"Navigate to Catalog on Developer Portal"},{"location":"starter-guide/#search-catalog","text":"Search the Catalog to verify your application has been added to the Catalog.","title":"Search Catalog"},{"location":"starter-guide/#view-catalog-entity","text":"Once you find the new entry to the Catalog, you can select it to view more detailed information about the application.","title":"View Catalog Entity"},{"location":"starter-guide/#additional-configuration-information","text":"Visit Backstage's documentation for more information about how to format catalog entity descriptor files.","title":"Additional Configuration Information"},{"location":"ADR/ADR-template/","text":"Title \u00b6 Decision Made: yes/no Decision Date: mo year Revisit Decision: yes/no Date mo year Revisit criteria: Decision Made: No, but open to revisiting Decision Date: 12/2018 Revisit Decision: Yes Revisit Date: July 2019 Revisit Criteria: If a developer is interested in Jest and has time or suggestions for fixing the speed issues, we should revisit this. Decision Makers: @githubusername, @githubsername tl;dr \u00b6 Summary of problem and decision History \u00b6 Dates, links to the PRs, notes- anything relevant to understand the decision making process / evolution. Important to list what you tried here so avoid retreading Pros \u00b6 upside Cons \u00b6 downside Decision \u00b6 Detailed reasoning about decision Example","title":"Title"},{"location":"ADR/ADR-template/#title","text":"Decision Made: yes/no Decision Date: mo year Revisit Decision: yes/no Date mo year Revisit criteria: Decision Made: No, but open to revisiting Decision Date: 12/2018 Revisit Decision: Yes Revisit Date: July 2019 Revisit Criteria: If a developer is interested in Jest and has time or suggestions for fixing the speed issues, we should revisit this. Decision Makers: @githubusername, @githubsername","title":"Title"},{"location":"ADR/ADR-template/#tldr","text":"Summary of problem and decision","title":"tl;dr"},{"location":"ADR/ADR-template/#history","text":"Dates, links to the PRs, notes- anything relevant to understand the decision making process / evolution. Important to list what you tried here so avoid retreading","title":"History"},{"location":"ADR/ADR-template/#pros","text":"upside","title":"Pros"},{"location":"ADR/ADR-template/#cons","text":"downside","title":"Cons"},{"location":"ADR/ADR-template/#decision","text":"Detailed reasoning about decision Example","title":"Decision"},{"location":"ADR/code-cov/","text":"Ignore Backend Code Coverage for CI \u00b6 Decision Made: yes Decision Date: 09/2021 Revisit criteria: Decision Made: Yes Decision Date: 9/2021 Revisit Decision: No Revisit Criteria: None Decision Makers: @keyluck, @rianfowler tl;dr \u00b6 Ignore the backend package when testing for code coverage on PR's to the main branch because the result is always 0% coverage. History \u00b6 One of the required status checks our CI performs is testing code coverage of all our packages/plugins. A GitHub Action checks if a package or plugin's code coverage is above or below a certain threshold. If it is below the threshold, then the GitHub Action fails and branch protection prevents the Pull Request from being merged to the main branch. When running unit tests with the --coverage flag, the backend package coverage summary always reports 0% coverage. Currently, the backend package has one unit test, and will accurately report if the test passed or not, but will not detect any code coverage. Pros \u00b6 Ignoring the code coverage reporting for the backend will: - allow us to continue to enforce code coverage for all other plugins and packages - prevent blocking all Pull Requests from being merged to the main branch due to failed status checks Cons \u00b6 Ignoring the code coverage reporting for the backend will: - prevent us from knowing the true code coverage of the backend if we add unit-tests or plugins to the backend Decision \u00b6 Since we do not plan to add plugins to the backend package, we have decided to ignore the code coverage report generated for the backend when running unit tests. This will allow us to continue working and merging Pull Requests to the main branch while still using branch protection that enforces code coverage for all other plugins and packages.","title":"Ignore Backend Code Coverage"},{"location":"ADR/code-cov/#ignore-backend-code-coverage-for-ci","text":"Decision Made: yes Decision Date: 09/2021 Revisit criteria: Decision Made: Yes Decision Date: 9/2021 Revisit Decision: No Revisit Criteria: None Decision Makers: @keyluck, @rianfowler","title":"Ignore Backend Code Coverage for CI"},{"location":"ADR/code-cov/#tldr","text":"Ignore the backend package when testing for code coverage on PR's to the main branch because the result is always 0% coverage.","title":"tl;dr"},{"location":"ADR/code-cov/#history","text":"One of the required status checks our CI performs is testing code coverage of all our packages/plugins. A GitHub Action checks if a package or plugin's code coverage is above or below a certain threshold. If it is below the threshold, then the GitHub Action fails and branch protection prevents the Pull Request from being merged to the main branch. When running unit tests with the --coverage flag, the backend package coverage summary always reports 0% coverage. Currently, the backend package has one unit test, and will accurately report if the test passed or not, but will not detect any code coverage.","title":"History"},{"location":"ADR/code-cov/#pros","text":"Ignoring the code coverage reporting for the backend will: - allow us to continue to enforce code coverage for all other plugins and packages - prevent blocking all Pull Requests from being merged to the main branch due to failed status checks","title":"Pros"},{"location":"ADR/code-cov/#cons","text":"Ignoring the code coverage reporting for the backend will: - prevent us from knowing the true code coverage of the backend if we add unit-tests or plugins to the backend","title":"Cons"},{"location":"ADR/code-cov/#decision","text":"Since we do not plan to add plugins to the backend package, we have decided to ignore the code coverage report generated for the backend when running unit tests. This will allow us to continue working and merging Pull Requests to the main branch while still using branch protection that enforces code coverage for all other plugins and packages.","title":"Decision"},{"location":"ADR/cypress-e2e-tests/","text":"Cypress e2e Tests ADR \u00b6 Decision Made: yes Decision Date: 08/2021 Revisit criteria: Decision Made: Yes Revisit Decision: Not planned, but if another e2e testing frameworks become available. Revisit Criteria: If another e2e testing frameworks becomes available that is faster, or performs better, we should revisit this. Decision Makers: @KaemonIsland tl;dr \u00b6 Backstage should have e2e testing to ensure that various dependencies of an application are working accurately. Along with allowing refactoring, and bug fixing to be much faster and easier. There are various e2e testing frameworks available and we chose Cypress. History \u00b6 Backstage came with Cypress already installed . So it was easy to get started. Testing locally was a bit weird (Ran into an issue with missing dependencies). For Linux users, or those using WSL. You can install all of the Cypress dependencies by following this guide. Tests are also run as part of the CI/CD. Here is the PR adding them. Pros \u00b6 It's open source. Allows End-to-end, Integration, and Unit tests. Time Travel - Takes snapshots of tests. Debuggability - Debugging can be done using the Developer tools. Cross Browser Testing - Run tests on Firefox, chrome, edge, etc. Stubbing responses. Cons \u00b6 Can have difficulty setting up. (I've had trouble on multiple systems getting Cypress to run locally) Uses synthetic events instead of native ones. Stubbing can get out of hand fairly quickly. Bad support for iFrames. Usually have to use custom code. Decision \u00b6 Backstage came with Cypress installed and most of the current developers have had an enjoyable experience working with it. So we've decided to stick with it.","title":"Cypress e2e Tests"},{"location":"ADR/cypress-e2e-tests/#cypress-e2e-tests-adr","text":"Decision Made: yes Decision Date: 08/2021 Revisit criteria: Decision Made: Yes Revisit Decision: Not planned, but if another e2e testing frameworks become available. Revisit Criteria: If another e2e testing frameworks becomes available that is faster, or performs better, we should revisit this. Decision Makers: @KaemonIsland","title":"Cypress e2e Tests ADR"},{"location":"ADR/cypress-e2e-tests/#tldr","text":"Backstage should have e2e testing to ensure that various dependencies of an application are working accurately. Along with allowing refactoring, and bug fixing to be much faster and easier. There are various e2e testing frameworks available and we chose Cypress.","title":"tl;dr"},{"location":"ADR/cypress-e2e-tests/#history","text":"Backstage came with Cypress already installed . So it was easy to get started. Testing locally was a bit weird (Ran into an issue with missing dependencies). For Linux users, or those using WSL. You can install all of the Cypress dependencies by following this guide. Tests are also run as part of the CI/CD. Here is the PR adding them.","title":"History"},{"location":"ADR/cypress-e2e-tests/#pros","text":"It's open source. Allows End-to-end, Integration, and Unit tests. Time Travel - Takes snapshots of tests. Debuggability - Debugging can be done using the Developer tools. Cross Browser Testing - Run tests on Firefox, chrome, edge, etc. Stubbing responses.","title":"Pros"},{"location":"ADR/cypress-e2e-tests/#cons","text":"Can have difficulty setting up. (I've had trouble on multiple systems getting Cypress to run locally) Uses synthetic events instead of native ones. Stubbing can get out of hand fairly quickly. Bad support for iFrames. Usually have to use custom code.","title":"Cons"},{"location":"ADR/cypress-e2e-tests/#decision","text":"Backstage came with Cypress installed and most of the current developers have had an enjoyable experience working with it. So we've decided to stick with it.","title":"Decision"},{"location":"ADR/github-packages/","text":"GitHub Packages \u00b6 Decision Made: yes Decision Date: 11/21 Revisit Decision: no Date N/A Revisit criteria: Decision Made: Yes, but open to revisiting decision if a better private package registry becomes available. Revisit Decision: No Revisit Date: N/A Revisit Criteria: If a better private package registry becomes available, we should look into it. Decision Makers: @kaemonisland, @rianfowler tl;dr \u00b6 Private packages will now be pulled from GitHub Packages. User's must login to the github package registry by using their account that's associated with the Department of Veteran Affairs along with a Personal Access Token with all Repo and write:packages permissions. History \u00b6 Packages GitHub Packages Pros \u00b6 Our codebase is on GitHub, so our packages should be there too! Integrates with GH APIs, Actions, etc. Free for public packages Cons \u00b6 Any users that want to pull/push to private packages must create a Personal Access Token with repo and write:packages sections checked. Decision \u00b6 GitHub Packages will be used for managing private packages for the lighthouse-backstage repo. Packages","title":"GitHub Packages"},{"location":"ADR/github-packages/#github-packages","text":"Decision Made: yes Decision Date: 11/21 Revisit Decision: no Date N/A Revisit criteria: Decision Made: Yes, but open to revisiting decision if a better private package registry becomes available. Revisit Decision: No Revisit Date: N/A Revisit Criteria: If a better private package registry becomes available, we should look into it. Decision Makers: @kaemonisland, @rianfowler","title":"GitHub Packages"},{"location":"ADR/github-packages/#tldr","text":"Private packages will now be pulled from GitHub Packages. User's must login to the github package registry by using their account that's associated with the Department of Veteran Affairs along with a Personal Access Token with all Repo and write:packages permissions.","title":"tl;dr"},{"location":"ADR/github-packages/#history","text":"Packages GitHub Packages","title":"History"},{"location":"ADR/github-packages/#pros","text":"Our codebase is on GitHub, so our packages should be there too! Integrates with GH APIs, Actions, etc. Free for public packages","title":"Pros"},{"location":"ADR/github-packages/#cons","text":"Any users that want to pull/push to private packages must create a Personal Access Token with repo and write:packages sections checked.","title":"Cons"},{"location":"ADR/github-packages/#decision","text":"GitHub Packages will be used for managing private packages for the lighthouse-backstage repo. Packages","title":"Decision"},{"location":"ADR/split-fe-be/","text":"Split Frontend and Backend ADR \u00b6 Decision Made: yes Decision Date: 08/2021 Revisit criteria: Decision Made: Yes Revisit Decision: No Revisit Criteria: None Decision Makers: @KaemonIsland, @rianfowler tl;dr \u00b6 The backstage app will have separate frontend and backend containers in order to scale both individually. i.e. We won't have to create as many frontend instances as backend instances if there is more traffic going to the backend. History \u00b6 Originally, the lighthouse backstage docker images were setup to use a Multi-stage Build. Both the frontend and backend instances were built together, which worked fine however we wouldn't be taking advantage of scaling our application. It doesn't make sense to create more instances of the frontend if the backend is receiving a lot of traffic. Backstage also has a guide to setup a separate frontend that uses NGINX. Pros \u00b6 Frontend and Backend containers can be scaled individually. Easier to work on one without altering the other. Faster deployment Cons \u00b6 The build time for docker compose now takes longer. Decision \u00b6 The Frontend and Backend will run in separate containers in order to allow for individual scaling. PR","title":"Split FE/BE"},{"location":"ADR/split-fe-be/#split-frontend-and-backend-adr","text":"Decision Made: yes Decision Date: 08/2021 Revisit criteria: Decision Made: Yes Revisit Decision: No Revisit Criteria: None Decision Makers: @KaemonIsland, @rianfowler","title":"Split Frontend and Backend ADR"},{"location":"ADR/split-fe-be/#tldr","text":"The backstage app will have separate frontend and backend containers in order to scale both individually. i.e. We won't have to create as many frontend instances as backend instances if there is more traffic going to the backend.","title":"tl;dr"},{"location":"ADR/split-fe-be/#history","text":"Originally, the lighthouse backstage docker images were setup to use a Multi-stage Build. Both the frontend and backend instances were built together, which worked fine however we wouldn't be taking advantage of scaling our application. It doesn't make sense to create more instances of the frontend if the backend is receiving a lot of traffic. Backstage also has a guide to setup a separate frontend that uses NGINX.","title":"History"},{"location":"ADR/split-fe-be/#pros","text":"Frontend and Backend containers can be scaled individually. Easier to work on one without altering the other. Faster deployment","title":"Pros"},{"location":"ADR/split-fe-be/#cons","text":"The build time for docker compose now takes longer.","title":"Cons"},{"location":"ADR/split-fe-be/#decision","text":"The Frontend and Backend will run in separate containers in order to allow for individual scaling. PR","title":"Decision"},{"location":"ADR/twelve-factor-app/","text":"12-Factor App Checklist \u00b6 Decision Made: yes Decision Date: 11/21 Revisit Decision: no Date 12/15 Revisit criteria: Decision Made: Yes Revisit Decision: No, but open to taking another look, Revisit Date: November 2022 Revisit Criteria: If a developer is interested in revisiting this. Decision Makers: @rianfowler, @mhyder1, @keyluck, @KaemonIsland tl;dr \u00b6 This ADR walks through each of the 12 factor app checklist items. Embark does not meet criteria on 3 facets. We will add tickets for these items and revisit this ADR on 12/15 History \u00b6 We plan to use Kubernetes to deploy our application. The 12-factor app pattern is the ideal type of app to run on Kubernetes. 1. Codebase \u00b6 One codebase tracked in revision control, many deploys. Meets Criteria The codebase exists on GitHub and uses Git for version control. GitHub Actions are used to automate tests/builds/deploys along with requiring peer-approval in order to merge a PR. The codebase uses Docker for building the Frontend and Backend applications. 2. Dependencies \u00b6 Explicitly declare and isolate dependencies. Meets Criteria The app Frontend and Backend can each be run individually and are not reliant on each other. The package/plugin items each have their own package.json files to list each dependency. Each package/plugin uses semver to determine individual versions. 3. Config \u00b6 Store config in the environment. Meets Criteria Configuration settings use ENV variables and are not hardcoded. Kubernetes can automatically connect to the DB. We are able to update the config, without needing to create a new deploy. Steps to Meet Criteria Use ENV to determine Ports. 4. Backing Services \u00b6 Treat backing services as attached resources. Meets Criteria Uses object storage where files are needed, (Stores component entries within the DB) Uses external Postgres DB to persist state. Uses ENV variables to connect to the DB. Steps to Meet Criteria We do not use ENV vars to configure timeouts/endpoints. We should do a spike to add timeouts to processes. 5. Build, release, run \u00b6 Strictly separate build and run stages. Meets Criteria The build process is well defined within the docs , and within the GitHub Action. We use a Dockerfile and docker-compose.yml files to define the entrypoint for the frontend and backend. Uses tags to determine releases. 6. Processes \u00b6 Execute the app as one or more stateless processes. Meets Criteria Backend exposes a health check endpoint. Processes do not depend on a process manager. Health checks do not depend on the health of backing services, the backend will just report them. The backend will respond with exit code 1 if it comes across an error. Responds to SIGTERM and exits gracefully. 7. Port binding \u00b6 Export services via port binding. Meets Criteria The Frontend Dockerfile defines PORT 3000. Each service listens on a preconfigured bind-address port using the app-config.yml file. Listens on non-privileged ports. (> 1024) 8. Concurrency \u00b6 Scale out via the process model. Meets Criteria The app can be run any number of times in parallel. (Multiple backends or frontends when needed) The app uses database transactions. App doesn't depend on sticky sessions. Requests can hit any process. Steps to Meet Criteria Create Spike to document DB connections, and find out more about pool size. 9. Disposability \u00b6 Maximize robustness with fast startup and graceful shutdown. Meets Criteria Does not use local state for processes. Every action, other than feature-flags, uses the DB to persist state. Processes can be easily created/destroyed without orchestrated shutdown process. (Able to just cancel the frontend/backend without other actions.) Does not use POSIX filesystem for persistence. 10. Dev/prod parity \u00b6 Keep development, staging, and production as similar as possible. Meets Criteria All environments function the same way when configured with the same settings. Devs are able to run the app with the same settings and get the same experience. Flags can enable/disable functionality without knowledge of environment. There are no env checks within the app. 11. Logs \u00b6 Treat logs as event streams. Meets Criteria Logs are emitted to stdout . This is done by default when creating a backstage app. Events are structured event streams, in JSON. No logs are written to disk. 12. Admin processes \u00b6 Run admin/management tasks as one-off processes. Meets Criteria We don't have any cronjobs. Batch processing run as a separate container. (Building techdocs is moved out) Steps to Meet Criteria Goes along with Spike to learn more about the DB. Decision \u00b6 The application will use the 12-factor application checklist to determine if it is able to run effectively on kubernetes. Reference Links \u00b6 GitHub GitHub Actions 12-Factor app Kubernetes App Backend Deployment Health Check Frontend Dockerfile Logging to stdout","title":"12-Factor App Checklist"},{"location":"ADR/twelve-factor-app/#12-factor-app-checklist","text":"Decision Made: yes Decision Date: 11/21 Revisit Decision: no Date 12/15 Revisit criteria: Decision Made: Yes Revisit Decision: No, but open to taking another look, Revisit Date: November 2022 Revisit Criteria: If a developer is interested in revisiting this. Decision Makers: @rianfowler, @mhyder1, @keyluck, @KaemonIsland","title":"12-Factor App Checklist"},{"location":"ADR/twelve-factor-app/#tldr","text":"This ADR walks through each of the 12 factor app checklist items. Embark does not meet criteria on 3 facets. We will add tickets for these items and revisit this ADR on 12/15","title":"tl;dr"},{"location":"ADR/twelve-factor-app/#history","text":"We plan to use Kubernetes to deploy our application. The 12-factor app pattern is the ideal type of app to run on Kubernetes.","title":"History"},{"location":"ADR/twelve-factor-app/#1-codebase","text":"One codebase tracked in revision control, many deploys. Meets Criteria The codebase exists on GitHub and uses Git for version control. GitHub Actions are used to automate tests/builds/deploys along with requiring peer-approval in order to merge a PR. The codebase uses Docker for building the Frontend and Backend applications.","title":"1. Codebase"},{"location":"ADR/twelve-factor-app/#2-dependencies","text":"Explicitly declare and isolate dependencies. Meets Criteria The app Frontend and Backend can each be run individually and are not reliant on each other. The package/plugin items each have their own package.json files to list each dependency. Each package/plugin uses semver to determine individual versions.","title":"2. Dependencies"},{"location":"ADR/twelve-factor-app/#3-config","text":"Store config in the environment. Meets Criteria Configuration settings use ENV variables and are not hardcoded. Kubernetes can automatically connect to the DB. We are able to update the config, without needing to create a new deploy. Steps to Meet Criteria Use ENV to determine Ports.","title":"3. Config"},{"location":"ADR/twelve-factor-app/#4-backing-services","text":"Treat backing services as attached resources. Meets Criteria Uses object storage where files are needed, (Stores component entries within the DB) Uses external Postgres DB to persist state. Uses ENV variables to connect to the DB. Steps to Meet Criteria We do not use ENV vars to configure timeouts/endpoints. We should do a spike to add timeouts to processes.","title":"4. Backing Services"},{"location":"ADR/twelve-factor-app/#5-build-release-run","text":"Strictly separate build and run stages. Meets Criteria The build process is well defined within the docs , and within the GitHub Action. We use a Dockerfile and docker-compose.yml files to define the entrypoint for the frontend and backend. Uses tags to determine releases.","title":"5. Build, release, run"},{"location":"ADR/twelve-factor-app/#6-processes","text":"Execute the app as one or more stateless processes. Meets Criteria Backend exposes a health check endpoint. Processes do not depend on a process manager. Health checks do not depend on the health of backing services, the backend will just report them. The backend will respond with exit code 1 if it comes across an error. Responds to SIGTERM and exits gracefully.","title":"6. Processes"},{"location":"ADR/twelve-factor-app/#7-port-binding","text":"Export services via port binding. Meets Criteria The Frontend Dockerfile defines PORT 3000. Each service listens on a preconfigured bind-address port using the app-config.yml file. Listens on non-privileged ports. (> 1024)","title":"7. Port binding"},{"location":"ADR/twelve-factor-app/#8-concurrency","text":"Scale out via the process model. Meets Criteria The app can be run any number of times in parallel. (Multiple backends or frontends when needed) The app uses database transactions. App doesn't depend on sticky sessions. Requests can hit any process. Steps to Meet Criteria Create Spike to document DB connections, and find out more about pool size.","title":"8. Concurrency"},{"location":"ADR/twelve-factor-app/#9-disposability","text":"Maximize robustness with fast startup and graceful shutdown. Meets Criteria Does not use local state for processes. Every action, other than feature-flags, uses the DB to persist state. Processes can be easily created/destroyed without orchestrated shutdown process. (Able to just cancel the frontend/backend without other actions.) Does not use POSIX filesystem for persistence.","title":"9. Disposability"},{"location":"ADR/twelve-factor-app/#10-devprod-parity","text":"Keep development, staging, and production as similar as possible. Meets Criteria All environments function the same way when configured with the same settings. Devs are able to run the app with the same settings and get the same experience. Flags can enable/disable functionality without knowledge of environment. There are no env checks within the app.","title":"10. Dev/prod parity"},{"location":"ADR/twelve-factor-app/#11-logs","text":"Treat logs as event streams. Meets Criteria Logs are emitted to stdout . This is done by default when creating a backstage app. Events are structured event streams, in JSON. No logs are written to disk.","title":"11. Logs"},{"location":"ADR/twelve-factor-app/#12-admin-processes","text":"Run admin/management tasks as one-off processes. Meets Criteria We don't have any cronjobs. Batch processing run as a separate container. (Building techdocs is moved out) Steps to Meet Criteria Goes along with Spike to learn more about the DB.","title":"12. Admin processes"},{"location":"ADR/twelve-factor-app/#decision","text":"The application will use the 12-factor application checklist to determine if it is able to run effectively on kubernetes.","title":"Decision"},{"location":"ADR/twelve-factor-app/#reference-links","text":"GitHub GitHub Actions 12-Factor app Kubernetes App Backend Deployment Health Check Frontend Dockerfile Logging to stdout","title":"Reference Links"},{"location":"ADR/vite/","text":"Vite \u00b6 Decision Made: yes Decision Date: 10/21 Revisit Decision: yes Date 01/22 Revisit criteria: Decision Made: Yes, but open to revisiting Decision Date: 01/2022 Revisit Decision: Yes Revisit Date: 01/2022 Revisit Criteria: If Vite updates for better coverage or Backstage allows for better Browser/Client support. Decision Makers: @KaemonIsland tl;dr \u00b6 I was able to get Vite to successfully build the frontend app (after much debugging and resolving errors). But, getting the app to run successfully either from the build, or by development was unsuccessful because of a multitude of dependency complications. Things like package versions and using nodejs code that was incompatible with the browser/client. History \u00b6 Lot's of issues with dependencies that had to be resolved just to run the application. Some of them were able to be resolved by using the resolve.alias setting within the Vite config. There was one specific problem I had with a @material-ui/pickers dependency that I had to manually alter within the yarn.lock file because it's a dependency of a dependency that specifies an incompatible version. The issue was posted on Vite here about 11 days ago. The primary issue seems to be that some dependencies really on Nodejs modules that are not supported by the browser and cannot be run on client code. I encountered this problem whenever we had an import from anything related to @backstage . This is where debugging the issues became increasingly more difficult as I could only view the symptoms that caused the error, and not the exact cause. Most of the error I would search for were unanswered, or had the response of \"use something else\". I tried building my own vite app and slowly moving dependencies over, but quickly ran into problems when I started adding @backstage packages. The weirdest issue I came across was when I tried importing the @backstage/integration-react package. I couldn't run the app locally, but I was able to serve if after building the app. Final Vite Config \u00b6 I was able to use the following vite.config.js to successfully run the build/serve/run commands without any errors. It just would come across issues when you tried to load the page. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 import { defineConfig } from 'vite' ; import react from '@vitejs/plugin-react' ; import path from 'path' ; import builtins from 'rollup-plugin-node-builtins' ; import globals from 'rollup-plugin-node-globals' ; import resolve from '@rollup/plugin-node-resolve' ; import json from '@rollup/plugin-json' ; import { injectHtml } from 'vite-plugin-html' ; // see https://vitejs.dev/config/ export default defineConfig (() : any => ({ // avoid clearing the bash' output clearScreen : false , plugins : [ react (), injectHtml ({ data : { title : 'vite-plugin-html-example' , }, }), ], esbuild : { jsxFactory : 'h' , jsxFragment : 'Fragment' , }, resolve : { alias : [ { find : /^es5-ext\\/(.*)?\\/#\\/(.*)$/ , replacement : path . resolve ( require . resolve ( 'es5-ext' ), '../$1/../#/$2' ), }, { find : /^.\\/#$/ , replacement : '../#/index.js' , }, { find : /^.react-virtualized/ , }, ], }, build : { minify : 'esbuild' , sourceMap : true , }, rollupInputOptions : { preserveEntrySignatures : 'strict' , plugins : [ globals , builtins , resolve ({ browser : true , preferBuiltins : false }), json , ], pluginsOptimizer : [ globals (), builtins ({ crypto : true })], }, })); Pros \u00b6 Build times can be very fast and live reloading helps to speed up development time. Supports TypeScript and React out of the Box. Uses Rollup and esbuild for speed. Easy to move back to webpack if we decide to move. Cons \u00b6 Lot's of difficulty working with packages that use Nodejs code or other incompatible dependencies. Still pretty new. Most of the issues I came across or problems I searched for were asked about a month ago. Difficult to debug. I had a lot of problems trying to understand errors within the browser console whereas there would be no problems listed in the server. Decision \u00b6 Unfortunately it looks like we'll have to stick with Webpack for now. Vite is a great build tool for Frontend applications, however I think that it would be best used when creating a brand new app by using their build command, yarn create vite app-name --template react-ts , and building off of that. I did try to create a base Vite app and slowly apply some of the code from lighthouse over in hopes of better debugging, but as soon as I applied @backstage/core I had a large list of items that needed to be resolved. I was able to find some answers on the Vite GH Repo + Stackoverflow, it looks like others are having the same problems that I am. A lot of the problems I had were unsolved as they were only asked about 2 months ago. The biggest issue I'm noticing is that Vite isn't as widely adopted yet, but it's getting there. Most of the issues I came across were still fairly recent. So potentially we could make the move to Vite or another similar build tool in the future when more apps/packages like @backstage build support for them.","title":"Vite"},{"location":"ADR/vite/#vite","text":"Decision Made: yes Decision Date: 10/21 Revisit Decision: yes Date 01/22 Revisit criteria: Decision Made: Yes, but open to revisiting Decision Date: 01/2022 Revisit Decision: Yes Revisit Date: 01/2022 Revisit Criteria: If Vite updates for better coverage or Backstage allows for better Browser/Client support. Decision Makers: @KaemonIsland","title":"Vite"},{"location":"ADR/vite/#tldr","text":"I was able to get Vite to successfully build the frontend app (after much debugging and resolving errors). But, getting the app to run successfully either from the build, or by development was unsuccessful because of a multitude of dependency complications. Things like package versions and using nodejs code that was incompatible with the browser/client.","title":"tl;dr"},{"location":"ADR/vite/#history","text":"Lot's of issues with dependencies that had to be resolved just to run the application. Some of them were able to be resolved by using the resolve.alias setting within the Vite config. There was one specific problem I had with a @material-ui/pickers dependency that I had to manually alter within the yarn.lock file because it's a dependency of a dependency that specifies an incompatible version. The issue was posted on Vite here about 11 days ago. The primary issue seems to be that some dependencies really on Nodejs modules that are not supported by the browser and cannot be run on client code. I encountered this problem whenever we had an import from anything related to @backstage . This is where debugging the issues became increasingly more difficult as I could only view the symptoms that caused the error, and not the exact cause. Most of the error I would search for were unanswered, or had the response of \"use something else\". I tried building my own vite app and slowly moving dependencies over, but quickly ran into problems when I started adding @backstage packages. The weirdest issue I came across was when I tried importing the @backstage/integration-react package. I couldn't run the app locally, but I was able to serve if after building the app.","title":"History"},{"location":"ADR/vite/#final-vite-config","text":"I was able to use the following vite.config.js to successfully run the build/serve/run commands without any errors. It just would come across issues when you tried to load the page. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 import { defineConfig } from 'vite' ; import react from '@vitejs/plugin-react' ; import path from 'path' ; import builtins from 'rollup-plugin-node-builtins' ; import globals from 'rollup-plugin-node-globals' ; import resolve from '@rollup/plugin-node-resolve' ; import json from '@rollup/plugin-json' ; import { injectHtml } from 'vite-plugin-html' ; // see https://vitejs.dev/config/ export default defineConfig (() : any => ({ // avoid clearing the bash' output clearScreen : false , plugins : [ react (), injectHtml ({ data : { title : 'vite-plugin-html-example' , }, }), ], esbuild : { jsxFactory : 'h' , jsxFragment : 'Fragment' , }, resolve : { alias : [ { find : /^es5-ext\\/(.*)?\\/#\\/(.*)$/ , replacement : path . resolve ( require . resolve ( 'es5-ext' ), '../$1/../#/$2' ), }, { find : /^.\\/#$/ , replacement : '../#/index.js' , }, { find : /^.react-virtualized/ , }, ], }, build : { minify : 'esbuild' , sourceMap : true , }, rollupInputOptions : { preserveEntrySignatures : 'strict' , plugins : [ globals , builtins , resolve ({ browser : true , preferBuiltins : false }), json , ], pluginsOptimizer : [ globals (), builtins ({ crypto : true })], }, }));","title":"Final Vite Config"},{"location":"ADR/vite/#pros","text":"Build times can be very fast and live reloading helps to speed up development time. Supports TypeScript and React out of the Box. Uses Rollup and esbuild for speed. Easy to move back to webpack if we decide to move.","title":"Pros"},{"location":"ADR/vite/#cons","text":"Lot's of difficulty working with packages that use Nodejs code or other incompatible dependencies. Still pretty new. Most of the issues I came across or problems I searched for were asked about a month ago. Difficult to debug. I had a lot of problems trying to understand errors within the browser console whereas there would be no problems listed in the server.","title":"Cons"},{"location":"ADR/vite/#decision","text":"Unfortunately it looks like we'll have to stick with Webpack for now. Vite is a great build tool for Frontend applications, however I think that it would be best used when creating a brand new app by using their build command, yarn create vite app-name --template react-ts , and building off of that. I did try to create a base Vite app and slowly apply some of the code from lighthouse over in hopes of better debugging, but as soon as I applied @backstage/core I had a large list of items that needed to be resolved. I was able to find some answers on the Vite GH Repo + Stackoverflow, it looks like others are having the same problems that I am. A lot of the problems I had were unsolved as they were only asked about 2 months ago. The biggest issue I'm noticing is that Vite isn't as widely adopted yet, but it's getting there. Most of the issues I came across were still fairly recent. So potentially we could make the move to Vite or another similar build tool in the future when more apps/packages like @backstage build support for them.","title":"Decision"},{"location":"ADR/zero-install/","text":"Yarn Zero Install ADR \u00b6 Decision Made: no Decision Date: 00/0000 Revisit Decision: yes Date 09/2021 Revisit criteria: Decision Made: No, but open to revisiting Decision Date: 09/2021 Revisit Decision: Yes, Revisit Date: September 2021 Revisit Criteria: When @backstage/cli replaces their @yarnpkg/lockfile parser usage with an updated package parser, @yarnpkg/parsers . Specifically, this file. Decision Makers: @kaemonisland, @rianfowler tl;dr \u00b6 CI/CD can take up to 10+ minutes to complete, specifically the build-containers action. Being able to use Yarn Zero-Install would allow us to cache dependencies thus lowering the run-time of CI/CD by a significant amount. History \u00b6 Yarn recently released a feature called Zero Install which allows packages to be cached. Yarn states the problem pretty clearly... 1 While Yarn does its best to guarantee that what works now will keep working, there's always the off chance that a future Yarn release will introduce a bug that will prevent you from installing your project. Or maybe your production environments will change and yarn install won't be able to write in the temporary directories anymore. Or maybe the network will fail and your packages won't be available anymore. Or maybe your credentials will rotate and you will start getting authentication issues. Or ... so many things can go wrong, and not all of them are things we can control. Pros \u00b6 Fast Caching packages will allow packages to be installed much faster than the current yarn install . ALso, when updating packages, Yarn 2 will change exactly one file for each updated package. Plug N Play Removes the need for node modules completely. If we were able to get Zero Install + Plug n Play our install time would be almost instantaneous. https://yarnpkg.com/features/pnp#fixing-node_modules Small File Size To give you an idea, a node_modules folder of 135k uncompressed files (for a total of 1.2GB) gives a Yarn cache of 2k binary archives (for a total of 139MB). Git simply cannot support the former, while the latter is perfectly fine. Secure Projects accepting PRs from external users will have to be careful that the PRs affecting the package archives are legit (since it would otherwise be possible to a malicious user to send a PR for a new dependency after having altered its archive content). yarn install --check-cache This way Yarn will re-download the package files from whatever their remote location would be and will report any mismatching checksum. Cons \u00b6 Configuration Zero install can get confusing when configuring zero install for Lerna + TypeScript. Additional configuration settings will need to be applied in order for the linter to recognize the package locations. Plugins Decision \u00b6 Adding Yarn Zero Installs will allow CI/CD and local development to get running faster. Currently, it takes about 2+ minutes to install all the necessary packages by running yarn install . Zero Install should allow us to cut that time down to less than one minute by caching packages. If we decide to also setup Plug N Play, we could cut that time down to almost instantaneous speeds. The only thing that is holding us back is @backstage/cli which uses an incompatible package, @yarnpkg/lockfile , to parse the yarn.lock file.","title":"Yarn Zero Install"},{"location":"ADR/zero-install/#yarn-zero-install-adr","text":"Decision Made: no Decision Date: 00/0000 Revisit Decision: yes Date 09/2021 Revisit criteria: Decision Made: No, but open to revisiting Decision Date: 09/2021 Revisit Decision: Yes, Revisit Date: September 2021 Revisit Criteria: When @backstage/cli replaces their @yarnpkg/lockfile parser usage with an updated package parser, @yarnpkg/parsers . Specifically, this file. Decision Makers: @kaemonisland, @rianfowler","title":"Yarn Zero Install ADR"},{"location":"ADR/zero-install/#tldr","text":"CI/CD can take up to 10+ minutes to complete, specifically the build-containers action. Being able to use Yarn Zero-Install would allow us to cache dependencies thus lowering the run-time of CI/CD by a significant amount.","title":"tl;dr"},{"location":"ADR/zero-install/#history","text":"Yarn recently released a feature called Zero Install which allows packages to be cached. Yarn states the problem pretty clearly... 1 While Yarn does its best to guarantee that what works now will keep working, there's always the off chance that a future Yarn release will introduce a bug that will prevent you from installing your project. Or maybe your production environments will change and yarn install won't be able to write in the temporary directories anymore. Or maybe the network will fail and your packages won't be available anymore. Or maybe your credentials will rotate and you will start getting authentication issues. Or ... so many things can go wrong, and not all of them are things we can control.","title":"History"},{"location":"ADR/zero-install/#pros","text":"Fast Caching packages will allow packages to be installed much faster than the current yarn install . ALso, when updating packages, Yarn 2 will change exactly one file for each updated package. Plug N Play Removes the need for node modules completely. If we were able to get Zero Install + Plug n Play our install time would be almost instantaneous. https://yarnpkg.com/features/pnp#fixing-node_modules Small File Size To give you an idea, a node_modules folder of 135k uncompressed files (for a total of 1.2GB) gives a Yarn cache of 2k binary archives (for a total of 139MB). Git simply cannot support the former, while the latter is perfectly fine. Secure Projects accepting PRs from external users will have to be careful that the PRs affecting the package archives are legit (since it would otherwise be possible to a malicious user to send a PR for a new dependency after having altered its archive content). yarn install --check-cache This way Yarn will re-download the package files from whatever their remote location would be and will report any mismatching checksum.","title":"Pros"},{"location":"ADR/zero-install/#cons","text":"Configuration Zero install can get confusing when configuring zero install for Lerna + TypeScript. Additional configuration settings will need to be applied in order for the linter to recognize the package locations. Plugins","title":"Cons"},{"location":"ADR/zero-install/#decision","text":"Adding Yarn Zero Installs will allow CI/CD and local development to get running faster. Currently, it takes about 2+ minutes to install all the necessary packages by running yarn install . Zero Install should allow us to cut that time down to less than one minute by caching packages. If we decide to also setup Plug N Play, we could cut that time down to almost instantaneous speeds. The only thing that is holding us back is @backstage/cli which uses an incompatible package, @yarnpkg/lockfile , to parse the yarn.lock file.","title":"Decision"},{"location":"RFC/browser-logs/","text":"[RFC] Browser Logs \u00b6 Summary \u00b6 We want to be able to record and view browser logging events so that we can gain real-time insight into how our application is operating from the end user's perspective. Background \u00b6 Typically when the front end of a web based application has some kind of event to log, it is displayed in the browser's console. Having access to the console logs is helpful for the purposes of identifying and debugging errors or other problems with the application during run time. Since console logs are reported in the browser, developers do not have access to or any way of knowing if users are experiencing client errors while using an application. Datadog Browser Logs provide a way where we as developers can have access to console logs. Any kind of info, warning, or error messages that would be displayed in the browser's console can be sent to Datadog and recorded as log events. Goal \u00b6 Use Datadog Browser Logs to implement real-time client side error reporting. Findings \u00b6 Initializing Datadog Logs \u00b6 Datadog Logs are initialized using the following: 1 2 3 4 5 6 7 8 import { datadogLogs } from '@datadog/browser-logs' datadogLogs.init({ clientToken: '<DATADOG_CLIENT_TOKEN>', site: '<DATADOG_SITE>', forwardErrorsToLogs: true, sampleRate: 100, }) For future reference, the following parameters are available to configure the Datadog browser logs SDK to send logs to Datadog: PARAMETER TYPE REQUIRED DEFAULT DESCRIPTION clientToken String Yes A Datadog client token. site String Yes datadoghq.com The Datadog site of your organization. US: datadoghq.com, EU: datadoghq.eu service String No The service name for your application. env String No The application\u2019s environment, for example: prod, pre-prod, staging, etc. version String No The application\u2019s version, for example: 1.2.3, 6c44da20, 2020.02.13, etc. forwardErrorsToLogs Boolean No true Set to false to stop forwarding console.error logs, uncaught exceptions and network errors to Datadog. sampleRate Number No 100 The percentage of sessions to track: 100 for all, 0 for none. Only tracked sessions send logs. silentMultipleInit Boolean No Prevent logging errors while having multiple init. proxyUrl Boolean No Optional proxy URL (ex: https://www.proxy.com/path ), see the full proxy setup guide for more information. Options that must have a matching configuration when using the RUM SDK: | PARAMETER | TYPE | REQUIRED | DEFAULT | DESCRIPTION | | --- | --- | --- | --- | ---| | trackSessionAcrossSubdomains | Boolean | No | false | Preserve the session across subdomains for the same site. | | useSecureSessionCookie | Boolean | No | false | Use a secure session cookie. This disables logs sent on insecure (non-HTTPS) connections. | | useCrossSiteSessionCookie | Boolean | No | false | Use a secure cross-site session cookie. This allows the logs SDK to run when the site is loaded from another one (iframe). Implies useSecureSessionCookie.| Note: The Datadog Client Token is exposed in the client by design. While it is possible for someone to use the token to send false information, the risk is considered limited . Custom Logs \u00b6 After the datadogLogs is initialized a default logger is created and will automatically send all console logs to Datadog. You can create custom logs using the following API: 1 logger.debug | info | warn | error (message: string, messageContext = Context) Example custom log: 1 2 3 import { datadogLogs } from '@datadog/browser-logs' datadogLogs.logger.info('Button clicked', { name: 'buttonName', id: 123 }) Multiple Loggers \u00b6 Datadog Logs automatically creates a default logger, but additional loggers can be created and defined by their level , handler , and context . 1 2 3 4 5 createLogger (name: string, conf?: { level?: 'debug' | 'info' | 'warn' | 'error', handler?: 'http' | 'console' | 'silent', context?: Context }) Once a Logger is created other components can use the custom loggers by importing datadogLogs and using the following API: 1 getLogger(name: string) Example: Create Logger called signupLogger 1 2 3 import { datadogLogs } from '@datadog/browser-logs' datadogLogs.createLogger('signupLogger', 'info', 'http', { env: 'staging' }) - Get signupLogger by using the API: 1 2 3 4 import { datadogLogs } from '@datadog/browser-logs' const signupLogger = datadogLogs.getLogger('signupLogger') signupLogger.info('Test sign up completed') Recommendation \u00b6 For the moment, I implemented the default logger with a very broad scope. The current configuration reports all client side log levels - debug, info, warn, and errors - and sends the reports to both the console and Datadog. I think we should create several loggers and use the context attribute to narrow the scope of the reporting based on the environment. Logger Minimum Log Level Handler(s) Context Default Debug console, http Staging Debug console, http Production Warn http Grouping client side errors by environment would give us both the ability to interrogate client side reports at any level in the development process, and the ability to maintain a separate collection of real-time feedback derived from production that would be restricted to a more elevated threshold. References \u00b6 Datadog Logs Exposed Client Token","title":"Browser Logs"},{"location":"RFC/browser-logs/#rfc-browser-logs","text":"","title":"[RFC] Browser Logs"},{"location":"RFC/browser-logs/#summary","text":"We want to be able to record and view browser logging events so that we can gain real-time insight into how our application is operating from the end user's perspective.","title":"Summary"},{"location":"RFC/browser-logs/#background","text":"Typically when the front end of a web based application has some kind of event to log, it is displayed in the browser's console. Having access to the console logs is helpful for the purposes of identifying and debugging errors or other problems with the application during run time. Since console logs are reported in the browser, developers do not have access to or any way of knowing if users are experiencing client errors while using an application. Datadog Browser Logs provide a way where we as developers can have access to console logs. Any kind of info, warning, or error messages that would be displayed in the browser's console can be sent to Datadog and recorded as log events.","title":"Background"},{"location":"RFC/browser-logs/#goal","text":"Use Datadog Browser Logs to implement real-time client side error reporting.","title":"Goal"},{"location":"RFC/browser-logs/#findings","text":"","title":"Findings"},{"location":"RFC/browser-logs/#initializing-datadog-logs","text":"Datadog Logs are initialized using the following: 1 2 3 4 5 6 7 8 import { datadogLogs } from '@datadog/browser-logs' datadogLogs.init({ clientToken: '<DATADOG_CLIENT_TOKEN>', site: '<DATADOG_SITE>', forwardErrorsToLogs: true, sampleRate: 100, }) For future reference, the following parameters are available to configure the Datadog browser logs SDK to send logs to Datadog: PARAMETER TYPE REQUIRED DEFAULT DESCRIPTION clientToken String Yes A Datadog client token. site String Yes datadoghq.com The Datadog site of your organization. US: datadoghq.com, EU: datadoghq.eu service String No The service name for your application. env String No The application\u2019s environment, for example: prod, pre-prod, staging, etc. version String No The application\u2019s version, for example: 1.2.3, 6c44da20, 2020.02.13, etc. forwardErrorsToLogs Boolean No true Set to false to stop forwarding console.error logs, uncaught exceptions and network errors to Datadog. sampleRate Number No 100 The percentage of sessions to track: 100 for all, 0 for none. Only tracked sessions send logs. silentMultipleInit Boolean No Prevent logging errors while having multiple init. proxyUrl Boolean No Optional proxy URL (ex: https://www.proxy.com/path ), see the full proxy setup guide for more information. Options that must have a matching configuration when using the RUM SDK: | PARAMETER | TYPE | REQUIRED | DEFAULT | DESCRIPTION | | --- | --- | --- | --- | ---| | trackSessionAcrossSubdomains | Boolean | No | false | Preserve the session across subdomains for the same site. | | useSecureSessionCookie | Boolean | No | false | Use a secure session cookie. This disables logs sent on insecure (non-HTTPS) connections. | | useCrossSiteSessionCookie | Boolean | No | false | Use a secure cross-site session cookie. This allows the logs SDK to run when the site is loaded from another one (iframe). Implies useSecureSessionCookie.| Note: The Datadog Client Token is exposed in the client by design. While it is possible for someone to use the token to send false information, the risk is considered limited .","title":"Initializing Datadog Logs"},{"location":"RFC/browser-logs/#custom-logs","text":"After the datadogLogs is initialized a default logger is created and will automatically send all console logs to Datadog. You can create custom logs using the following API: 1 logger.debug | info | warn | error (message: string, messageContext = Context) Example custom log: 1 2 3 import { datadogLogs } from '@datadog/browser-logs' datadogLogs.logger.info('Button clicked', { name: 'buttonName', id: 123 })","title":"Custom Logs"},{"location":"RFC/browser-logs/#multiple-loggers","text":"Datadog Logs automatically creates a default logger, but additional loggers can be created and defined by their level , handler , and context . 1 2 3 4 5 createLogger (name: string, conf?: { level?: 'debug' | 'info' | 'warn' | 'error', handler?: 'http' | 'console' | 'silent', context?: Context }) Once a Logger is created other components can use the custom loggers by importing datadogLogs and using the following API: 1 getLogger(name: string) Example: Create Logger called signupLogger 1 2 3 import { datadogLogs } from '@datadog/browser-logs' datadogLogs.createLogger('signupLogger', 'info', 'http', { env: 'staging' }) - Get signupLogger by using the API: 1 2 3 4 import { datadogLogs } from '@datadog/browser-logs' const signupLogger = datadogLogs.getLogger('signupLogger') signupLogger.info('Test sign up completed')","title":"Multiple Loggers"},{"location":"RFC/browser-logs/#recommendation","text":"For the moment, I implemented the default logger with a very broad scope. The current configuration reports all client side log levels - debug, info, warn, and errors - and sends the reports to both the console and Datadog. I think we should create several loggers and use the context attribute to narrow the scope of the reporting based on the environment. Logger Minimum Log Level Handler(s) Context Default Debug console, http Staging Debug console, http Production Warn http Grouping client side errors by environment would give us both the ability to interrogate client side reports at any level in the development process, and the ability to maintain a separate collection of real-time feedback derived from production that would be restricted to a more elevated threshold.","title":"Recommendation"},{"location":"RFC/browser-logs/#references","text":"Datadog Logs Exposed Client Token","title":"References"},{"location":"RFC/catalog-import-research/","text":"[RFC] Catalog Import Research \u00b6 Summary : Users can add GitHub and GitHub Enterprise repositories to the backstage catalog. We need to also check if it's possible to add repositories from other sources like GitLab. Background \u00b6 The GitHub integration supports loading catalog entities from github.com or GitHub Enterprise. Entities can be added to static catalog configuration , registered with the catalog-import plugin, or discovered from a GitHub organization. Users and Groups can also be loaded from an organization . Goal \u00b6 Our goal is to allow multiple repository hosting services, like GitHub/GitLab, to import repositories using the catalog-plugin. Information \u00b6 Integrations \u00b6 Backstage supports integrations from multiple repository hosting websites. GitHub is supported by default. Integrations are required in order to understand how to retrieve a given URL. Adding an integration can be done by editing the app-config.yml file manually, or adding via the backstage API. For Example if we had this location within our app-config.yml file: 1 2 3 4 catalog: locations: - type: url target: https://github.com/backstage/backstage/blob/master/packages/catalog-model/examples/artist-lookup-component.yaml We would also need to list GitHub under integrations. 1 2 3 4 integrations: github: - host: github.com token: ${GITHUB_TOKEN} Example adding a GitLab integration : 1 2 3 4 integrations: gitlab: - host: gitlab.com token: ${GITLAB_TOKEN} Each integration has options configurations. host - The host fo the instance, e.g. github.com token (optional) - An authentication token as expected by the host. IF this is no supplied, anonymous access will be used apiBaseUrl (optional) - The URL fo the GitHub/GitLab API. For self-hosted installations, it is commonly at https://<host>/api/ . For gitlab.com, this configuration is not needed as it can be inferred. baseUrl (optional) - The base URL for this provider, e.g. https://gitlab.com . If this is not provided, it is assumed to be https://<host> Note: You need to supply either apiBaseUrl or rawBaseUrl or both (except for public GitHub, for which we can infer them). The apiBaseUrl will always be preferred over the other if a token is given, otherwise rawBaseUrl will be preferred. Catalog-Plugin \u00b6 catalog-plugin The Catalog Import Plugin provides a wizard to onboard projects with existing catalog-info.yaml files. It also assists by creating pull requests in repositories where no catalog-info.yaml exists. The plugin can import any catalog-info.yml file that is listed within one of our integrations. GitHub Only: The plugin can also search for all catalog-info.yml files along with analyse, generate a Component Entity, and create pull requests. Recommendation \u00b6 It seems like the best idea would be to add GitHub Enterprise and GitLab to the integrations section of app-config.yml . Then, we could provide a way for users to request additional integrations. Otherwise it doesn't look like we will need to make any updates to the catalog-plugin, or to even create our own. The only reason I could think to add our own is if we want special behavior. Other things like titles, descriptions, and other wording can be changed manually. Questions \u00b6 Q: Do we need to make a catalog import plugin to support multiple instances of GitHub? A: No, a catalog-import plugin is already included within backstage. This plugin provides a wizard to onboard projects with existing catalog-info.yml files, or creates a pull request with an example catalog-info.yml if none are found. Integrations from other repositories can be included within the app-config.yml config. Q: How might we support importing catalog entries that aren't in GitHub? A: Other repository hosting services can be added by adding them to integrations within app-config.yml . For example to add support for GitLab we would add: 1 2 3 4 integrations: gitlab: - host: gitlab.com token: ${GITLAB_TOKEN} Q: How does it determine what credentials to use? A: Credentials are based off of the configuration set in app-config.yml or added using the catalog-import plugin. A token can be applied for authentication. Anonymous access will be used if none is applied. Q: Do we need to modify the starter kit UI? A: I don't believe so. Per, Backstage Integrations Integrations are configured at the root level of app-config.yaml since integrations are used by many Backstage core features and other plugins. Each key under integrations is a separate configuration for a single external provider. Providers each have different configuration; here's an example of configuration to use both GitHub and Bitbucket: 1 2 3 4 5 6 7 8 integrations: github: - host: github.com token: ${GITHUB_TOKEN} bitbucket: - host: bitbucket.org username: ${BITBUCKET_USERNAME} appPassword: ${BITBUCKET_APP_PASSWORD} Q: Would we have naming conflicts? A: Yes, but only for specific situations. Per the Backstage catalog configuration : When multiple catalog-info.yaml files with the same metadata.name property are discovered, one will be processed and all others will be skipped. This action is logged for further investigation.","title":"Catalog Import Research"},{"location":"RFC/catalog-import-research/#rfc-catalog-import-research","text":"Summary : Users can add GitHub and GitHub Enterprise repositories to the backstage catalog. We need to also check if it's possible to add repositories from other sources like GitLab.","title":"[RFC] Catalog Import Research"},{"location":"RFC/catalog-import-research/#background","text":"The GitHub integration supports loading catalog entities from github.com or GitHub Enterprise. Entities can be added to static catalog configuration , registered with the catalog-import plugin, or discovered from a GitHub organization. Users and Groups can also be loaded from an organization .","title":"Background"},{"location":"RFC/catalog-import-research/#goal","text":"Our goal is to allow multiple repository hosting services, like GitHub/GitLab, to import repositories using the catalog-plugin.","title":"Goal"},{"location":"RFC/catalog-import-research/#information","text":"","title":"Information"},{"location":"RFC/catalog-import-research/#integrations","text":"Backstage supports integrations from multiple repository hosting websites. GitHub is supported by default. Integrations are required in order to understand how to retrieve a given URL. Adding an integration can be done by editing the app-config.yml file manually, or adding via the backstage API. For Example if we had this location within our app-config.yml file: 1 2 3 4 catalog: locations: - type: url target: https://github.com/backstage/backstage/blob/master/packages/catalog-model/examples/artist-lookup-component.yaml We would also need to list GitHub under integrations. 1 2 3 4 integrations: github: - host: github.com token: ${GITHUB_TOKEN} Example adding a GitLab integration : 1 2 3 4 integrations: gitlab: - host: gitlab.com token: ${GITLAB_TOKEN} Each integration has options configurations. host - The host fo the instance, e.g. github.com token (optional) - An authentication token as expected by the host. IF this is no supplied, anonymous access will be used apiBaseUrl (optional) - The URL fo the GitHub/GitLab API. For self-hosted installations, it is commonly at https://<host>/api/ . For gitlab.com, this configuration is not needed as it can be inferred. baseUrl (optional) - The base URL for this provider, e.g. https://gitlab.com . If this is not provided, it is assumed to be https://<host> Note: You need to supply either apiBaseUrl or rawBaseUrl or both (except for public GitHub, for which we can infer them). The apiBaseUrl will always be preferred over the other if a token is given, otherwise rawBaseUrl will be preferred.","title":"Integrations"},{"location":"RFC/catalog-import-research/#catalog-plugin","text":"catalog-plugin The Catalog Import Plugin provides a wizard to onboard projects with existing catalog-info.yaml files. It also assists by creating pull requests in repositories where no catalog-info.yaml exists. The plugin can import any catalog-info.yml file that is listed within one of our integrations. GitHub Only: The plugin can also search for all catalog-info.yml files along with analyse, generate a Component Entity, and create pull requests.","title":"Catalog-Plugin"},{"location":"RFC/catalog-import-research/#recommendation","text":"It seems like the best idea would be to add GitHub Enterprise and GitLab to the integrations section of app-config.yml . Then, we could provide a way for users to request additional integrations. Otherwise it doesn't look like we will need to make any updates to the catalog-plugin, or to even create our own. The only reason I could think to add our own is if we want special behavior. Other things like titles, descriptions, and other wording can be changed manually.","title":"Recommendation"},{"location":"RFC/catalog-import-research/#questions","text":"Q: Do we need to make a catalog import plugin to support multiple instances of GitHub? A: No, a catalog-import plugin is already included within backstage. This plugin provides a wizard to onboard projects with existing catalog-info.yml files, or creates a pull request with an example catalog-info.yml if none are found. Integrations from other repositories can be included within the app-config.yml config. Q: How might we support importing catalog entries that aren't in GitHub? A: Other repository hosting services can be added by adding them to integrations within app-config.yml . For example to add support for GitLab we would add: 1 2 3 4 integrations: gitlab: - host: gitlab.com token: ${GITLAB_TOKEN} Q: How does it determine what credentials to use? A: Credentials are based off of the configuration set in app-config.yml or added using the catalog-import plugin. A token can be applied for authentication. Anonymous access will be used if none is applied. Q: Do we need to modify the starter kit UI? A: I don't believe so. Per, Backstage Integrations Integrations are configured at the root level of app-config.yaml since integrations are used by many Backstage core features and other plugins. Each key under integrations is a separate configuration for a single external provider. Providers each have different configuration; here's an example of configuration to use both GitHub and Bitbucket: 1 2 3 4 5 6 7 8 integrations: github: - host: github.com token: ${GITHUB_TOKEN} bitbucket: - host: bitbucket.org username: ${BITBUCKET_USERNAME} appPassword: ${BITBUCKET_APP_PASSWORD} Q: Would we have naming conflicts? A: Yes, but only for specific situations. Per the Backstage catalog configuration : When multiple catalog-info.yaml files with the same metadata.name property are discovered, one will be processed and all others will be skipped. This action is logged for further investigation.","title":"Questions"},{"location":"RFC/ci-cd/","text":"[RFC] Modifications to existing CI/CD process \u00b6 Summary : We are currently in the process of building our CI/CD pipeline. We know we need to make modifications to our existing CI/CD process to incorporate changesets for releases and also to use the Lightkeeper CLI for creating Kubernetes clusters. Background \u00b6 The process of integrating and delivering new changes for an application from source code to a production environment requires a multi-staged process to build, test, and deploy each new release. Automating the process of building, testing, and deploying an application is known as a Continuous Integration/Continuous Delivery, or CI/CD, pipeline. Utilizing a CI/CD pipeline is the preferred method for delivering new releases because it accelerates the process while reducing errors and allows the use a variety of tests and tools to generate feedback for the software development lifecycle. There is not a single way to structure a CI/CD Pipeline, but the intent is to use a multistaged process that creates a feedback loop to drive the development cycle forward. Goal \u00b6 Our goal is to build an optimized CI/CD pipeline that produces fast, accurate, reliable, and comprehensive feedback to expedite our development cycle. We plan to achieve this goal by implementing automation tools and workflows that are triggered by merges pushed to the \"main\" branch of our Lighthouse Backstage repository. Note: Is this an accurate description of our goals relating to creating a CI/CD pipeline? Are there other goals I should include? Sequence Diagram of CI/CD Pipeline (WIP) \u00b6 This is a rough sequence diagram of the CI/CD pipeline to illustrate the role of each stage and how it can provide feedback to the developer in the event of an error at some stage in the process. Note: Still a work in progress and needs more steps or more stages to reflect multiple deployment environments Source Code \u00b6 Merging to the \"main\" branch initiates the CI/CD Pipeline so those changes can be deployed to the production environment. The \"Release\" Github action will automatically generate changesets and create a new release. We can extend this workflow to include different types of releases like prereleases and to store images for the frontend and backend containers. Build Stage \u00b6 This stage is responsible for building the images for our application and storing these images in a container registry. It is important that we build only once during the CI/CD process to reduce time/resources because the build process is lengthy. Once an image is built, subsequent steps of the CI/CD process that require an image can utilize the already built artifact instead of spending time/resources on rebuilding the image. Testing Stage \u00b6 This stage is responsible for running a variety of automated tests and checks to catch potential errors early and provide feedback to the developer. Deployment Stage \u00b6 This stage is responsible for deploying our application as a Kubernetes cluster to a cloud server. We plan to use Lightkeeper CLI to generate the assets for our Kubernetes cluster. Right now the Lightkeeper CLI requires a login to use that must be done manually through the browser.","title":"CI/CD Modifications"},{"location":"RFC/ci-cd/#rfc-modifications-to-existing-cicd-process","text":"Summary : We are currently in the process of building our CI/CD pipeline. We know we need to make modifications to our existing CI/CD process to incorporate changesets for releases and also to use the Lightkeeper CLI for creating Kubernetes clusters.","title":"[RFC] Modifications to existing CI/CD process"},{"location":"RFC/ci-cd/#background","text":"The process of integrating and delivering new changes for an application from source code to a production environment requires a multi-staged process to build, test, and deploy each new release. Automating the process of building, testing, and deploying an application is known as a Continuous Integration/Continuous Delivery, or CI/CD, pipeline. Utilizing a CI/CD pipeline is the preferred method for delivering new releases because it accelerates the process while reducing errors and allows the use a variety of tests and tools to generate feedback for the software development lifecycle. There is not a single way to structure a CI/CD Pipeline, but the intent is to use a multistaged process that creates a feedback loop to drive the development cycle forward.","title":"Background"},{"location":"RFC/ci-cd/#goal","text":"Our goal is to build an optimized CI/CD pipeline that produces fast, accurate, reliable, and comprehensive feedback to expedite our development cycle. We plan to achieve this goal by implementing automation tools and workflows that are triggered by merges pushed to the \"main\" branch of our Lighthouse Backstage repository. Note: Is this an accurate description of our goals relating to creating a CI/CD pipeline? Are there other goals I should include?","title":"Goal"},{"location":"RFC/ci-cd/#sequence-diagram-of-cicd-pipeline-wip","text":"This is a rough sequence diagram of the CI/CD pipeline to illustrate the role of each stage and how it can provide feedback to the developer in the event of an error at some stage in the process. Note: Still a work in progress and needs more steps or more stages to reflect multiple deployment environments","title":"Sequence Diagram of CI/CD Pipeline (WIP)"},{"location":"RFC/ci-cd/#source-code","text":"Merging to the \"main\" branch initiates the CI/CD Pipeline so those changes can be deployed to the production environment. The \"Release\" Github action will automatically generate changesets and create a new release. We can extend this workflow to include different types of releases like prereleases and to store images for the frontend and backend containers.","title":"Source Code"},{"location":"RFC/ci-cd/#build-stage","text":"This stage is responsible for building the images for our application and storing these images in a container registry. It is important that we build only once during the CI/CD process to reduce time/resources because the build process is lengthy. Once an image is built, subsequent steps of the CI/CD process that require an image can utilize the already built artifact instead of spending time/resources on rebuilding the image.","title":"Build Stage"},{"location":"RFC/ci-cd/#testing-stage","text":"This stage is responsible for running a variety of automated tests and checks to catch potential errors early and provide feedback to the developer.","title":"Testing Stage"},{"location":"RFC/ci-cd/#deployment-stage","text":"This stage is responsible for deploying our application as a Kubernetes cluster to a cloud server. We plan to use Lightkeeper CLI to generate the assets for our Kubernetes cluster. Right now the Lightkeeper CLI requires a login to use that must be done manually through the browser.","title":"Deployment Stage"},{"location":"RFC/custom-actions/","text":"[RFC] Custom Github Actions \u00b6 Summary : We want to implement custom Github Actions to retain both the simplicity, readability and maintainability of published Github Actions and the ability to create tailored workflows that fit our specific operational needs. Background \u00b6 Github Actions are workflows that allow us to automate complex tasks. Some steps are handled by actions published to the Github Actions Marketplace but other steps require a tailored solution often involving many steps that using scripts. These tailored solutions can quickly become unwieldy and difficult to understand. Moreover, some of these steps are tightly coupled which obfuscates the process of maintaining, modifying, or extending these workflows. We want to add a layer of abstraction by encapsulating these steps into a single custom Github Action. Using custom Github actions we intend to transform these complex, and often tightly coupled, multi-step solutions into reusable atomic actions with simple interfaces. Goal \u00b6 Create custom Github Actions with simple interfaces that we can use in our workflows. Creating Custom Actions \u00b6 To identify an action Github looks for an action manifest which is simply a file called action.yml or action.yaml There are three types of github actions : Docker container JavaScript Composite Actions Docker Container Actions \u00b6 OS: Linux Packages the Github action into a docker container to create a more reliable unit of code Specify aspects of the environment for the action(e.g. version of OS, dependencies, tools, etc.) Additional overhead for retrieving/building the container causing the action to be slower JavaScript Actions \u00b6 OS: Linux, macOS, Windows Runs directly on a runner machine Can separate the action code from the environment used to run the code Executes faster than Docker container action Should be written in pure JavaScript and not rely on other binaries Composite Actions \u00b6 OS: Linux, macOS, Windows Allows you to bundle multiple workflow steps into a single action You can refer to this action using a single step from a workflow to run the bundle of commands Implementation \u00b6 I created an example of a composite action using this repository. This method uses the path to .github/actions/<custom_action_name> to find the custom action's manifest file. Note: There is another method of locating actions by creating a separate repository with tags to reference. I created a repository lighthouse-backstage-actions but it is currently set to internal by default and an organization owner/admin needs to change it to public for us to use it. Inside the .github directory is an actions directory 1 2 3 4 5 6 7 . \u2514\u2500\u2500 .github/ \u251c\u2500\u2500 pull_request_template.md \u251c\u2500\u2500 workflows/ \u2514\u2500\u2500 actions/ \u2514\u2500\u2500 logger/ \u2514\u2500\u2500 action.yml Inside the actions directory is a logger directory containing a single action.yml file The logger directory refers to the name of the action The action.yml is an action manifest file The path to the action.yml will be used later to find the action manifest when a workflow tries to invoke it Contents of the manifest: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # action.yml name: 'Logger' description: 'Logs things' inputs: string-to-log: description: 'string to log' required: true default: 'Enter a phrase to log' # outputs can also be defined here but this action doesn't have any runs: using: \"composite\" steps: - uses: actions/checkout@v2 - uses: actions/github-script@v3 with: script: console.log(\"${{ inputs.string-to-log }}\") Using the action in a workflow .github/workflows/example_custom_action.yml \u00b6 name: Example custom action on: push: branches: [custom-actions] pull_request: branches: [main] workflow_dispatch: jobs: example-custom-action: runs-on: ubuntu-latest continue-on-error: true steps: - uses: actions/checkout@v2 # Using path to same repo works - uses: ./.github/actions/logger with: string-to-log: 'This should log in the console' Result from workflow run : 1 2 3 4 5 6 7 8 Run actions/github-script@v3 with: script: console.log(\"This should log in the console\") github-token: *** debug: false user-agent: actions/github-script result-encoding: json This should log in the console Recommendation \u00b6 I think that we should first audit our existing workflows to determine which workflows, or parts of workflows, we intend to keep. Of these workflows we can then identify steps or groups of steps where we can potentionally implement a custom action. Then we can further consolidate the group of potential custom actions by identifying redundancies. Once we have a refined list of custom actions we can create each action and implement it by replace the corresponding steps in our workflows with our custom actions.","title":"Custom Actions"},{"location":"RFC/custom-actions/#rfc-custom-github-actions","text":"Summary : We want to implement custom Github Actions to retain both the simplicity, readability and maintainability of published Github Actions and the ability to create tailored workflows that fit our specific operational needs.","title":"[RFC] Custom Github Actions"},{"location":"RFC/custom-actions/#background","text":"Github Actions are workflows that allow us to automate complex tasks. Some steps are handled by actions published to the Github Actions Marketplace but other steps require a tailored solution often involving many steps that using scripts. These tailored solutions can quickly become unwieldy and difficult to understand. Moreover, some of these steps are tightly coupled which obfuscates the process of maintaining, modifying, or extending these workflows. We want to add a layer of abstraction by encapsulating these steps into a single custom Github Action. Using custom Github actions we intend to transform these complex, and often tightly coupled, multi-step solutions into reusable atomic actions with simple interfaces.","title":"Background"},{"location":"RFC/custom-actions/#goal","text":"Create custom Github Actions with simple interfaces that we can use in our workflows.","title":"Goal"},{"location":"RFC/custom-actions/#creating-custom-actions","text":"To identify an action Github looks for an action manifest which is simply a file called action.yml or action.yaml There are three types of github actions : Docker container JavaScript Composite Actions","title":"Creating Custom Actions"},{"location":"RFC/custom-actions/#docker-container-actions","text":"OS: Linux Packages the Github action into a docker container to create a more reliable unit of code Specify aspects of the environment for the action(e.g. version of OS, dependencies, tools, etc.) Additional overhead for retrieving/building the container causing the action to be slower","title":"Docker Container Actions"},{"location":"RFC/custom-actions/#javascript-actions","text":"OS: Linux, macOS, Windows Runs directly on a runner machine Can separate the action code from the environment used to run the code Executes faster than Docker container action Should be written in pure JavaScript and not rely on other binaries","title":"JavaScript Actions"},{"location":"RFC/custom-actions/#composite-actions","text":"OS: Linux, macOS, Windows Allows you to bundle multiple workflow steps into a single action You can refer to this action using a single step from a workflow to run the bundle of commands","title":"Composite Actions"},{"location":"RFC/custom-actions/#implementation","text":"I created an example of a composite action using this repository. This method uses the path to .github/actions/<custom_action_name> to find the custom action's manifest file. Note: There is another method of locating actions by creating a separate repository with tags to reference. I created a repository lighthouse-backstage-actions but it is currently set to internal by default and an organization owner/admin needs to change it to public for us to use it. Inside the .github directory is an actions directory 1 2 3 4 5 6 7 . \u2514\u2500\u2500 .github/ \u251c\u2500\u2500 pull_request_template.md \u251c\u2500\u2500 workflows/ \u2514\u2500\u2500 actions/ \u2514\u2500\u2500 logger/ \u2514\u2500\u2500 action.yml Inside the actions directory is a logger directory containing a single action.yml file The logger directory refers to the name of the action The action.yml is an action manifest file The path to the action.yml will be used later to find the action manifest when a workflow tries to invoke it Contents of the manifest: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # action.yml name: 'Logger' description: 'Logs things' inputs: string-to-log: description: 'string to log' required: true default: 'Enter a phrase to log' # outputs can also be defined here but this action doesn't have any runs: using: \"composite\" steps: - uses: actions/checkout@v2 - uses: actions/github-script@v3 with: script: console.log(\"${{ inputs.string-to-log }}\") Using the action in a workflow","title":"Implementation"},{"location":"RFC/custom-actions/#githubworkflowsexample_custom_actionyml","text":"name: Example custom action on: push: branches: [custom-actions] pull_request: branches: [main] workflow_dispatch: jobs: example-custom-action: runs-on: ubuntu-latest continue-on-error: true steps: - uses: actions/checkout@v2 # Using path to same repo works - uses: ./.github/actions/logger with: string-to-log: 'This should log in the console' Result from workflow run : 1 2 3 4 5 6 7 8 Run actions/github-script@v3 with: script: console.log(\"This should log in the console\") github-token: *** debug: false user-agent: actions/github-script result-encoding: json This should log in the console","title":".github/workflows/example_custom_action.yml"},{"location":"RFC/custom-actions/#recommendation","text":"I think that we should first audit our existing workflows to determine which workflows, or parts of workflows, we intend to keep. Of these workflows we can then identify steps or groups of steps where we can potentionally implement a custom action. Then we can further consolidate the group of potential custom actions by identifying redundancies. Once we have a refined list of custom actions we can create each action and implement it by replace the corresponding steps in our workflows with our custom actions.","title":"Recommendation"},{"location":"RFC/doc-generator/","text":"[RFC] Documentation Generator \u00b6 Summary : Allow documentation to be generated from code comments. Most documentation generators have a fairly similar syntax. So the main purpose of this RFC is to help decide between which syntax we would like more. I was able to find two generators that I think we should choose between, JSDoc and TypeDoc. Both of these generators have eslint-plugins that would allow us to require comments, and other nitpicking. They also allow us to generate documentation within GitHub Actions which is great. Like I said, the main difference is how the syntax will work with TypeScript. Note: Please feel free to let me know if there is another doc generator that you think we should consider. Background \u00b6 Why do we need a documentation generator? I think I speak for everyone when I say that documenting everything can become a huge pain and can take up time a lot of our time. A documentation generator can take our commented code and build a good looking HTML website, so long as we make good explanatory comments. Automating documentation is just another thing that can save us a ton of time and allow us to work on more important/fun features. Goal \u00b6 To decide on which documentation generator, JSDoc or TypeDoc, should be used when adding comments to the codebase. Similar to the summary both generators have similar syntax, eslint support, and GitHub Actions. The only real decision to make is which syntax we'd rather use. Generating documentation \u00b6 Both doc generators are able to run with a single CLI command. They can take inline arguments, or use a config file to determine where to look for files, and where it should output the generated documentation. For example if we want to generate docs from src using a specified config and then putting the generated docs into a docs folder. JSDoc can have plugins or other settings added to a jsdoc.json file as well. JSDoc -> /path/to/jsdoc src -r -c /path/to/my/conf.json -d docs Typedoc can use most of the TypeScript compiler options as well as the tsconfig file. TypeDoc -> typedoc --out docs src JSDoc \u00b6 JSDoc GitHub JSDoc + TypeScript Playground JSDoc has a lot of support and is quite popular. Documenting code can be incredibly easy and provide a lot of information about what's going on in a file. It's been a great way to add typing to JavaScript without using TypeScript. Supported Types: @type @param (or @arg or @argument) @returns (or @return) @typedef @callback @template @class (or @constructor) @this @extends (or @augments) @enum @deprecated In JSDoc types are listed within the comment. 1 2 3 4 5 6 7 8 9 10 11 12 13 /** @type {string} */ let s ; /** * @param {string} p1 - A string param. * @param {string=} p2 - An optional param (Closure syntax) * @param {string} [p3] - Another optional param (JSDoc syntax). * @param {string} [p4=\"test\"] - An optional param with a default value * @return {string} This is the result */ function stringsStringStrings ( p1 , p2 , p3 , p4 ) { // TODO } Unfortunately, JSDoc doesn't support TypeScript out of the box. There are some plugins like better-docs that can parse .ts | .tsx files. However, types aren't displayed correctly within the generated documentation. Meaning that we'd have to list out types within the comments and within the code. There is a way around it (I've done it before) but it requires some extra time and monkey-patching to get it to work correctly. TypeDoc \u00b6 TypeDoc GitHub It appears that TypeDoc can do everything that JSDoc can, but with out of the box support for TypeScript files. It also uses the tsconfig file to figure out where it should look for .ts | .tsx files. All comments are parsed as Markdown which adds additional styling for us! TypeDoc doesn't support all the tags that JSDoc does, but that's because it infers more information from the TypeScript code. So you are more than welcome to type regular TypeScript without needing to declare types within comments. 1 2 3 4 /** * @param text Comment for parameter \u00b4text\u00b4. */ function doSomething ( target : any , text : string ) : number ; Supported Tags: @param <param name> @typeParam <param name> or @template <param name> @return(s) @event @hidden and @ignore @internal @category @module @typedef, @callback @public, @protected, and @private TypeDoc can also support .js | .jsx files if the allowJS option is set to true in the tsconfig . Just note that it can't derive the type from the code so they would need to be explicitly stated within the comment. Recommendation \u00b6 Considering everything that we would want for a documentation generator. Linting Syntax Highlighting Doc Generation with CI/CD JavaScript/TypeScript Support Tags I believe that TypeDoc is the best choice considering it supports TypeScript out of the box. The syntax isn't very different from JSDoc, we just omit the types after the @param and @returns because it's written within the code! Techdocs \u00b6 We can specify another documentation folder so that we don't conflict with TechDocs. The only issue will be that GitHub Pages can only have one site per project. I don't think that there is a way for us to merge Techdocs + another site. We could possibly setup an account with Netlify or Heroku to publish the generated documentation. Or open another repository which we push to using GitHub Actions? eslint enforcement \u00b6 In my experience, I'm used to enforcing comments on All functions, just to describe the purpose, params, and return values. It helps devs to quickly understand how to use the function. It also helps to reveal if the function is doing too much. Anything else like Types, Interfaces, Variables have felt pretty self explanatory. When something does become convoluted, or can be confusing I'll usually add a comment explaining it's purpose. Errors Classes (Which should be avoided) Required for all Functions and include: A short description All parameters (If applicable) Returns (If applicable) Anything else is optional for me. We could include warnings for Types and Interfaces?","title":"Doc Generator"},{"location":"RFC/doc-generator/#rfc-documentation-generator","text":"Summary : Allow documentation to be generated from code comments. Most documentation generators have a fairly similar syntax. So the main purpose of this RFC is to help decide between which syntax we would like more. I was able to find two generators that I think we should choose between, JSDoc and TypeDoc. Both of these generators have eslint-plugins that would allow us to require comments, and other nitpicking. They also allow us to generate documentation within GitHub Actions which is great. Like I said, the main difference is how the syntax will work with TypeScript. Note: Please feel free to let me know if there is another doc generator that you think we should consider.","title":"[RFC] Documentation Generator"},{"location":"RFC/doc-generator/#background","text":"Why do we need a documentation generator? I think I speak for everyone when I say that documenting everything can become a huge pain and can take up time a lot of our time. A documentation generator can take our commented code and build a good looking HTML website, so long as we make good explanatory comments. Automating documentation is just another thing that can save us a ton of time and allow us to work on more important/fun features.","title":"Background"},{"location":"RFC/doc-generator/#goal","text":"To decide on which documentation generator, JSDoc or TypeDoc, should be used when adding comments to the codebase. Similar to the summary both generators have similar syntax, eslint support, and GitHub Actions. The only real decision to make is which syntax we'd rather use.","title":"Goal"},{"location":"RFC/doc-generator/#generating-documentation","text":"Both doc generators are able to run with a single CLI command. They can take inline arguments, or use a config file to determine where to look for files, and where it should output the generated documentation. For example if we want to generate docs from src using a specified config and then putting the generated docs into a docs folder. JSDoc can have plugins or other settings added to a jsdoc.json file as well. JSDoc -> /path/to/jsdoc src -r -c /path/to/my/conf.json -d docs Typedoc can use most of the TypeScript compiler options as well as the tsconfig file. TypeDoc -> typedoc --out docs src","title":"Generating documentation"},{"location":"RFC/doc-generator/#jsdoc","text":"JSDoc GitHub JSDoc + TypeScript Playground JSDoc has a lot of support and is quite popular. Documenting code can be incredibly easy and provide a lot of information about what's going on in a file. It's been a great way to add typing to JavaScript without using TypeScript. Supported Types: @type @param (or @arg or @argument) @returns (or @return) @typedef @callback @template @class (or @constructor) @this @extends (or @augments) @enum @deprecated In JSDoc types are listed within the comment. 1 2 3 4 5 6 7 8 9 10 11 12 13 /** @type {string} */ let s ; /** * @param {string} p1 - A string param. * @param {string=} p2 - An optional param (Closure syntax) * @param {string} [p3] - Another optional param (JSDoc syntax). * @param {string} [p4=\"test\"] - An optional param with a default value * @return {string} This is the result */ function stringsStringStrings ( p1 , p2 , p3 , p4 ) { // TODO } Unfortunately, JSDoc doesn't support TypeScript out of the box. There are some plugins like better-docs that can parse .ts | .tsx files. However, types aren't displayed correctly within the generated documentation. Meaning that we'd have to list out types within the comments and within the code. There is a way around it (I've done it before) but it requires some extra time and monkey-patching to get it to work correctly.","title":"JSDoc"},{"location":"RFC/doc-generator/#typedoc","text":"TypeDoc GitHub It appears that TypeDoc can do everything that JSDoc can, but with out of the box support for TypeScript files. It also uses the tsconfig file to figure out where it should look for .ts | .tsx files. All comments are parsed as Markdown which adds additional styling for us! TypeDoc doesn't support all the tags that JSDoc does, but that's because it infers more information from the TypeScript code. So you are more than welcome to type regular TypeScript without needing to declare types within comments. 1 2 3 4 /** * @param text Comment for parameter \u00b4text\u00b4. */ function doSomething ( target : any , text : string ) : number ; Supported Tags: @param <param name> @typeParam <param name> or @template <param name> @return(s) @event @hidden and @ignore @internal @category @module @typedef, @callback @public, @protected, and @private TypeDoc can also support .js | .jsx files if the allowJS option is set to true in the tsconfig . Just note that it can't derive the type from the code so they would need to be explicitly stated within the comment.","title":"TypeDoc"},{"location":"RFC/doc-generator/#recommendation","text":"Considering everything that we would want for a documentation generator. Linting Syntax Highlighting Doc Generation with CI/CD JavaScript/TypeScript Support Tags I believe that TypeDoc is the best choice considering it supports TypeScript out of the box. The syntax isn't very different from JSDoc, we just omit the types after the @param and @returns because it's written within the code!","title":"Recommendation"},{"location":"RFC/doc-generator/#techdocs","text":"We can specify another documentation folder so that we don't conflict with TechDocs. The only issue will be that GitHub Pages can only have one site per project. I don't think that there is a way for us to merge Techdocs + another site. We could possibly setup an account with Netlify or Heroku to publish the generated documentation. Or open another repository which we push to using GitHub Actions?","title":"Techdocs"},{"location":"RFC/doc-generator/#eslint-enforcement","text":"In my experience, I'm used to enforcing comments on All functions, just to describe the purpose, params, and return values. It helps devs to quickly understand how to use the function. It also helps to reveal if the function is doing too much. Anything else like Types, Interfaces, Variables have felt pretty self explanatory. When something does become convoluted, or can be confusing I'll usually add a comment explaining it's purpose. Errors Classes (Which should be avoided) Required for all Functions and include: A short description All parameters (If applicable) Returns (If applicable) Anything else is optional for me. We could include warnings for Types and Interfaces?","title":"eslint enforcement"},{"location":"RFC/docker-scanning/","text":"[RFC] Docker Image Scanning \u00b6 Summary \u00b6 Lighthouse-backstage is a public repo. All deployments are available as public packages. We need to ensure that secrets and private environment variables do not get injected into the docker image. We should be able to fail a build/push script that hardcodes secrets into the image definition. We don't want to commit secrets to GitHub packages. Background \u00b6 Each lighthouse-backstage deployment is available publicly for anyone to access. This means that if a secret were hardcoded within a docker image, everyone could now see it. Vulnerability scanning for Docker local images allows developers and development teams to review the security state of the container images and take actions to fix issues identified during the scan, resulting in more secure deployments. Docker Scan runs on the Snyk engine, providing users with visibility into the security posture of their local Dockerfiles and local images. Goal \u00b6 Scan each build/push for vulnerabilities to prevent a deployment from releasing private information to GitHub Packages. Findings \u00b6 Docker Scan \u00b6 By default, docker allows an image to be scanned ten times within a month before it requires a free account with Snyk . Docker and Snyk have partnered together to bring security natively into the development workflow by providing a simple and streamlined approach for developers to build and deploy secure containers. An image can be scanned using Docker Hub or by using docker scan <image_name> within the CLI. Some important tags can be included to limit issues: Excluding the base image . --exclude-base View the JSON output . --json The scan checks for known vulnerabilities in public GitHub repos, npm packages and Docker images. So it does not scan for leaked credentials/secrets. Credential / Secret scanning \u00b6 git-secrets is an open source repo that is designed to prevent passwords and other sensitive information from being committed to a repo. It scans commits, commit messages, and --no-ff merges to prevent adding secrets into your git repositories. If a commit, commit message, or any commit in a --no-ff merge history matches one of your configured prohibited regular expression patterns, then the commit is rejected. Recommendation \u00b6 Vulnerability Scanning \u00b6 I tried building a fresh frontend image using docker build -t backstage-frontend -f Dockerfile.dockerbuild . and then scanning it by using docker scan backstage-frontend I think using docker scan for our CI/CD pipelines would be an excellent fit for our security needs. The only issue is that we'll need to create a Snyk account for lighthouse-backstage to scan Docker images more than ten times a month. Another thing I found is that scanning the backstage-frontend image returned a large number of issues related to the base image (NGINX). Excluding the base, the image limited the number of issues to just 7, which I think is excellent, and a lot easier to work with. I wasn't able to find a convenient GH Action that can fail a pipeline for specified criteria. So we might have to build a script that can analyze the JSON output of the docker scan. Otherwise, we could follow this guide on how to use Snyk to scan our docker image within CI/CD. I don't think it would be too hard to adapt it to use GitHub Actions. As stated above, Docker Scan / Snyk only checks for KNOWN vulnerabilities reported from other packages. It does not scan for leaked credentials/secrets. Git-Secrets \u00b6 I think that using git-secrets within a pre-commit hook would be the best route to prevent secrets from being committed to the lighthouse-backstage repo. It's pretty easy to hard-code a secret when building a feature or debugging an issue and forget to remove it. References \u00b6 Scan Images Vulnerability Scanning Snyk Snyk CI/CD Scanning git-secrets","title":"Docker Scanning"},{"location":"RFC/docker-scanning/#rfc-docker-image-scanning","text":"","title":"[RFC] Docker Image Scanning"},{"location":"RFC/docker-scanning/#summary","text":"Lighthouse-backstage is a public repo. All deployments are available as public packages. We need to ensure that secrets and private environment variables do not get injected into the docker image. We should be able to fail a build/push script that hardcodes secrets into the image definition. We don't want to commit secrets to GitHub packages.","title":"Summary"},{"location":"RFC/docker-scanning/#background","text":"Each lighthouse-backstage deployment is available publicly for anyone to access. This means that if a secret were hardcoded within a docker image, everyone could now see it. Vulnerability scanning for Docker local images allows developers and development teams to review the security state of the container images and take actions to fix issues identified during the scan, resulting in more secure deployments. Docker Scan runs on the Snyk engine, providing users with visibility into the security posture of their local Dockerfiles and local images.","title":"Background"},{"location":"RFC/docker-scanning/#goal","text":"Scan each build/push for vulnerabilities to prevent a deployment from releasing private information to GitHub Packages.","title":"Goal"},{"location":"RFC/docker-scanning/#findings","text":"","title":"Findings"},{"location":"RFC/docker-scanning/#docker-scan","text":"By default, docker allows an image to be scanned ten times within a month before it requires a free account with Snyk . Docker and Snyk have partnered together to bring security natively into the development workflow by providing a simple and streamlined approach for developers to build and deploy secure containers. An image can be scanned using Docker Hub or by using docker scan <image_name> within the CLI. Some important tags can be included to limit issues: Excluding the base image . --exclude-base View the JSON output . --json The scan checks for known vulnerabilities in public GitHub repos, npm packages and Docker images. So it does not scan for leaked credentials/secrets.","title":"Docker Scan"},{"location":"RFC/docker-scanning/#credential-secret-scanning","text":"git-secrets is an open source repo that is designed to prevent passwords and other sensitive information from being committed to a repo. It scans commits, commit messages, and --no-ff merges to prevent adding secrets into your git repositories. If a commit, commit message, or any commit in a --no-ff merge history matches one of your configured prohibited regular expression patterns, then the commit is rejected.","title":"Credential / Secret scanning"},{"location":"RFC/docker-scanning/#recommendation","text":"","title":"Recommendation"},{"location":"RFC/docker-scanning/#vulnerability-scanning","text":"I tried building a fresh frontend image using docker build -t backstage-frontend -f Dockerfile.dockerbuild . and then scanning it by using docker scan backstage-frontend I think using docker scan for our CI/CD pipelines would be an excellent fit for our security needs. The only issue is that we'll need to create a Snyk account for lighthouse-backstage to scan Docker images more than ten times a month. Another thing I found is that scanning the backstage-frontend image returned a large number of issues related to the base image (NGINX). Excluding the base, the image limited the number of issues to just 7, which I think is excellent, and a lot easier to work with. I wasn't able to find a convenient GH Action that can fail a pipeline for specified criteria. So we might have to build a script that can analyze the JSON output of the docker scan. Otherwise, we could follow this guide on how to use Snyk to scan our docker image within CI/CD. I don't think it would be too hard to adapt it to use GitHub Actions. As stated above, Docker Scan / Snyk only checks for KNOWN vulnerabilities reported from other packages. It does not scan for leaked credentials/secrets.","title":"Vulnerability Scanning"},{"location":"RFC/docker-scanning/#git-secrets","text":"I think that using git-secrets within a pre-commit hook would be the best route to prevent secrets from being committed to the lighthouse-backstage repo. It's pretty easy to hard-code a secret when building a feature or debugging an issue and forget to remove it.","title":"Git-Secrets"},{"location":"RFC/docker-scanning/#references","text":"Scan Images Vulnerability Scanning Snyk Snyk CI/CD Scanning git-secrets","title":"References"},{"location":"RFC/enforce-build-performance/","text":"[RFC] Enforce Build Performance \u00b6 Summary : Enforcing a built time will help us track how efficient our software pipeline is along with helping to expose issues that may surface as random spikes in the build time. We can surface these issues more directly by explicitly causing builds to fail when they go over a pre-determined amount of time. Background \u00b6 Backstage has been setup to use a container for a separate frontend that uses NGINX and a backend container. The build time seems to be about 12-13 minutes on average. Some can take as long as 15 while others can be as short as 7 minutes. Goal \u00b6 Our goal is to enforce a maximum build time (15 minutes) that will allow us to keep track of how efficient our software pipeline is along with helping to expose issues that effect the application. GitHub Action Artifacts should be used the track the build-time and prevent PR's that increase the overall build time. Information \u00b6 GitHub Action Artifacts \u00b6 Storing Workflow Data as Artifacts Artifacts allow you to persist data after a job has completed, and share that data with another job in the same workflow. An artifact is a file or collection of files produced during a workflow run. For example, you can use artifacts to save your build and test output after a workflow run has ended. To share data between jobs: Uploading files: Give the uploaded file a name and upload the data before the job ends. Downloading files: You can only download artifacts that were uploaded during the same workflow run. When you download a file, you can reference it by name. Upload Artifact Action Download Artifact Action Tracking Build Time \u00b6 Get Workflow Run Usage Returns the total run time for a specific workflow run. Usage is listed for each GitHub-hosted runner operating system in milliseconds. List workflow runs This action can list all workflow runs for a workflow. Additional parameters can be used to narrow a search. Stopping PR's by Build Time \u00b6 Jobs can have a timeout-minutes setting that will fail a job if it exceeds the timeout. By default, it is 360. It can be set on a job or a single step within a job. Timeout-minutes ref 1 2 3 my-job : runs-on : ubuntu-latest timeout-minutes : 15 Recommendation \u00b6 I think the best way to enforce a build time along with keeping track of builds over time should be done by using GitHub Artifacts. An extra job should be added to the build-containers workflow that grabs all the previous workflow build times from the main branch, calculates an average build time, and then sets that value as an ENV variable. 1 2 3 4 5 # Obtaining workflow run usage curl \\ -H \"Accept: application/vnd.github.v3+json\" \\ https://api.github.com/repos/ { owner } / { repo } /actions/runs/ { run_id } /timing 1 2 3 4 5 6 7 # Setting an ENV variable jobs : example-job : steps : - run : | echo 'NEW_ENV_VAR='<value> >> $GITHUB_ENV Then, each build job can have a set timeout that adds a few minutes onto the average build. If the build exceeds the minutes it'll fail and display an error message that the build time was exceeded.","title":"Enforce Build Performance"},{"location":"RFC/enforce-build-performance/#rfc-enforce-build-performance","text":"Summary : Enforcing a built time will help us track how efficient our software pipeline is along with helping to expose issues that may surface as random spikes in the build time. We can surface these issues more directly by explicitly causing builds to fail when they go over a pre-determined amount of time.","title":"[RFC] Enforce Build Performance"},{"location":"RFC/enforce-build-performance/#background","text":"Backstage has been setup to use a container for a separate frontend that uses NGINX and a backend container. The build time seems to be about 12-13 minutes on average. Some can take as long as 15 while others can be as short as 7 minutes.","title":"Background"},{"location":"RFC/enforce-build-performance/#goal","text":"Our goal is to enforce a maximum build time (15 minutes) that will allow us to keep track of how efficient our software pipeline is along with helping to expose issues that effect the application. GitHub Action Artifacts should be used the track the build-time and prevent PR's that increase the overall build time.","title":"Goal"},{"location":"RFC/enforce-build-performance/#information","text":"","title":"Information"},{"location":"RFC/enforce-build-performance/#github-action-artifacts","text":"Storing Workflow Data as Artifacts Artifacts allow you to persist data after a job has completed, and share that data with another job in the same workflow. An artifact is a file or collection of files produced during a workflow run. For example, you can use artifacts to save your build and test output after a workflow run has ended. To share data between jobs: Uploading files: Give the uploaded file a name and upload the data before the job ends. Downloading files: You can only download artifacts that were uploaded during the same workflow run. When you download a file, you can reference it by name. Upload Artifact Action Download Artifact Action","title":"GitHub Action Artifacts"},{"location":"RFC/enforce-build-performance/#tracking-build-time","text":"Get Workflow Run Usage Returns the total run time for a specific workflow run. Usage is listed for each GitHub-hosted runner operating system in milliseconds. List workflow runs This action can list all workflow runs for a workflow. Additional parameters can be used to narrow a search.","title":"Tracking Build Time"},{"location":"RFC/enforce-build-performance/#stopping-prs-by-build-time","text":"Jobs can have a timeout-minutes setting that will fail a job if it exceeds the timeout. By default, it is 360. It can be set on a job or a single step within a job. Timeout-minutes ref 1 2 3 my-job : runs-on : ubuntu-latest timeout-minutes : 15","title":"Stopping PR's by Build Time"},{"location":"RFC/enforce-build-performance/#recommendation","text":"I think the best way to enforce a build time along with keeping track of builds over time should be done by using GitHub Artifacts. An extra job should be added to the build-containers workflow that grabs all the previous workflow build times from the main branch, calculates an average build time, and then sets that value as an ENV variable. 1 2 3 4 5 # Obtaining workflow run usage curl \\ -H \"Accept: application/vnd.github.v3+json\" \\ https://api.github.com/repos/ { owner } / { repo } /actions/runs/ { run_id } /timing 1 2 3 4 5 6 7 # Setting an ENV variable jobs : example-job : steps : - run : | echo 'NEW_ENV_VAR='<value> >> $GITHUB_ENV Then, each build job can have a set timeout that adds a few minutes onto the average build. If the build exceeds the minutes it'll fail and display an error message that the build time was exceeded.","title":"Recommendation"},{"location":"RFC/env-promotion/","text":"Environment Promotion Process \u00b6 Summary : We want to determine a set of promotion criteria that can be applied to our application to ensure code meets a certain standard before being deployed to the production environment. Background \u00b6 Part of our CI/CD process involves testing our application in a series of environments to validate that our application Goal \u00b6 Ensure that our application must meet a set of requirements before it is deemed fit to deploy to the production environment. We want to validate that the: - Application's deployment mechanism functions correctly - Application is bug/error free and performs all of its functions as intended - Application responds appropriately to stress testing Note: Do we need more things to validate? What else will ensure the code is production ready? Environments \u00b6 Sandbox Environment \u00b6 This is a testing environment that will only be deployed to using a manual process. Dev Environment \u00b6 For our CI/CD pipeline, the Development environment is the first environment the application will be deployed to. One of the main purposes of this stage is to validate our deployment mechanism. Successfully deploying to this environment is the first step to ensuring that we are able to deploy to any subsequent environment and ultimately our production environment. The other main purpose of this environment is to validate that the application is bug/error free and that the application is functioning as intended. Here we can implement most of our testing, like browser tests, integration tests, regression testing, database testing, etc. to ensure that the application meets a certain requirement before being promoted to the next stage. QA Environment \u00b6 QA is the final environment before code can be promoted to a Production environment. The QA environment provides the opportunity to run any additional tests in an as-close-to-production environment as possible. At this stage we can implement performance and load testing for our application. The purpose of these tests is to validate the reliability, scalability, and resource usage of our application. Note: Typically, more resources are allocated for a production environment than a non-production so it is important to take that into consideration when conducting performance and load testing in a non-production environment. Production \u00b6 Final Stage in the code promotion process. At this point, all of our changes are live and customers can interact with our application. Recommendations \u00b6 Use a single build for every environment \u00b6 We can modify app-config.yaml to use variables so a single image can be re-used to deploy to multiple environments. This way we do not have to spend time or resources rebuilding the image when promoting from one environment to the next. This also eliminates the possibility of producing a build, at some point in the pipeline, that differs from the original build that was created at the start of the pipeline. We need to be able to ensure that the product we have thoroughly tested is also the same product we deliver. # Example app-config.yaml app: title: DVP Developer Portal baseUrl: ${FE_ENDPOINT} organization: name: My Company backend: baseUrl: ${BE_ENDPOINT} listen: port: ${BE_LISTEN_PORT} csp: connect-src: [\"'self'\", 'http:', 'https:'] cors: origin: ${FE_ENDPOINT} methods: [GET, POST, PUT, DELETE] credentials: true cache: store: memory database: client: pg connection: host: ${DB_HOST} port: ${DB_PORT} user: ${DB_USER} password: ${DB_PASSWORD} ...","title":"Environment Promotion"},{"location":"RFC/env-promotion/#environment-promotion-process","text":"Summary : We want to determine a set of promotion criteria that can be applied to our application to ensure code meets a certain standard before being deployed to the production environment.","title":"Environment Promotion Process"},{"location":"RFC/env-promotion/#background","text":"Part of our CI/CD process involves testing our application in a series of environments to validate that our application","title":"Background"},{"location":"RFC/env-promotion/#goal","text":"Ensure that our application must meet a set of requirements before it is deemed fit to deploy to the production environment. We want to validate that the: - Application's deployment mechanism functions correctly - Application is bug/error free and performs all of its functions as intended - Application responds appropriately to stress testing Note: Do we need more things to validate? What else will ensure the code is production ready?","title":"Goal"},{"location":"RFC/env-promotion/#environments","text":"","title":"Environments"},{"location":"RFC/env-promotion/#sandbox-environment","text":"This is a testing environment that will only be deployed to using a manual process.","title":"Sandbox Environment"},{"location":"RFC/env-promotion/#dev-environment","text":"For our CI/CD pipeline, the Development environment is the first environment the application will be deployed to. One of the main purposes of this stage is to validate our deployment mechanism. Successfully deploying to this environment is the first step to ensuring that we are able to deploy to any subsequent environment and ultimately our production environment. The other main purpose of this environment is to validate that the application is bug/error free and that the application is functioning as intended. Here we can implement most of our testing, like browser tests, integration tests, regression testing, database testing, etc. to ensure that the application meets a certain requirement before being promoted to the next stage.","title":"Dev Environment"},{"location":"RFC/env-promotion/#qa-environment","text":"QA is the final environment before code can be promoted to a Production environment. The QA environment provides the opportunity to run any additional tests in an as-close-to-production environment as possible. At this stage we can implement performance and load testing for our application. The purpose of these tests is to validate the reliability, scalability, and resource usage of our application. Note: Typically, more resources are allocated for a production environment than a non-production so it is important to take that into consideration when conducting performance and load testing in a non-production environment.","title":"QA Environment"},{"location":"RFC/env-promotion/#production","text":"Final Stage in the code promotion process. At this point, all of our changes are live and customers can interact with our application.","title":"Production"},{"location":"RFC/env-promotion/#recommendations","text":"","title":"Recommendations"},{"location":"RFC/env-promotion/#use-a-single-build-for-every-environment","text":"We can modify app-config.yaml to use variables so a single image can be re-used to deploy to multiple environments. This way we do not have to spend time or resources rebuilding the image when promoting from one environment to the next. This also eliminates the possibility of producing a build, at some point in the pipeline, that differs from the original build that was created at the start of the pipeline. We need to be able to ensure that the product we have thoroughly tested is also the same product we deliver. # Example app-config.yaml app: title: DVP Developer Portal baseUrl: ${FE_ENDPOINT} organization: name: My Company backend: baseUrl: ${BE_ENDPOINT} listen: port: ${BE_LISTEN_PORT} csp: connect-src: [\"'self'\", 'http:', 'https:'] cors: origin: ${FE_ENDPOINT} methods: [GET, POST, PUT, DELETE] credentials: true cache: store: memory database: client: pg connection: host: ${DB_HOST} port: ${DB_PORT} user: ${DB_USER} password: ${DB_PASSWORD} ...","title":"Use a single build for every environment"},{"location":"RFC/github-access/","text":"[RFC] GitHub Access and Authentication \u00b6 Summary \u00b6 The Backstage application's backend needs access to GitHub API's and OAuth which both require some form of authentication. There are several ways to provide authentication credentials. Currently we are using Personal Access Tokens but these are tied to individual users. We want to determine if we can avoid using Personal Access Tokens for GitHub access. Background \u00b6 One of the core features of the Backstage applications is the software catalog. The software catalog needs to pull information from YAML files. These YAML files are stored on repositories located on GitHub. The Backstage application will need some form of authentication in order to access different repositories across an organization. The Backstage documentation outlines two approaches for backend authentication: using GitHub Apps and using GitHub OAuth. The authentication models of these two approaches are different and we want to compare the two to evaluate which one is a better suit for our needs. Goal \u00b6 Compare the pros and cons of using GitHub Apps versus GitHub OAuth for Backstage's backend authentication. Findings \u00b6 GitHub Apps \u00b6 GitHub Apps are the officially recommended way to integrate with GitHub because they offer much more granular permissions to access data. GitHub Apps are first-class actors within GitHub. A GitHub App acts on its own behalf, taking actions via the API directly using its own identity, which means you don't need to maintain a bot or service account as a separate user. GitHub Apps can be installed directly on organizations and user accounts and granted access to specific repositories. They come with built-in webhooks and narrow, specific permissions. Don't expect the GitHub App to know and do everything a user can. Don't use a GitHub App if you just need a \"Login with GitHub\" service. But a GitHub App can use a user identification flow to log users in and do other things. Don't build a GitHub App if you only want to act as a GitHub user and do everything that user can do. OAuth App \u00b6 An OAuth App uses GitHub as an identity provider to authenticate as the user who grants access to the app. This means when a user grants an OAuth App access, they grant permissions to all repositories they have access to in their account, and also to any organizations they belong to that haven't blocked third-party access. An OAuth App should always act as the authenticated GitHub user across all of GitHub (for example, when providing user notifications). An OAuth App can be used as an identity provider by enabling a \"Login with GitHub\" for the authenticated user. Don't build an OAuth App if you want your application to act on a single repository. With the repo OAuth scope, OAuth apps can act on all of the authenticated user's repositories. Don't build an OAuth App to act as an application for your team or company. OAuth Apps authenticate as a single user, so if one person creates an OAuth App for a company to use, and then they leave the company, no one else will have access to it. Backstage GitHub Apps \u00b6 The benefits of using GitHub Apps for Backstage include: - Higher rate limits - Backstage can act as an application instead of a user or bot - Clearer and better authorization model Caveats - It's not possible to have multiple Backstage GitHub Apps installed in the same GitHub organization, to be handled by Backstage. Backstage doesn't check through all the registered GitHub Apps to see which ones are installed for a particular repository. Backstage only respects global Organization installs right now. - App permissions is not managed by Backstage. They're created with some simple default permissions which we can change as we need, but we will need to update them in the GitHub web console, not in Backstage right now. The permissions that are defaulted are metadata:read and contents:read . - The created GitHub App is private by default, this is most likely what you want for github.com but it's recommended to make your application public for GitHub Enterprise in order to share application across your GHE organizations. Recommendation \u00b6 Creating a Backstage GitHub App \u00b6 It seems like the recommended approach is to create a GitHub App but Backstage cannot have multiple Backstage GitHub Apps on the same organization. So I believe this means that we would only be able to use a Backstage GitHub App if no one else in the DVA is using Backstage GitHub App. I know there is another developer portal but I don't know if they are even using Backstage or if they're planning to use a Backstage GitHub App. If they are using Backstage and plan to use a GitHub App then I think we will have to use OAuth apps.","title":"GitHub Access"},{"location":"RFC/github-access/#rfc-github-access-and-authentication","text":"","title":"[RFC] GitHub Access and Authentication"},{"location":"RFC/github-access/#summary","text":"The Backstage application's backend needs access to GitHub API's and OAuth which both require some form of authentication. There are several ways to provide authentication credentials. Currently we are using Personal Access Tokens but these are tied to individual users. We want to determine if we can avoid using Personal Access Tokens for GitHub access.","title":"Summary"},{"location":"RFC/github-access/#background","text":"One of the core features of the Backstage applications is the software catalog. The software catalog needs to pull information from YAML files. These YAML files are stored on repositories located on GitHub. The Backstage application will need some form of authentication in order to access different repositories across an organization. The Backstage documentation outlines two approaches for backend authentication: using GitHub Apps and using GitHub OAuth. The authentication models of these two approaches are different and we want to compare the two to evaluate which one is a better suit for our needs.","title":"Background"},{"location":"RFC/github-access/#goal","text":"Compare the pros and cons of using GitHub Apps versus GitHub OAuth for Backstage's backend authentication.","title":"Goal"},{"location":"RFC/github-access/#findings","text":"","title":"Findings"},{"location":"RFC/github-access/#github-apps","text":"GitHub Apps are the officially recommended way to integrate with GitHub because they offer much more granular permissions to access data. GitHub Apps are first-class actors within GitHub. A GitHub App acts on its own behalf, taking actions via the API directly using its own identity, which means you don't need to maintain a bot or service account as a separate user. GitHub Apps can be installed directly on organizations and user accounts and granted access to specific repositories. They come with built-in webhooks and narrow, specific permissions. Don't expect the GitHub App to know and do everything a user can. Don't use a GitHub App if you just need a \"Login with GitHub\" service. But a GitHub App can use a user identification flow to log users in and do other things. Don't build a GitHub App if you only want to act as a GitHub user and do everything that user can do.","title":"GitHub Apps"},{"location":"RFC/github-access/#oauth-app","text":"An OAuth App uses GitHub as an identity provider to authenticate as the user who grants access to the app. This means when a user grants an OAuth App access, they grant permissions to all repositories they have access to in their account, and also to any organizations they belong to that haven't blocked third-party access. An OAuth App should always act as the authenticated GitHub user across all of GitHub (for example, when providing user notifications). An OAuth App can be used as an identity provider by enabling a \"Login with GitHub\" for the authenticated user. Don't build an OAuth App if you want your application to act on a single repository. With the repo OAuth scope, OAuth apps can act on all of the authenticated user's repositories. Don't build an OAuth App to act as an application for your team or company. OAuth Apps authenticate as a single user, so if one person creates an OAuth App for a company to use, and then they leave the company, no one else will have access to it.","title":"OAuth App"},{"location":"RFC/github-access/#backstage-github-apps","text":"The benefits of using GitHub Apps for Backstage include: - Higher rate limits - Backstage can act as an application instead of a user or bot - Clearer and better authorization model Caveats - It's not possible to have multiple Backstage GitHub Apps installed in the same GitHub organization, to be handled by Backstage. Backstage doesn't check through all the registered GitHub Apps to see which ones are installed for a particular repository. Backstage only respects global Organization installs right now. - App permissions is not managed by Backstage. They're created with some simple default permissions which we can change as we need, but we will need to update them in the GitHub web console, not in Backstage right now. The permissions that are defaulted are metadata:read and contents:read . - The created GitHub App is private by default, this is most likely what you want for github.com but it's recommended to make your application public for GitHub Enterprise in order to share application across your GHE organizations.","title":"Backstage GitHub Apps"},{"location":"RFC/github-access/#recommendation","text":"","title":"Recommendation"},{"location":"RFC/github-access/#creating-a-backstage-github-app","text":"It seems like the recommended approach is to create a GitHub App but Backstage cannot have multiple Backstage GitHub Apps on the same organization. So I believe this means that we would only be able to use a Backstage GitHub App if no one else in the DVA is using Backstage GitHub App. I know there is another developer portal but I don't know if they are even using Backstage or if they're planning to use a Backstage GitHub App. If they are using Backstage and plan to use a GitHub App then I think we will have to use OAuth apps.","title":"Creating a Backstage GitHub App"},{"location":"RFC/google-analytics/","text":"[RFC] Google Analytics \u00b6 Summary \u00b6 Analytic tracking allows us to track and understand our customer's behavior, user experience, online content, device functionality and more for the application. Background \u00b6 Currently we have no method to track user events within the app. Backstage has a basic Analytics api that can be used. There are also some integrations for other analytic trackers like GA. Goal \u00b6 Decide what needs to be done in order to use Google Analytics within the app. Findings \u00b6 Analytics Module: Google Analytics \u00b6 Backstage has a API integration for Google Analytics. Setting up is very easy: install the plugin within packages/app and run yarn add @backstage/plugin-analytics-module-ga Open packages/app/src/api.ts Import analyticsApiRef and configApiRef from @backstage/core-plugin-api Import GoogleAnalytics from @backstage/plugin-analytics-module-ga use createApiFactory to add the analytics api to the app. api.ts should then look like the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 // packages/app/src/apis.ts import { analyticsApiRef , configApiRef } from '@backstage/core-plugin-api' ; import { GoogleAnalytics } from '@backstage/plugin-analytics-module-ga' ; export const apis : AnyApiFactory [] = [ ... OtherImports // Instantiate and register the GA Analytics API Implementation. createApiFactory ({ api : analyticsApiRef , deps : { configApi : configApiRef }, factory : ({ configApi }) => GoogleAnalytics . fromConfig ( configApi ), }), ]; Configure app-config.yaml for GA. 1 2 3 4 app : analytics : ga : trackingId : <ga-tracking-id> # Might also be named Measurement ID within GA Custom Dimensions can also be included for further customization. Like tracking specific plugins. There is also a Test/Debug mode and a Development mode Once everything is configured you can use the useAnalytics react hook within any React components to send events. For Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import React from 'react' ; import { useAnalytics } from '@backstage/core-plugin-api' ; export const MyCoolComponent = () => { const analytics = useAnalytics (); const trackFunctionality = () => { analytics . captureEvent ( 'test' , 'test-service' ); }; const handleClick = () => { analytics . captureEvent ( 'click' , 'The user clicked something!' ); // do something }; useEffect (() => { // Capture page view analytics . captureEvent ( 'navigate' , 'Viewed MyCoolComponent' ); }, []); return < div > {... content } < /div>; }; @analytics/google-analytics \u00b6 Another package that allows us to send Google Analytics from the frontend is @analytics/google-analytics . It's a library that exports a plugin for analytics. Analytics is a lightweight analytics abstraction library for tracking page views, custom events, and identify visitors. You must install the analytics and @analytics/google-analytics packages in order for it to work. 1 2 yarn add analytics yarn add @analytics/google-analytics Then we just have to initialize the plugin. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import Analytics from 'analytics' ; import googleAnalytics from '@analytics/google-analytics' ; const analytics = Analytics ({ app : 'awesome-app' , plugins : [ googleAnalytics ({ trackingId : 'UA-1234567' , }), ], }); /* Track a page view */ analytics . page (); /* Track a custom event */ analytics . track ( 'playedVideo' , { category : 'Videos' , label : 'Fall Campaign' , value : 42 , }); /* Identify a visitor */ analytics . identify ( 'user-id-xyz' , { firstName : 'bill' , lastName : 'murray' , }); Recommendation \u00b6 I think it'd be best to use Analytics Module: Google Analytics to send events to GA. It's easy to setup and allows us to use the useAnalytics() hook within any components. Reference \u00b6 API integration Backstage Analytics @analytics/google-analytics","title":"Google Analytics"},{"location":"RFC/google-analytics/#rfc-google-analytics","text":"","title":"[RFC] Google Analytics"},{"location":"RFC/google-analytics/#summary","text":"Analytic tracking allows us to track and understand our customer's behavior, user experience, online content, device functionality and more for the application.","title":"Summary"},{"location":"RFC/google-analytics/#background","text":"Currently we have no method to track user events within the app. Backstage has a basic Analytics api that can be used. There are also some integrations for other analytic trackers like GA.","title":"Background"},{"location":"RFC/google-analytics/#goal","text":"Decide what needs to be done in order to use Google Analytics within the app.","title":"Goal"},{"location":"RFC/google-analytics/#findings","text":"","title":"Findings"},{"location":"RFC/google-analytics/#analytics-module-google-analytics","text":"Backstage has a API integration for Google Analytics. Setting up is very easy: install the plugin within packages/app and run yarn add @backstage/plugin-analytics-module-ga Open packages/app/src/api.ts Import analyticsApiRef and configApiRef from @backstage/core-plugin-api Import GoogleAnalytics from @backstage/plugin-analytics-module-ga use createApiFactory to add the analytics api to the app. api.ts should then look like the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 // packages/app/src/apis.ts import { analyticsApiRef , configApiRef } from '@backstage/core-plugin-api' ; import { GoogleAnalytics } from '@backstage/plugin-analytics-module-ga' ; export const apis : AnyApiFactory [] = [ ... OtherImports // Instantiate and register the GA Analytics API Implementation. createApiFactory ({ api : analyticsApiRef , deps : { configApi : configApiRef }, factory : ({ configApi }) => GoogleAnalytics . fromConfig ( configApi ), }), ]; Configure app-config.yaml for GA. 1 2 3 4 app : analytics : ga : trackingId : <ga-tracking-id> # Might also be named Measurement ID within GA Custom Dimensions can also be included for further customization. Like tracking specific plugins. There is also a Test/Debug mode and a Development mode Once everything is configured you can use the useAnalytics react hook within any React components to send events. For Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import React from 'react' ; import { useAnalytics } from '@backstage/core-plugin-api' ; export const MyCoolComponent = () => { const analytics = useAnalytics (); const trackFunctionality = () => { analytics . captureEvent ( 'test' , 'test-service' ); }; const handleClick = () => { analytics . captureEvent ( 'click' , 'The user clicked something!' ); // do something }; useEffect (() => { // Capture page view analytics . captureEvent ( 'navigate' , 'Viewed MyCoolComponent' ); }, []); return < div > {... content } < /div>; };","title":"Analytics Module: Google Analytics"},{"location":"RFC/google-analytics/#analyticsgoogle-analytics","text":"Another package that allows us to send Google Analytics from the frontend is @analytics/google-analytics . It's a library that exports a plugin for analytics. Analytics is a lightweight analytics abstraction library for tracking page views, custom events, and identify visitors. You must install the analytics and @analytics/google-analytics packages in order for it to work. 1 2 yarn add analytics yarn add @analytics/google-analytics Then we just have to initialize the plugin. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import Analytics from 'analytics' ; import googleAnalytics from '@analytics/google-analytics' ; const analytics = Analytics ({ app : 'awesome-app' , plugins : [ googleAnalytics ({ trackingId : 'UA-1234567' , }), ], }); /* Track a page view */ analytics . page (); /* Track a custom event */ analytics . track ( 'playedVideo' , { category : 'Videos' , label : 'Fall Campaign' , value : 42 , }); /* Identify a visitor */ analytics . identify ( 'user-id-xyz' , { firstName : 'bill' , lastName : 'murray' , });","title":"@analytics/google-analytics"},{"location":"RFC/google-analytics/#recommendation","text":"I think it'd be best to use Analytics Module: Google Analytics to send events to GA. It's easy to setup and allows us to use the useAnalytics() hook within any components.","title":"Recommendation"},{"location":"RFC/google-analytics/#reference","text":"API integration Backstage Analytics @analytics/google-analytics","title":"Reference"},{"location":"RFC/investigate-esbuild-and-vite/","text":"[RFC] Investigate Esbuild and Vite \u00b6 Summary \u00b6 Webpack is the default bundler that came with our instance of Backstage. It's worked great for us so far, but building our application can take up to 5-7 minutes to build. There are some bundler alternatives like esbuild and vite that claim to be 10x-100x faster than webpack. This could speed up our build/deploy time by a significant amount. Background \u00b6 Webpack is a tool that lets you bundle your JavaScript applications (supporting both ESM and CommonJS), and it can be extended to support many different assets such as images, fonts, and stylesheets. It's the default bundler that came with our backstage implementation. Goal \u00b6 Decide whether it's worth moving to esbuild or vite for building our application. They should be able to bundle our application and support React + Typescript. Findings \u00b6 esbuild \u00b6 Build Time: < 1s The bundler outputs code for the browser by default. For development builds, we can add source maps. The build can also be minified for production. Build API The build API call operates on one or more files in the files system. This allows the files to reference each other and be bundled together. Esbuild does not bundle by default and must use the --bundle flag. The Build API can take a ton of options, to name a few: Bundle Entry Points Format Inject Loaders Splitting Watch Esbuild can also use some advanced options. Here are some that stick out to me: Asset names Conditions Entry names Global name JSX (factory/fragment) Log level Node Paths Tsconfig Esbuild has a loader that is enabled by default for .js , .cjs , .mjs . The loader has support for TypeScript, JSX, JSON, and CSS. Note: Esbuild DOES NOT do any type checking This blog post covers setting up esbuild for TypeScript applications. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // Config used to bundle app const esbuild = require ( 'esbuild' ) // Automatically exclude all node_modules from the bundled version const { nodeExternalsPlugin } = require ( 'esbuild-node-externals' ) esbuild . build ({ entryPoints : [ './src/index.ts' ], outfile : 'lib/index.js' , bundle : true , minify : true , platform : 'node' , sourcemap : true , target : 'node14' , plugins : [ nodeExternalsPlugin ()] }). catch (() => process . exit ( 1 )) Vite \u00b6 (Pronounced like \"veet,\" it's french for quick) Build Time: 45s-47s Vite uses esbuild to build/bundle applications quickly. It consists of two major parts: A dev server that provides rich feature enhancements over native ES modules, for example, extremely fast Hot Module Replacement (HMR). A build command that bundles your code with Rollup, pre-configured to output highly optimized static assets for production. Has full Typing support for TypeScript, an extensible Plugin API, and JavaScript API. Vite has a scaffold that can be adapted for React + TypeScript. yarn create vite --template react-ts Also, for Vite to work, we need an index.html file within the root of the app folder. We can specify a different index.html for the dist directory. Some required settings must be set in the tsconfig file for Vite to work with TypeScript. isolatedModules : esbuild transpiles without type info. Doesn't support features like const enum and implicit type-only imports useDefineForClassFields : This should be true by default per typescript anyways, but is still required. 1 2 3 4 5 6 { \"compilerOptions\" : { \"isolatedModules\" : true , \"useDefineForClassFields\" : true , } } Recommendation \u00b6 I believe it is worth our time to move from webpack to using Vite to build our application. I was able to bundle the frontend application by following Vite's getting started section. Vite uses esbuild to bundle projects, so we won't need to use it directly. There might be some configuration settings we'll need to pass to esbuild, but those are easy to locate. I found a fantastic article about setting up Vite for an application that uses React + TypeScript. It looks like we need to create the vite scaffold using yarn create vite --template react-ts and copy/paste some of the configurations over to the app. This would be our best resource, along with Vites documentation, to convert our application. https://javascript.plainenglish.io/migrating-a-150k-loc-codebase-to-vite-and-esbuild-how-part-2-3-91b0b873f388 The upgrade burden seems minimal. Vite has out-of-the-box features for projects that use React + TypeScript. From what I could tell, we wouldn't need to change any of our code, just updating the tsconfig and adding vite configs to our project. Because of this, it'd be easy to swap back to webpack if we come across any problems down the line. Reference \u00b6 esbuild Setting up esbuild for typescript libraries vite Moving from Webpack to Vite","title":"Investigate Esbuild and Vite"},{"location":"RFC/investigate-esbuild-and-vite/#rfc-investigate-esbuild-and-vite","text":"","title":"[RFC] Investigate Esbuild and Vite"},{"location":"RFC/investigate-esbuild-and-vite/#summary","text":"Webpack is the default bundler that came with our instance of Backstage. It's worked great for us so far, but building our application can take up to 5-7 minutes to build. There are some bundler alternatives like esbuild and vite that claim to be 10x-100x faster than webpack. This could speed up our build/deploy time by a significant amount.","title":"Summary"},{"location":"RFC/investigate-esbuild-and-vite/#background","text":"Webpack is a tool that lets you bundle your JavaScript applications (supporting both ESM and CommonJS), and it can be extended to support many different assets such as images, fonts, and stylesheets. It's the default bundler that came with our backstage implementation.","title":"Background"},{"location":"RFC/investigate-esbuild-and-vite/#goal","text":"Decide whether it's worth moving to esbuild or vite for building our application. They should be able to bundle our application and support React + Typescript.","title":"Goal"},{"location":"RFC/investigate-esbuild-and-vite/#findings","text":"","title":"Findings"},{"location":"RFC/investigate-esbuild-and-vite/#esbuild","text":"Build Time: < 1s The bundler outputs code for the browser by default. For development builds, we can add source maps. The build can also be minified for production. Build API The build API call operates on one or more files in the files system. This allows the files to reference each other and be bundled together. Esbuild does not bundle by default and must use the --bundle flag. The Build API can take a ton of options, to name a few: Bundle Entry Points Format Inject Loaders Splitting Watch Esbuild can also use some advanced options. Here are some that stick out to me: Asset names Conditions Entry names Global name JSX (factory/fragment) Log level Node Paths Tsconfig Esbuild has a loader that is enabled by default for .js , .cjs , .mjs . The loader has support for TypeScript, JSX, JSON, and CSS. Note: Esbuild DOES NOT do any type checking This blog post covers setting up esbuild for TypeScript applications. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // Config used to bundle app const esbuild = require ( 'esbuild' ) // Automatically exclude all node_modules from the bundled version const { nodeExternalsPlugin } = require ( 'esbuild-node-externals' ) esbuild . build ({ entryPoints : [ './src/index.ts' ], outfile : 'lib/index.js' , bundle : true , minify : true , platform : 'node' , sourcemap : true , target : 'node14' , plugins : [ nodeExternalsPlugin ()] }). catch (() => process . exit ( 1 ))","title":"esbuild"},{"location":"RFC/investigate-esbuild-and-vite/#vite","text":"(Pronounced like \"veet,\" it's french for quick) Build Time: 45s-47s Vite uses esbuild to build/bundle applications quickly. It consists of two major parts: A dev server that provides rich feature enhancements over native ES modules, for example, extremely fast Hot Module Replacement (HMR). A build command that bundles your code with Rollup, pre-configured to output highly optimized static assets for production. Has full Typing support for TypeScript, an extensible Plugin API, and JavaScript API. Vite has a scaffold that can be adapted for React + TypeScript. yarn create vite --template react-ts Also, for Vite to work, we need an index.html file within the root of the app folder. We can specify a different index.html for the dist directory. Some required settings must be set in the tsconfig file for Vite to work with TypeScript. isolatedModules : esbuild transpiles without type info. Doesn't support features like const enum and implicit type-only imports useDefineForClassFields : This should be true by default per typescript anyways, but is still required. 1 2 3 4 5 6 { \"compilerOptions\" : { \"isolatedModules\" : true , \"useDefineForClassFields\" : true , } }","title":"Vite"},{"location":"RFC/investigate-esbuild-and-vite/#recommendation","text":"I believe it is worth our time to move from webpack to using Vite to build our application. I was able to bundle the frontend application by following Vite's getting started section. Vite uses esbuild to bundle projects, so we won't need to use it directly. There might be some configuration settings we'll need to pass to esbuild, but those are easy to locate. I found a fantastic article about setting up Vite for an application that uses React + TypeScript. It looks like we need to create the vite scaffold using yarn create vite --template react-ts and copy/paste some of the configurations over to the app. This would be our best resource, along with Vites documentation, to convert our application. https://javascript.plainenglish.io/migrating-a-150k-loc-codebase-to-vite-and-esbuild-how-part-2-3-91b0b873f388 The upgrade burden seems minimal. Vite has out-of-the-box features for projects that use React + TypeScript. From what I could tell, we wouldn't need to change any of our code, just updating the tsconfig and adding vite configs to our project. Because of this, it'd be easy to swap back to webpack if we come across any problems down the line.","title":"Recommendation"},{"location":"RFC/investigate-esbuild-and-vite/#reference","text":"esbuild Setting up esbuild for typescript libraries vite Moving from Webpack to Vite","title":"Reference"},{"location":"RFC/investigate-pact/","text":"[RFC] Investigate Pact \u00b6 Summary \u00b6 Contract testing is a tool used for testing HTTP and message integrations. It could help us catch CORS issues that can occur when the fronted communicates with the backend app. Background \u00b6 Pact is a code-first tool for testing HTTP and message integrations using contract tests. Contract tests assert that inter-application messages conform to a shared understanding that is documented in a contract. Without contract testing, the only way to ensure that applications will work correctly together is by using expensive and brittle integration tests. Goal \u00b6 Decide whether using Pact would allow us to catch CORS issues. Findings \u00b6 Pact \u00b6 Pact can be used within a JS app by using pact-js . They provide multiple testing examples for various frameworks, including Jest. There is a 5 Minute Guide to understand how Pact works. I highly recommend reading through it. Basic steps to using Pact: Configure the mock API Write a consumer test. Run the consumer test. Share the created contracts with the broker. Write provider verification tests. Run the provider tests. Consumer Side Testing \u00b6 Pact has a package specifically for working with Jest, jest-pact . This can make writing tests much more effortless. Instead of setting up a new Pact({...}) we can use pactWith . pactWith(JestPactOptions, (providerMock) => { /* tests go here */ }) is a wrapper that sets up a pact mock provider, applies sensible default options, and applies the setup and verification hooks so you don't have to. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 import { pactWith } from 'jest-pact' ; import { Matchers } from '@pact-foundation/pact' ; import api from 'yourCode' ; pactWith ({ consumer : 'MyConsumer' , provider : 'MyProvider' }, provider => { let client ; beforeEach (() => { client = api ( provider . mockService . baseUrl ) }); describe ( 'health endpoint' , () => { // Here we set up the interaction that the Pact // mock provider will expect. // // jest-pact takes care of validating and tearing // down the provider for you. beforeEach (() => // note the implicit return. // addInteraction returns a promise. // If you don't want to implicit return, // you will need to `await` the result provider . addInteraction ({ state : \"Server is healthy\" , uponReceiving : 'A request for API health' , willRespondWith : { status : 200 , body : { status : Matchers . like ( 'up' ), }, }, withRequest : { method : 'GET' , path : '/health' , }, }) ); // You also test that the API returns the correct // response to the data layer. // // Although Pact will ensure that the provider // returned the expected object. You need to test that // your code receives the right object. // // This is often the same as the object that was // in the network response, but (as illustrated // here) not always. it ( 'returns server health' , () => // implicit return again client . getHealth (). then ( health => { expect ( health ). toEqual ( 'up' ); })); }); Pact Broker \u00b6 It enables you to share your pacts and verification results between projects and make them useful for people too. It is the recommended way forward for serious Pact development. You can run an example pact-broker using Ruby and Rails. https://github.com/pact-foundation/pact_broker#usage It's also possible to use their docker image, https://github.com/pact-foundation/pact-broker-docker Otherwise, you'll need to run your own pact-broker . Once the broker is up and running, you can publish your pact files to it using the following command: \"pact:publish\": \"pact-broker publish <YOUR_PACT_FILES_OR_DIR> --consumer-app-version=1.0.0 --tag-with-git-branch --broker-base-url=https://your-broker-url.example.com\" Additional CLI options can be found here under pact-broker client . You can also include the username and password with --broker-username and --broker-password. Obviously, you should NEVER hard code these values. ENV variables can/should be used for this. It's also possible to create a script that can further control publishing pacts to a broker. Provider tests \u00b6 Provider tests are a lot simpler than consumer tests. We need to pull the contract from the Pact Broker and verify that the contract is correct. Provider API Testing Here is a very basic example of a Provider Test: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 const { Verifier } = require ( '@pact-foundation/pact' ); const opts = { providerBaseUrl : 'http://127.0.0.1:7000' , pactBrokerUrl : 'broker_url' , provider : 'MyProvider' , providerVersion : '1.0.0' , pactBrokerUsername : 'test' , pactBrokerPassword : 'test' , logLevel : 'INFO' , }; describe ( 'Pact Verification' , () => { it ( 'should validate the expectations of our consumer' , () => { return new Verifier ( opts ). verifyProvider (). then (() => {}); }); }); Recommendation \u00b6 Pact was pretty confusing to set up and get running. Everything eventually fell into place once I was able to get the full loop working. Pact testing would be a great idea to test any endpoints that we hit when communicating with the backend. Because of this, Pact would be a great fit for preventing CORS issues when merging branches into main or testing before we make a deployment. I also think it'd be worth our time to get the pact broker running on Docker. However, using a host like Heroku would work as well. Unfortunately, I couldn't get Pact working with Cypress, but I was able to get it working by using Jest. It wasn't easy at first, but some great documentation and some articles can walk through the setup. They are listed under the Reference section. To make setup extra easy, I'll include the files I used to create the tests below. Consumer Tests \u00b6 The Function: 1 2 3 4 5 6 7 8 9 10 11 12 import axios from 'axios' ; const defaultBaseUrl = 'http://127.0.0.1:5000' ; export const getCatalogs = () => axios . get ( ` ${ defaultBaseUrl } /catalog` , { headers : { Accept : 'application/json' , }, }) . then ( response => response . data . status ); The test: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 import { pactWith } from 'jest-pact' ; import { Matchers } from '@pact-foundation/pact' ; import { getCatalogs } from './pactCode.js' ; const EXPECTED_BODY = { metadata : { namespace : 'default' , name : 'guest' , }, apiVersion : 'backstage.io/v1alpha1' , kind : 'User' , }; pactWith ( { consumer : 'MyConsumer' , provider : 'MyProvider' , port : 5000 }, provider => { describe ( 'health endpoint' , () => { // Here we set up the interaction that the Pact // mock provider will expect. // // jest-pact takes care of validating and tearing // down the provider for you. beforeEach (() => { provider . addInteraction ({ state : 'Server is healthy' , uponReceiving : 'A request for the Catalogs' , willRespondWith : { status : 200 , body : { status : Matchers . like ( EXPECTED_BODY ), }, headers : { 'Access-Control-Allow-Origin' : '*' }, }, withRequest : { method : 'GET' , path : '/catalog' , }, }); }); // You also test that the API returns the correct // response to the data layer. // // Although Pact will ensure that the provider // returned the expected object. You need to test that // your code receives the right object. // // This is often the same as the object that was // in the network response, but (as illustrated // here) not always. it ( 'returns catalogs' , () => getCatalogs (). then ( catalogs => { expect ( catalogs ). toEqual ( EXPECTED_BODY ); })); }); }, ); Pact Broker \u00b6 The pact broker was made using Ruby and Rails, then published to Heroku. Information can be found within the findings section. Provider Tests \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 const { Verifier } = require ( '@pact-foundation/pact' ); const opts = { providerBaseUrl : 'http://127.0.0.1:7000' , pactBrokerUrl : 'https://polar-forest-27410.herokuapp.com/' , provider : 'MyProvider' , providerVersion : '1.0.0' , pactBrokerUsername : 'test' , pactBrokerPassword : 'test' , logLevel : 'INFO' , }; describe ( 'Pact Verification' , () => { it ( 'should validate the expectations of our consumer' , () => { return new Verifier ( opts ). verifyProvider (). then (() => {}); }); }); Reference \u00b6 Pact pact-js jest-pact pact-broker pact-broker-docker jest-pact-article","title":"Investigate Pact"},{"location":"RFC/investigate-pact/#rfc-investigate-pact","text":"","title":"[RFC] Investigate Pact"},{"location":"RFC/investigate-pact/#summary","text":"Contract testing is a tool used for testing HTTP and message integrations. It could help us catch CORS issues that can occur when the fronted communicates with the backend app.","title":"Summary"},{"location":"RFC/investigate-pact/#background","text":"Pact is a code-first tool for testing HTTP and message integrations using contract tests. Contract tests assert that inter-application messages conform to a shared understanding that is documented in a contract. Without contract testing, the only way to ensure that applications will work correctly together is by using expensive and brittle integration tests.","title":"Background"},{"location":"RFC/investigate-pact/#goal","text":"Decide whether using Pact would allow us to catch CORS issues.","title":"Goal"},{"location":"RFC/investigate-pact/#findings","text":"","title":"Findings"},{"location":"RFC/investigate-pact/#pact","text":"Pact can be used within a JS app by using pact-js . They provide multiple testing examples for various frameworks, including Jest. There is a 5 Minute Guide to understand how Pact works. I highly recommend reading through it. Basic steps to using Pact: Configure the mock API Write a consumer test. Run the consumer test. Share the created contracts with the broker. Write provider verification tests. Run the provider tests.","title":"Pact"},{"location":"RFC/investigate-pact/#consumer-side-testing","text":"Pact has a package specifically for working with Jest, jest-pact . This can make writing tests much more effortless. Instead of setting up a new Pact({...}) we can use pactWith . pactWith(JestPactOptions, (providerMock) => { /* tests go here */ }) is a wrapper that sets up a pact mock provider, applies sensible default options, and applies the setup and verification hooks so you don't have to. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 import { pactWith } from 'jest-pact' ; import { Matchers } from '@pact-foundation/pact' ; import api from 'yourCode' ; pactWith ({ consumer : 'MyConsumer' , provider : 'MyProvider' }, provider => { let client ; beforeEach (() => { client = api ( provider . mockService . baseUrl ) }); describe ( 'health endpoint' , () => { // Here we set up the interaction that the Pact // mock provider will expect. // // jest-pact takes care of validating and tearing // down the provider for you. beforeEach (() => // note the implicit return. // addInteraction returns a promise. // If you don't want to implicit return, // you will need to `await` the result provider . addInteraction ({ state : \"Server is healthy\" , uponReceiving : 'A request for API health' , willRespondWith : { status : 200 , body : { status : Matchers . like ( 'up' ), }, }, withRequest : { method : 'GET' , path : '/health' , }, }) ); // You also test that the API returns the correct // response to the data layer. // // Although Pact will ensure that the provider // returned the expected object. You need to test that // your code receives the right object. // // This is often the same as the object that was // in the network response, but (as illustrated // here) not always. it ( 'returns server health' , () => // implicit return again client . getHealth (). then ( health => { expect ( health ). toEqual ( 'up' ); })); });","title":"Consumer Side Testing"},{"location":"RFC/investigate-pact/#pact-broker","text":"It enables you to share your pacts and verification results between projects and make them useful for people too. It is the recommended way forward for serious Pact development. You can run an example pact-broker using Ruby and Rails. https://github.com/pact-foundation/pact_broker#usage It's also possible to use their docker image, https://github.com/pact-foundation/pact-broker-docker Otherwise, you'll need to run your own pact-broker . Once the broker is up and running, you can publish your pact files to it using the following command: \"pact:publish\": \"pact-broker publish <YOUR_PACT_FILES_OR_DIR> --consumer-app-version=1.0.0 --tag-with-git-branch --broker-base-url=https://your-broker-url.example.com\" Additional CLI options can be found here under pact-broker client . You can also include the username and password with --broker-username and --broker-password. Obviously, you should NEVER hard code these values. ENV variables can/should be used for this. It's also possible to create a script that can further control publishing pacts to a broker.","title":"Pact Broker"},{"location":"RFC/investigate-pact/#provider-tests","text":"Provider tests are a lot simpler than consumer tests. We need to pull the contract from the Pact Broker and verify that the contract is correct. Provider API Testing Here is a very basic example of a Provider Test: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 const { Verifier } = require ( '@pact-foundation/pact' ); const opts = { providerBaseUrl : 'http://127.0.0.1:7000' , pactBrokerUrl : 'broker_url' , provider : 'MyProvider' , providerVersion : '1.0.0' , pactBrokerUsername : 'test' , pactBrokerPassword : 'test' , logLevel : 'INFO' , }; describe ( 'Pact Verification' , () => { it ( 'should validate the expectations of our consumer' , () => { return new Verifier ( opts ). verifyProvider (). then (() => {}); }); });","title":"Provider tests"},{"location":"RFC/investigate-pact/#recommendation","text":"Pact was pretty confusing to set up and get running. Everything eventually fell into place once I was able to get the full loop working. Pact testing would be a great idea to test any endpoints that we hit when communicating with the backend. Because of this, Pact would be a great fit for preventing CORS issues when merging branches into main or testing before we make a deployment. I also think it'd be worth our time to get the pact broker running on Docker. However, using a host like Heroku would work as well. Unfortunately, I couldn't get Pact working with Cypress, but I was able to get it working by using Jest. It wasn't easy at first, but some great documentation and some articles can walk through the setup. They are listed under the Reference section. To make setup extra easy, I'll include the files I used to create the tests below.","title":"Recommendation"},{"location":"RFC/investigate-pact/#consumer-tests","text":"The Function: 1 2 3 4 5 6 7 8 9 10 11 12 import axios from 'axios' ; const defaultBaseUrl = 'http://127.0.0.1:5000' ; export const getCatalogs = () => axios . get ( ` ${ defaultBaseUrl } /catalog` , { headers : { Accept : 'application/json' , }, }) . then ( response => response . data . status ); The test: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 import { pactWith } from 'jest-pact' ; import { Matchers } from '@pact-foundation/pact' ; import { getCatalogs } from './pactCode.js' ; const EXPECTED_BODY = { metadata : { namespace : 'default' , name : 'guest' , }, apiVersion : 'backstage.io/v1alpha1' , kind : 'User' , }; pactWith ( { consumer : 'MyConsumer' , provider : 'MyProvider' , port : 5000 }, provider => { describe ( 'health endpoint' , () => { // Here we set up the interaction that the Pact // mock provider will expect. // // jest-pact takes care of validating and tearing // down the provider for you. beforeEach (() => { provider . addInteraction ({ state : 'Server is healthy' , uponReceiving : 'A request for the Catalogs' , willRespondWith : { status : 200 , body : { status : Matchers . like ( EXPECTED_BODY ), }, headers : { 'Access-Control-Allow-Origin' : '*' }, }, withRequest : { method : 'GET' , path : '/catalog' , }, }); }); // You also test that the API returns the correct // response to the data layer. // // Although Pact will ensure that the provider // returned the expected object. You need to test that // your code receives the right object. // // This is often the same as the object that was // in the network response, but (as illustrated // here) not always. it ( 'returns catalogs' , () => getCatalogs (). then ( catalogs => { expect ( catalogs ). toEqual ( EXPECTED_BODY ); })); }); }, );","title":"Consumer Tests"},{"location":"RFC/investigate-pact/#pact-broker_1","text":"The pact broker was made using Ruby and Rails, then published to Heroku. Information can be found within the findings section.","title":"Pact Broker"},{"location":"RFC/investigate-pact/#provider-tests_1","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 const { Verifier } = require ( '@pact-foundation/pact' ); const opts = { providerBaseUrl : 'http://127.0.0.1:7000' , pactBrokerUrl : 'https://polar-forest-27410.herokuapp.com/' , provider : 'MyProvider' , providerVersion : '1.0.0' , pactBrokerUsername : 'test' , pactBrokerPassword : 'test' , logLevel : 'INFO' , }; describe ( 'Pact Verification' , () => { it ( 'should validate the expectations of our consumer' , () => { return new Verifier ( opts ). verifyProvider (). then (() => {}); }); });","title":"Provider Tests"},{"location":"RFC/investigate-pact/#reference","text":"Pact pact-js jest-pact pact-broker pact-broker-docker jest-pact-article","title":"Reference"},{"location":"RFC/linting/","text":"[RFC] Linting Rules \u00b6 Summary : Utilize linting to improve code quality. Integrating linting into our CI process will help enforce uniform standards for code styling which will reduce syntax errors and improve readability. Background \u00b6 Programming languages like C or C++ utilize compilers to catch syntax errors before the code reaches a runtime environment. JavaScript is an interpreted language so it lacks a compilation phase where syntax errors can be identified before the code is ran. Linting software is a form of static code analyzing that can help enforce a set of stylistic rules or check for syntactical errors. Enforcing a single style will also improve the code review process by the need to address styling issues as well as ensure all team members are familiar with the style of the code. Goal \u00b6 Add linting to our CI process by using lerna to call the backstage-cli command for each of our Backstage packages and plugins. Upgrading from 7.0.0 to 11.0.0 \u00b6 Updates & changes \u00b6 Added \"@backstage/core-components\": \"^0.6.0\", to packages/app/package.json due to 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 $ backstage-cli lint --fix lerna ERR! yarn run lint --fix exited 1 in 'app' lerna ERR! yarn run lint --fix stdout: $ backstage-cli lint --fix \u2718 https://google.com/#q=import%2Fno-extraneous-dependencies '@backstage/core-components' should be listed in the project's dependencies. Run 'npm i -S @backstage/core-components' to add it src/components/catalog/EntityPage.tsx:56:1 54 | } from '@backstage/plugin-org'; 55 | import { EntityTechdocsContent } from '@backstage/plugin-techdocs'; > 56 | import { EmptyState } from '@backstage/core-components'; | ^ 57 | 58 | const cicdContent = ( 59 | // This is an example of how you can implement your company's logic in entity page. \u2718 1 problem (1 error, 0 warnings) Errors: 1 https://google.com/#q=import%2Fno-extraneous-dependencies info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command. lerna ERR! yarn run lint --fix stderr: Command 'eslint' exited with code 1 error Command failed with exit code 1. lerna ERR! yarn run lint --fix exited 1 in 'app' error Command failed with exit code 1. Added spacing with --fix option due to formatting issues in: packages/app/src/components/Root/LogoFull.tsx packages/app/src/themes/colorTypes.tsx Linting Rules \u00b6 Backstage utilizes the backstage-cli lint command to run the linting process. This check uses the default eslint behavior, and can also check Typescript files, interprets warning as errors, and will default to linting the entire directory if no specific files are listed. The backstage-cli lint command will flag warnings as errors and the linting process with exit with a non-zero code if there any rule violations. This means if we add the linting process to our GitHub actions for CI, then branch protection prevent any code that doesn't meet our standards from being pushed to the main branch because the GitHub Action will fail. Currently we are using the default eslint behavior provided by @backstage/cli/config/eslint for the app package and @backstage/cli/config/eslint.backend for the backend package . I added a .eslintignore so we can also run yarn backstage-cli lint in the root workspace directory for enforcing linting rules outside of the packages/plugins folders. Additional Rules \u00b6 At the moment I don't have any strong opinions towards adding additional linting rules. If anyone else does or if in the future we develop the need for more, we can always add them to our current linting configuration. Default eslint Config for App \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 module.exports = { extends: [ '@spotify/eslint-config-base', '@spotify/eslint-config-react', '@spotify/eslint-config-typescript', 'prettier', 'plugin:jest/recommended', 'plugin:monorepo/recommended', ], parser: '@typescript-eslint/parser', plugins: ['import'], env: { jest: true, }, parserOptions: { ecmaVersion: 2018, ecmaFeatures: { jsx: true, }, sourceType: 'module', lib: require('./tsconfig.json').compilerOptions.lib, }, settings: { react: { version: 'detect', }, }, ignorePatterns: ['.eslintrc.js', '**/dist/**', '**/dist-types/**'], rules: { 'no-shadow': 'off', 'no-redeclare': 'off', '@typescript-eslint/no-shadow': 'error', '@typescript-eslint/no-redeclare': 'error', 'no-undef': 'off', 'import/newline-after-import': 'error', 'import/no-duplicates': 'warn', 'import/no-extraneous-dependencies': [ 'error', { devDependencies: false, optionalDependencies: true, peerDependencies: true, bundledDependencies: true, }, ], 'no-unused-expressions': 'off', '@typescript-eslint/no-unused-expressions': 'error', '@typescript-eslint/consistent-type-assertions': 'error', '@typescript-eslint/no-unused-vars': [ 'warn', { vars: 'all', args: 'after-used', ignoreRestSiblings: true, argsIgnorePattern: '^_', }, ], 'no-restricted-imports': [ 2, { paths: [ { // Importing the entire MUI icons packages kills build performance as the list of icons is huge. name: '@material-ui/icons', message: \"Please import '@material-ui/icons/<Icon>' instead.\", }, ...require('module').builtinModules, ], // Avoid cross-package imports patterns: ['**/../../**/*/src/**', '**/../../**/*/src'], }, ], }, overrides: [ { files: ['**/*.ts?(x)'], rules: { // Default to not enforcing prop-types in typescript 'react/prop-types': 0, '@typescript-eslint/no-unused-vars': 'off', 'no-undef': 'off', }, }, { files: ['*.test.*', '*.stories.*', 'src/setupTests.*', 'dev/**'], rules: { // Tests are allowed to import dev dependencies 'import/no-extraneous-dependencies': [ 'error', { devDependencies: true, optionalDependencies: true, peerDependencies: true, bundledDependencies: true, }, ], }, }, ], }; Default eslint Config for Backend \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 module.exports = { extends: [ '@spotify/eslint-config-base', '@spotify/eslint-config-typescript', 'prettier', 'plugin:jest/recommended', 'plugin:monorepo/recommended', ], parser: '@typescript-eslint/parser', plugins: ['import'], env: { jest: true, }, globals: { __non_webpack_require__: 'readonly', }, parserOptions: { ecmaVersion: 2018, sourceType: 'module', lib: require('./tsconfig.json').compilerOptions.lib, }, ignorePatterns: ['.eslintrc.js', '**/dist/**', '**/dist-types/**'], rules: { 'no-shadow': 'off', 'no-redeclare': 'off', '@typescript-eslint/no-shadow': 'error', '@typescript-eslint/no-redeclare': 'error', 'no-console': 0, // Permitted in console programs 'new-cap': ['error', { capIsNew: false }], // Because Express constructs things e.g. like 'const r = express.Router()' 'import/newline-after-import': 'error', 'import/no-duplicates': 'warn', 'import/no-extraneous-dependencies': [ 'error', { devDependencies: false, optionalDependencies: true, peerDependencies: true, bundledDependencies: true, }, ], 'no-unused-expressions': 'off', '@typescript-eslint/no-unused-expressions': 'error', '@typescript-eslint/no-unused-vars': [ 'warn', { vars: 'all', args: 'after-used', ignoreRestSiblings: true }, ], // Avoid cross-package imports 'no-restricted-imports': [ 2, { patterns: ['**/../../**/*/src/**', '**/../../**/*/src'] }, ], // Avoid default import from winston as it breaks at runtime 'no-restricted-syntax': [ 'error', { message: 'Default import from winston is not allowed, import `* as winston` instead.', selector: 'ImportDeclaration[source.value=\"winston\"] ImportDefaultSpecifier', }, { message: \"`__dirname` doesn't refer to the same dir in production builds, try `resolvePackagePath()` from `@backstage/backend-common` instead.\", selector: 'Identifier[name=\"__dirname\"]', }, ], }, overrides: [ { files: ['**/*.ts?(x)'], rules: { '@typescript-eslint/no-unused-vars': 'off', 'no-undef': 'off', }, }, { files: ['*.test.*', 'src/setupTests.*', 'dev/**'], rules: { // Tests are allowed to import dev dependencies 'import/no-extraneous-dependencies': [ 'error', { devDependencies: true, optionalDependencies: true, peerDependencies: true, bundledDependencies: true, }, ], }, }, ], };","title":"Linting Rules"},{"location":"RFC/linting/#rfc-linting-rules","text":"Summary : Utilize linting to improve code quality. Integrating linting into our CI process will help enforce uniform standards for code styling which will reduce syntax errors and improve readability.","title":"[RFC] Linting Rules"},{"location":"RFC/linting/#background","text":"Programming languages like C or C++ utilize compilers to catch syntax errors before the code reaches a runtime environment. JavaScript is an interpreted language so it lacks a compilation phase where syntax errors can be identified before the code is ran. Linting software is a form of static code analyzing that can help enforce a set of stylistic rules or check for syntactical errors. Enforcing a single style will also improve the code review process by the need to address styling issues as well as ensure all team members are familiar with the style of the code.","title":"Background"},{"location":"RFC/linting/#goal","text":"Add linting to our CI process by using lerna to call the backstage-cli command for each of our Backstage packages and plugins.","title":"Goal"},{"location":"RFC/linting/#upgrading-from-700-to-1100","text":"","title":"Upgrading from 7.0.0 to 11.0.0"},{"location":"RFC/linting/#updates-changes","text":"Added \"@backstage/core-components\": \"^0.6.0\", to packages/app/package.json due to 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 $ backstage-cli lint --fix lerna ERR! yarn run lint --fix exited 1 in 'app' lerna ERR! yarn run lint --fix stdout: $ backstage-cli lint --fix \u2718 https://google.com/#q=import%2Fno-extraneous-dependencies '@backstage/core-components' should be listed in the project's dependencies. Run 'npm i -S @backstage/core-components' to add it src/components/catalog/EntityPage.tsx:56:1 54 | } from '@backstage/plugin-org'; 55 | import { EntityTechdocsContent } from '@backstage/plugin-techdocs'; > 56 | import { EmptyState } from '@backstage/core-components'; | ^ 57 | 58 | const cicdContent = ( 59 | // This is an example of how you can implement your company's logic in entity page. \u2718 1 problem (1 error, 0 warnings) Errors: 1 https://google.com/#q=import%2Fno-extraneous-dependencies info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command. lerna ERR! yarn run lint --fix stderr: Command 'eslint' exited with code 1 error Command failed with exit code 1. lerna ERR! yarn run lint --fix exited 1 in 'app' error Command failed with exit code 1. Added spacing with --fix option due to formatting issues in: packages/app/src/components/Root/LogoFull.tsx packages/app/src/themes/colorTypes.tsx","title":"Updates &amp; changes"},{"location":"RFC/linting/#linting-rules","text":"Backstage utilizes the backstage-cli lint command to run the linting process. This check uses the default eslint behavior, and can also check Typescript files, interprets warning as errors, and will default to linting the entire directory if no specific files are listed. The backstage-cli lint command will flag warnings as errors and the linting process with exit with a non-zero code if there any rule violations. This means if we add the linting process to our GitHub actions for CI, then branch protection prevent any code that doesn't meet our standards from being pushed to the main branch because the GitHub Action will fail. Currently we are using the default eslint behavior provided by @backstage/cli/config/eslint for the app package and @backstage/cli/config/eslint.backend for the backend package . I added a .eslintignore so we can also run yarn backstage-cli lint in the root workspace directory for enforcing linting rules outside of the packages/plugins folders.","title":"Linting Rules"},{"location":"RFC/linting/#additional-rules","text":"At the moment I don't have any strong opinions towards adding additional linting rules. If anyone else does or if in the future we develop the need for more, we can always add them to our current linting configuration.","title":"Additional Rules"},{"location":"RFC/linting/#default-eslint-config-for-app","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 module.exports = { extends: [ '@spotify/eslint-config-base', '@spotify/eslint-config-react', '@spotify/eslint-config-typescript', 'prettier', 'plugin:jest/recommended', 'plugin:monorepo/recommended', ], parser: '@typescript-eslint/parser', plugins: ['import'], env: { jest: true, }, parserOptions: { ecmaVersion: 2018, ecmaFeatures: { jsx: true, }, sourceType: 'module', lib: require('./tsconfig.json').compilerOptions.lib, }, settings: { react: { version: 'detect', }, }, ignorePatterns: ['.eslintrc.js', '**/dist/**', '**/dist-types/**'], rules: { 'no-shadow': 'off', 'no-redeclare': 'off', '@typescript-eslint/no-shadow': 'error', '@typescript-eslint/no-redeclare': 'error', 'no-undef': 'off', 'import/newline-after-import': 'error', 'import/no-duplicates': 'warn', 'import/no-extraneous-dependencies': [ 'error', { devDependencies: false, optionalDependencies: true, peerDependencies: true, bundledDependencies: true, }, ], 'no-unused-expressions': 'off', '@typescript-eslint/no-unused-expressions': 'error', '@typescript-eslint/consistent-type-assertions': 'error', '@typescript-eslint/no-unused-vars': [ 'warn', { vars: 'all', args: 'after-used', ignoreRestSiblings: true, argsIgnorePattern: '^_', }, ], 'no-restricted-imports': [ 2, { paths: [ { // Importing the entire MUI icons packages kills build performance as the list of icons is huge. name: '@material-ui/icons', message: \"Please import '@material-ui/icons/<Icon>' instead.\", }, ...require('module').builtinModules, ], // Avoid cross-package imports patterns: ['**/../../**/*/src/**', '**/../../**/*/src'], }, ], }, overrides: [ { files: ['**/*.ts?(x)'], rules: { // Default to not enforcing prop-types in typescript 'react/prop-types': 0, '@typescript-eslint/no-unused-vars': 'off', 'no-undef': 'off', }, }, { files: ['*.test.*', '*.stories.*', 'src/setupTests.*', 'dev/**'], rules: { // Tests are allowed to import dev dependencies 'import/no-extraneous-dependencies': [ 'error', { devDependencies: true, optionalDependencies: true, peerDependencies: true, bundledDependencies: true, }, ], }, }, ], };","title":"Default eslint Config for App"},{"location":"RFC/linting/#default-eslint-config-for-backend","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 module.exports = { extends: [ '@spotify/eslint-config-base', '@spotify/eslint-config-typescript', 'prettier', 'plugin:jest/recommended', 'plugin:monorepo/recommended', ], parser: '@typescript-eslint/parser', plugins: ['import'], env: { jest: true, }, globals: { __non_webpack_require__: 'readonly', }, parserOptions: { ecmaVersion: 2018, sourceType: 'module', lib: require('./tsconfig.json').compilerOptions.lib, }, ignorePatterns: ['.eslintrc.js', '**/dist/**', '**/dist-types/**'], rules: { 'no-shadow': 'off', 'no-redeclare': 'off', '@typescript-eslint/no-shadow': 'error', '@typescript-eslint/no-redeclare': 'error', 'no-console': 0, // Permitted in console programs 'new-cap': ['error', { capIsNew: false }], // Because Express constructs things e.g. like 'const r = express.Router()' 'import/newline-after-import': 'error', 'import/no-duplicates': 'warn', 'import/no-extraneous-dependencies': [ 'error', { devDependencies: false, optionalDependencies: true, peerDependencies: true, bundledDependencies: true, }, ], 'no-unused-expressions': 'off', '@typescript-eslint/no-unused-expressions': 'error', '@typescript-eslint/no-unused-vars': [ 'warn', { vars: 'all', args: 'after-used', ignoreRestSiblings: true }, ], // Avoid cross-package imports 'no-restricted-imports': [ 2, { patterns: ['**/../../**/*/src/**', '**/../../**/*/src'] }, ], // Avoid default import from winston as it breaks at runtime 'no-restricted-syntax': [ 'error', { message: 'Default import from winston is not allowed, import `* as winston` instead.', selector: 'ImportDeclaration[source.value=\"winston\"] ImportDefaultSpecifier', }, { message: \"`__dirname` doesn't refer to the same dir in production builds, try `resolvePackagePath()` from `@backstage/backend-common` instead.\", selector: 'Identifier[name=\"__dirname\"]', }, ], }, overrides: [ { files: ['**/*.ts?(x)'], rules: { '@typescript-eslint/no-unused-vars': 'off', 'no-undef': 'off', }, }, { files: ['*.test.*', 'src/setupTests.*', 'dev/**'], rules: { // Tests are allowed to import dev dependencies 'import/no-extraneous-dependencies': [ 'error', { devDependencies: true, optionalDependencies: true, peerDependencies: true, bundledDependencies: true, }, ], }, }, ], };","title":"Default eslint Config for Backend"},{"location":"RFC/node-app-logging/","text":"[RFC] Node App Logging \u00b6 Summary : Logging provides numerous benefits to applications. Mainly to help provide visibility into how our applications are running on each of the various infrastructure components. There are three major concerns for choosing a suitable logging library: recording, formatting, and storing messages. We need to make sure that our library of choice addresses all three concerns in a satisfactory manner. Background \u00b6 Backstage comes with Winston already included which is nice as Winston is a popular logger for node applications. The logger is being used within various points of the src/backend . Routes seem to be the only thing being logged at the moment. Backstage initiate's the logger within the makeCreateEnv() function. Whether we decide to use Winston or another logging service ( Bunyan ) we should decouple the various import statements for winston and only use it within a single file. Any file wanting to use the logger should then import from our controlled logger making it easy to manage and/or change in the future. Goal \u00b6 Our goal is to utilize a logger in order to provide a viewpoint into the backstage backend application. This will help to locate errors, catch bugs, and gather general information on how our application is used. We should also decide on how or what should be logged within the application, and what logging levels should be used. Either way logs will need to be output to stdout for easy consumption for services like Datadog . Recommendation \u00b6 Winston is designed to be a simple and universal logging library with support for multiple storage devices. Each logger can have multiple transports configured at different levels. Winston also allows for flexibility and configuration for logging levels, formats, and interpolation of error messages. It's also one of the more popular logging libraries which means that others give it a lot of trust as well. All current instances of importing the logger from backstage should be replaced with an import from a single file that we can name logger.js . This will decouple the logger from our code and make iterations or changing services much easier. For logging levels, and usage I think it'd be best to be as vanilla as possible. Logging levels should conform to the severity ordering specified by RFC5424 : severity of all levels is assumed to be numerically ascending from most important to least important. Logging Levels \u00b6 1 2 3 4 5 6 7 8 9 10 // Default logging levels const levels = { error : 0 , warn : 1 , info : 2 , http : 3 , verbose : 4 , debug : 5 , silly : 6 , }; Logging Level Descriptions ERROR - Represents an error condition in the system that happens to halt a specific operation, but not the overall system. WARN - Indicates runtime conditions that are undesirable or unusual, but not necessarily errors. An example could be using a backup data source when the primary source is unavailable. INFO - Info messages are purely informative. Events that are user-driven or application-specific may be logged at this level. A common use of this level is to log interesting runtime events, such ast he startup or shutdown of a service. DEBUG - Used to represent diagnostic information that may be needed for troubleshooting. Example usage. logger.info(\"Informative Message\") // {\"message\":\"Informative Message\", \"level\":\"info\"} Be Descriptive \u00b6 Log entries should adequately describe the events that they represent. Each message should be unique to the situation and should clearly explain the event that occurred at that point. Bad Example \"Request Failed, will retry.\" Good Example \"POST\" request to \" https://example.com/api \" failed. Response code: \"429\", response message: \"too many requests\". Retrying after \"60\" seconds.","title":"Node App Logging"},{"location":"RFC/node-app-logging/#rfc-node-app-logging","text":"Summary : Logging provides numerous benefits to applications. Mainly to help provide visibility into how our applications are running on each of the various infrastructure components. There are three major concerns for choosing a suitable logging library: recording, formatting, and storing messages. We need to make sure that our library of choice addresses all three concerns in a satisfactory manner.","title":"[RFC] Node App Logging"},{"location":"RFC/node-app-logging/#background","text":"Backstage comes with Winston already included which is nice as Winston is a popular logger for node applications. The logger is being used within various points of the src/backend . Routes seem to be the only thing being logged at the moment. Backstage initiate's the logger within the makeCreateEnv() function. Whether we decide to use Winston or another logging service ( Bunyan ) we should decouple the various import statements for winston and only use it within a single file. Any file wanting to use the logger should then import from our controlled logger making it easy to manage and/or change in the future.","title":"Background"},{"location":"RFC/node-app-logging/#goal","text":"Our goal is to utilize a logger in order to provide a viewpoint into the backstage backend application. This will help to locate errors, catch bugs, and gather general information on how our application is used. We should also decide on how or what should be logged within the application, and what logging levels should be used. Either way logs will need to be output to stdout for easy consumption for services like Datadog .","title":"Goal"},{"location":"RFC/node-app-logging/#recommendation","text":"Winston is designed to be a simple and universal logging library with support for multiple storage devices. Each logger can have multiple transports configured at different levels. Winston also allows for flexibility and configuration for logging levels, formats, and interpolation of error messages. It's also one of the more popular logging libraries which means that others give it a lot of trust as well. All current instances of importing the logger from backstage should be replaced with an import from a single file that we can name logger.js . This will decouple the logger from our code and make iterations or changing services much easier. For logging levels, and usage I think it'd be best to be as vanilla as possible. Logging levels should conform to the severity ordering specified by RFC5424 : severity of all levels is assumed to be numerically ascending from most important to least important.","title":"Recommendation"},{"location":"RFC/node-app-logging/#logging-levels","text":"1 2 3 4 5 6 7 8 9 10 // Default logging levels const levels = { error : 0 , warn : 1 , info : 2 , http : 3 , verbose : 4 , debug : 5 , silly : 6 , }; Logging Level Descriptions ERROR - Represents an error condition in the system that happens to halt a specific operation, but not the overall system. WARN - Indicates runtime conditions that are undesirable or unusual, but not necessarily errors. An example could be using a backup data source when the primary source is unavailable. INFO - Info messages are purely informative. Events that are user-driven or application-specific may be logged at this level. A common use of this level is to log interesting runtime events, such ast he startup or shutdown of a service. DEBUG - Used to represent diagnostic information that may be needed for troubleshooting. Example usage. logger.info(\"Informative Message\") // {\"message\":\"Informative Message\", \"level\":\"info\"}","title":"Logging Levels"},{"location":"RFC/node-app-logging/#be-descriptive","text":"Log entries should adequately describe the events that they represent. Each message should be unique to the situation and should clearly explain the event that occurred at that point. Bad Example \"Request Failed, will retry.\" Good Example \"POST\" request to \" https://example.com/api \" failed. Response code: \"429\", response message: \"too many requests\". Retrying after \"60\" seconds.","title":"Be Descriptive"}]}